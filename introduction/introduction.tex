% !TEX root = ../proj_report_outline.tex
\chapter{Introduction}\label{C:intro}
Recurrent Neural Networks (RNNs) are powerful machine learning models for learning mappings between
pairs of sequences. Such tasks are highly challenging and include language modelling,
machine translation, speech recognition and audio classification. They have achieved remarkable
success at immense scale \autocite{Wu2016a}, but the most successful architectures are
complicated and poorly understood while simple alternatives tend to perform badly \autocite{Bengio1994}. Understanding how RNNs 
solve complex tasks and designing new, simpler architectures that are well-understood from the
outset is an important goal.

RNNs make computations at each step of the input sequence based on the current input value and
a fixed size vector of hidden states. At each step these states are updated and passed forward which enables
the network to model complex temporal dynamics using the hidden states as a working memory. 
The most successful architectures have two key ingredients:
additive state updates and multiplicative gating. The former allows running integration of new information
at each time step while the latter provides the ability to selectively ignore irrelevancies. 

Multiplicative gating involves computing element-wise products of vectors. We can
generalise this by computing a product involving a three-way tensor of weights, termed
a bilinear product. Looking to understand and generalise the structures prevalent in
existing RNNs, we investigate the properties of the bilinear product in detail. 
This leads to the remarkable
result, expressed in theorem~\ref{thm:xor}, that these bilinear products operate on the exclusive-or
of features. The implications of this are clear and strong -- bilinear products operate
at a higher level than the standard linear operations composed to form neural networks.

The downside of these tensor products is that using them explicitly requires storing a very large tensor.
We therefore look to the multi-linear algebra literature to investigate methods of
tensor decomposition. Finding a useful decomposition would allow us to incorporate this highly expressive
tensor product in a neural network with a feasible number of parameters. We find that the simplest
decomposition, CANDECOMP/PARAFAC \autocite{Carroll1970, Harshman1970}, fulfils our criteria
both theoretically and experimentally.

Equipped with this tensor decomposition we propose two new classes of RNNs: the Generalised
Multiplicative RNN (GMR) and the Tensor Gate Unit (TGU).
The GMR is a very simple way to incorporate a tensor into a neural network and generalises a number of
recently proposed architectures \autocite{Martens2011a, Wu2016}. The TGU includes an
additive state update and standard multiplicative gates as well as the bilinear product. 
This architecture is based on the observation that the gate is the most important part of such an architecture
and that if it is sufficiently expressive, other components of the network can be simplified. This
results in a unique, clearly defined architecture with full expressive power and a minimum of extraneous
components.

These architectures are then evaluated empirically. We achieve
excellent results on synthetic tasks
including successfully training a TGU to capture time dependencies up to 10,000 steps,
more than 10 times the longest previously reported. To evaluate the ability of the model to carefully
write arbitrary patterns to its memory we propose a novel form of \emph{variable binding} as no
satisfactory task was present in the literature.

Finally, we demonstrate that our proposed RNNs are highly competitive with the best existing techniques
on real world data. In doing so, we investigate the hypothesis that controlling the rank of the tensor
decomposition can provide an important regularisation effect, helping greatly to reduce overfitting.
These results justify the intuitions and theoretical analysis behind the proposed architectures
as well as indicating they provide a useful class of novel RNNs with clear performance benefits.