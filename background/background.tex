% !TEX root = ../proj_report_outline.tex

\chapter{Background and Related Work}\label{C:bg}
\section{Background}
\subsection{Feed-Forward Neural Networks}
This is to be pretty brief. Cover useful things -- what they look like, gradient descent
(in brief) and outline some results on the expressive power.

\subsection{Recurrent Neural Networks}
Recurrent Neural Networks (RNNs) of the form considered here generalise feed-forward networks to
address problems in which we wish to map a \emph{sequence} of inputs 
\(\vec{x} = (\vec{x}_1, \vec{x}_2, \dots \vec{x}_T) \) to a sequence of outputs
\(\vec{y} = (\vec{y}_1, \vec{y}_2, \dots \vec{y}_T) \). They have been applied successful to a
wide range of tasks which can be framed in this way including statistical language modelling
\autocite{Mikolov2012} (including machine translation \autocite{Cho2014}), speech recognition \autocite{Graves2006}, polyphonic music 
modelling \autocite{Boulanger-Lewandowski2012}, music classification \autocite{Choi2016},
image generation \autocite{Gregor2015} and more.

\subsubsection{Original Formulation}
An RNN is able to maintain context over a sequence by transferring its hidden state from one
time-step to the next. We refer to the vector of states at time \(t\) as \(\vec{h}_t\).

The classic RNN (often termed ``vanilla'') originally proposed in \autocite{Elman1990}
computes its hidden states with the following recurrence:
\begin{equation}
	\vec{h}_t = f(\mat{W}\vec{h}_{t-1} + \mat{U}\vec{x}_t +  \vec{b})
\label{eq:vanillarnn}
\end{equation} where \(f(\cdot)\) is some elementwise non-linearity, often the hyperbolic tangent:
\(\tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\).

Equation \eqref{eq:vanillarnn} bears a striking resemblance to the building block of a 
feed-forward network. The key difference is the (square) matrix \(\mat{W}\) which contains weights
controlling how the previous state affects the computation of the new activations.

\subsubsection{Training}
We can train this (or any of the variants we will see subsequently) using back-propagation.
Often termed ``Back Propagation Through Time'' \autocite{Werbos1990} which requires using the
chain rule to determine the gradients of the loss with respect to the network parameters in
the same manner as for feedforward networks.

To understand what is required to perform this, consider a loss function for the whole sequence of
the form
\begin{equation}
	\mathcal{L}(\hat{\vec{y}}_1, \hat{\vec{y}}_2, \dots, \hat{\vec{y}}_T,
				\vec{y}_1, \vec{y}_2, \dots, \vec{y}_T)
	= \sum_{i=1}^T \mathcal{L}_i(\hat{\vec{y}}_i, \vec{y}_i),
	\label{eq:seqloss}
\end{equation} which is a sum of the loss accrued at each time-step. This captures all common
cases including sequence classification or regression, as the \(\mathcal{L}_i\) may simply return
\(0\) for all but the last time-step. To find gradients of the loss with respect the parameters
which generate the hidden states, we must first find the gradient of the loss with respect to the
hidden states themselves. Choosing a hidden state \(i\) somewhere in the sequence we have:
\begin{equation}
	\nabla_{\vec{h}_i}\mathcal{L} = \sum_{j=i}^t \nabla_{\vec{h}_i}\mathcal{L}_j
	\label{eq:delhL}
\end{equation} from the definition of the loss and the fact that a hidden state may affect all
future losses. To determine each \(\nabla_{\vec{h_i}}\mathcal{L}_j\) (noting \(j \geq i\)), we
apply the chain rule, to back-propagate the error from time \(j\) to time \(i\). This is the step
from which the algorithm derives its name, and simply requires multiplying through adjacent
timesteps. Let \(\vec{z}_k = \mat{W}\vec{h}_{t-1} + \mat{U}\vec{x}_t +  \vec{b}\) be the
pre-activation of the hidden states. Then
\begin{align}
	\nabla_{\vec{h}_i} \mathcal{L}_j &= 
	(\nabla_{\vec{h}_j})^{\mathsf{T}}\mathcal{L}_j \left(
		\prod_{k=i+1}^j \frac{\partial \vec{h}_k}{\partial\vec{h}_{k-1}}\right)\\
	&=  (\nabla_{\vec{h}_j}\mathcal{L}_j)^{\mathsf{T}} \left(
		\prod_{k=i+1}^j \nabla_{\vec{z}_k}f \cdot \mat{W}\right).
	\label{eq:bptt-v}
\end{align} This has two key components: \(\nabla_{\vec{h}_j}\mathcal{L}_j\) quantifies the degree
to which the hidden states at time \(j\) affect the loss (computing this will most likely require
further back-propagation through one or more output layers) while the second term in
equation~\eqref{eq:bptt-v} measures how much the hidden state at time \(i\) affects the hidden
state at time \(j\).

We can now derive an update rule for the parameters by observing
\begin{align}
	\nabla_{\mat{W}} \mathcal{L} &= \sum_{i=1}^T \nabla_{\mat{W}} \mathcal{L}_i \\
	&= \sum_{i=1}^T\sum_{j=1}^i\nabla_{\mat{h}_j} \mathcal{L}_i \nabla_{\mat{W}} \vec{h}_j
\end{align} 
{\Large shapes, get them right}\\
and applying the above. For the input matrix and the bias the process is the same.

\subsubsection{Issues}
Equation~\eqref{eq:bptt-v} reveals a key pathology of the vanilla RNN -- vanishing gradients.
This occurs when the gradient of the loss vanishes to a negligibly small value as we
propagate it backward in time, leading to a negligible update to the weights.
 \((\nabla_{\vec{h}_j}\mathcal{L}_j)^\mathsf{T}\) (a vector) is multiplied by a long product of
 matrices, alternating between \(\nabla_{\vec{z}_k} f\) and \(\mat{W}\). If we assume for
 illustrative purposes that \(f(\cdot)\) is the identity function (so we have a linear network),
then the loss vector is multiplied by \(\mat{W}\) taken to the \((j-i)\)th power. If the largest
eigenvalue of \(\mat{W}\) is large, then this will cause the gradient to eventually explode.
If the largest eignevalue is small, then the gradient will vanish. This issue was first presented
in 1994 \autocite{Bengio1994}, for a thorough treatment including necessary conditions 
for vanishing and the complementary exploding problem, see \autocite{Pascanu2012}.

In the non-linear case, this remains a serious issue. While exploding gradients are often
mitigated by using a \emph{saturating} non-linearity so that the gradient tends to zero as the
hidden states grow, this only exacerbates the vanishing problem. 

A second issue when training RNNs can be illustrated by viewing them as iterated non-linear	
dynamical systems and thus susceptible to the ``butterfly effect'': seemingly negligible changes
in initial conditions can lead to catastrophic changes after a number of iterations
\autocite{Lorenz1963}. In RNNs this manifests as near-discontinuity of the loss surface
\autocite{Pascanu2012} as a change (for example, to a weight during back-propagation) which may
even reduce the loss for a short period can cause instabilities further on which lead to steep
increases in loss. This problem is not as well studied as vanishing gradients although some
partial solutions exist such as clipping the norm of gradients \autocite{Pascanu2012} or
using a regulariser to encourage gradual changes in hidden state \autocite{Krueger2016}.

\subsubsection{Alternate Architectures}
To address these fundamental problems a number of alternate architectures have been proposed.
Here we will outline two popular variants: the Long Short Term Memory (LSTM) and the Gated
Recurrent Unit (GRU). Both of these belong to a class of \emph{gated} RNNs, which have a
markedly different method of computing a new state.

The LSTM was proposed to alleviate the vanishing gradient problem. It uses a number of gates
to control the flow of information during the computation of new states. Although a large number
of variants exist, the standard LSTM we will consider here has the form: 
\autocite{Hochreiter1997, Graves2013}
\begin{align}\label{eq:LSTM}
	\vec{h}_t &= \vec{o}_i \odot \tau(\vec{c}_t)\\
	\vec{c}_t &= \vec{f}_t \odot \vec{c}_{t-1} + \vec{i}_t \odot \vec{g}_t\\
	\vec{g}_t &= \tau(\mat{W}_g \vec{c}_{t-1} + \mat{U}_g \vec{x}_t + \vec{b}_g)\\
	\vec{o}_t &= \sigma(\mat{W}_o \vec{c}_{t-1} + \mat{U}_o \vec{x}_t + \vec{b}_o)\\
	\vec{f}_t &= \sigma(\mat{W}_f \vec{c}_{t-1} + \mat{U}_f \vec{x}_t + \vec{b}_f)\\
	\vec{i}_t &= \sigma(\mat{W}_i \vec{c}_{t-1} + \mat{U}_i \vec{x}_t + \vec{b}_i)
\end{align} where \(\tau(\cdot)\) refers to the elementwise \(\tanh\), \(\sigma(\cdot)\) the
elementwise logistic sigmoid \(\sigma(x) = \frac{1}{1 + e^{-x}}\) and \(\cdot \odot \cdot\)
is used to denote elementwise multiplication between vectors. These equations can be hard to take
at face value -- the key elements are \(\vec{i}_t, \vec{o}_t, \vec{f}_t\), termed the
\emph{input}, \emph{output} and \emph{forget} gates respectively, are computed in the same fashion
as the activations of a vanilla neural network but use sigmoid activation function which varies
smoothly between zero and one. Combining this with elementwise multiplication has the eponymous
gating effect, attenuating the contributions of other components. 

The output gate is fairly straightforward, it simply allows the network to prevent its hidden
state from being exposed. The forget and input gates have a more difficult role to characterise.
Taken together, these control the acceptance or rejection of new information by modulating the
amount by which the new candidate state \(\vec{g}_t\) is accepted into the hidden state
\(\vec{c}_t\).

A closely related architecture proposed much more recently by Cho et al. \autocite{Cho2014} is the
GRU, which computes its state as follows:
\begin{align}\label{eq:GRU}
	\vec{h}_t &= \vec{f}_t \odot \vec{h}_{t-1} + (1 - \vec{f}_t)\odot\vec{z}_t\\
	\vec{z}_t &= \tau(\mat{W}_z(\vec{r}_t \odot \vec{h}_{t-1}) + \mat{U}_z\vec{x}_t + \vec{b}_z)\\
	\vec{f}_t &= \sigma(\mat{W}_f\vec{h}_{t-1} + \mat{U}_f\vec{x}_t + \vec{b}_f)\\
	\vec{r}_t &= \sigma(\mat{W}_r\vec{h}_{t-1} + \mat{U}_r\vec{x}_t + \vec{b}_r).
\end{align} This is a slightly simpler form than the LSTM although the alterations go beyond
simply removing the output gate. Notably, the forget gate now controls both parts of the state
update. Further, in the computation of \(\vec{z}_t\) there is a departure from the vanilla
RNN-style building block that makes up all of the LSTM's operations. This is interesting as a
half of the model's parameters, and therefore a large part of its computational power, is
dedicated towards computing state updates. However, the mechanism of their computation places
significant emphasis on using temporally local information to do so -- the \emph{reset} gate
\(\vec{r}_t\) provides the model with the ability to ignore parts of its state.

The key shared component of these architectures is an \emph{additive} state update. Another way
of phrasing this is that while the vanilla RNN attempts to learn an opaque function
\(\vec{h}_t = \mathcal{F}(\vec{x}_t, \vec{h}_{t-1})\), these gated architectures instead learn
a \emph{residual} mapping \(\vec{h}_t = \mathcal{F}(\vec{x}_t, \vec{h}_{t-1}) + \vec{h}_{t-1}\).
By itself, this would alleviate the main cause of vanishing gradients 
\autocite{Jozefowicz2015, Hochreiter1997}. This is equivalent to adding a skip connection,
allowing the state to skip a time-step and is directly analogous to the residual connections
now commonly used to address the vanishing gradient problem in very deep feed-forward networks
\autocite{He2015, Duvenaud2014, Szegedy2016}. Unfortunately the presence of the gate complicates
this perspective -- this will be analysed in detail in chapter~\ref{C:arch}.


\section{Related Work}
\subsection{Long Time Dependencies}
The key symptom of vanishing gradients in RNNs is that it makes it much harder to learn to
store information for long time periods \autocite{Bengio1994}. This makes RNNs often struggle to
solve simple-seeming tasks in which the solution requires remembering an input for many
time-steps. There are two main categories of solutions to this issue -- architectural and
algorithmic.

\subsubsection{Architectural Solutions}
These attempts to solve the problem focus on alleviating the issue by changing the manner in which
hidden states are calculated. The aforementioned LSTM and GRU are the most widespread, but
several alternatives have been proposed. 
An early attempt to solve the problem involves feeding inputs to different units at different
rates. The primary motivation for this seems to be to force the network to attend to longer
time-scales by passing some parts of it information at a slower rate, although the authors note an
intuition that more abstract representations of the sequence will change slower and hence not
require re-calculation at every step \autocite{Hihi1995}. This idea has been revisited recently in
the form of the Clockwork RNN \autocite{Koutnik2014} which flattens the network, simply forcing
the various hidden states to be updated at different predefined rates. Further approaches in this
direction attempt to have the network learn the time scale at which it should update -- recent
examples include Hierarchical Multiscale RNNs \autocite{Chung2016} and Gated Feedback RNNs
\autocite{Chung2015} which suggest various ways of connecting, via additional gated connections,
the outputs of various layers in a stacked RNN architecture.

These approaches have been successful, although it is not precisely clear how adding extra
multiplicative gates could alleviate the vanishing gradient problem. Secondly, many of these
approaches add significant extra structures to the network (typically a large LSTM)
to force it to behave in a manner which
it was already capable of. That significant extra structures need to be added to able to reliably
train it to express such behaviours seems to suggest that a more fundamental change in the model
would be wise.

A simpler recent architecture of note is the Unitary Evolution RNN
\autocite{Arjovsky2015} which guarantee the eigenvalues of the recurrent weight matrix have a
magnitude of one. While this leads to provably non-vanishing or exploding gradients, in
practice they still seem to struggle to learn to store information for very long time periods.
This is potentially due to the seemingly ad-hoc composition of unitary operators used to allow
unconstrained optimisation of parameters while maintaining the desired properties of the
recurrent matrix.

\hl{Strongly Typed}

Another line of enquiry which can help with long time-dependencies focuses on the initialisation
of the network. IRNNs \autocite{Le2015}, which simply initialise the recurrent weights matrix to the
 identity (presaged by the nearly diagonal initialisation in \autocite{Mikolov2015}), perform
remarkably well on pathological tasks. The importance of the initialisation of the recurrent weights
matrix was further emphasised in \autocite{Henaff2016} where marked differences were found between
initialising a slightly modified vanilla RNN with the identity matrix or a random orthogonal matrix.
While simply initialising the network to have certain beneficial properties provides no guarantees
on final performance after training, it is important to note the significant results reported
especially as they are likely to at least be partially transferable to any given architecture.

\subsubsection{Algorithmic Solutions}
The second class of attempts to address the issue focuses on techniques for training the network
that help overcome the problems with the gradients. The first of this is to use approximate
second-order methods to try and rescale the gradients appropriately. A number of notable attempts
used Hessian-Free Optimization \autocite{Martens2011, Boulanger-Lewandowski2012} and achieved
remarkable results. Perhaps more interestingly, it was subsequently found that many of the results
could in fact be achieved with very careful tuning of standard stochastic gradient descent with
momentum and careful initialisation \autocite{Sutskever2013a}.

Another interesting approach which seems to help learn networks learn tasks requiring longer time
dependencies is to add a regularisation term into the loss to penalise the difference between
successive hidden states \autocite{Krueger2016}. This encourages the network to learn smooth
transitions and may help it ``bootstrap'' itself past the vanishing gradients.


\subsection{Memory}
Something about using memory, needing to add extra memory/weird ways of using it to make LSTMs et al.
do cool stuff. Repeat observation that having to go to all this trouble indicates the LSTM is the
issue.

\subsection{Tensors in Neural Networks}
Including gated networks, MRNN and so on.
