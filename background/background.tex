%% $RCSfile: using.tex,v $
%% $Revision: 1.1 $
%% $Date: 2010/04/23 01:57:05 $
%% $Author: kevin $
%%
\chapter{Background and Related Work}\label{C:bg}
\section{Background}
\subsection{Feed-Forward Neural Networks}
This is to be pretty brief. Cover useful things -- what they look like, gradient descent
(in brief) and outline some results on the expressive power.
\subsection{Recurrent Neural Networks}
Recurrent Neural Networks (RNNs) generalise feed-forward networks to address problems 
in which we wish to map a \emph{sequence} of inputs 
\(\vec{x} = (\vec{x}_1, \vec{x}_2, \dots \vec{x}_T) \) to a sequence of outputs
\(\vec{y} = (\vec{y}_1, \vec{y}_2, \dots \vec{y}_T) \)


The classic RNN (often termed ``vanilla'') originally proposed in \autocite{ElmanXXXX}
computes its hidden states with the following recurrence:
\begin{equation}
	\vec{h}_t = f(\mat{W}\vec{h}_{t-1} + \mat{U}\vec{x}_t +  b)
\label{eq:vanillarnn}
\end{equation}

\section{Related Work}
\subsection{Long Time Dependencies}
Summary of approaches to fix vanishing gradients.
\subsection{Memory}
Summary of approaches to augmenting RNNs with extra memory, or other approaches to better
use memory.
\subsection{Tensors in Neural Networks}
Including gated networks, MRNN and so on.
