% !TEX root = ../proj_report_outline.tex
\chapter{Background and Related Work}\label{C:bg}
\section{Feed-Forward Neural Networks}
Feed-forward neural networks are a class of parameterised function approximators which consist of
artificial neurons arranged in layers. The building block derives from the perceptron
\autocite{Rosenblatt1958} and the networks are often termed \emph{multi-layer perceptrons} (MLPs). MLPs solve
the key limitations of the earlier perceptron \autocite{Minsky1969}, but it was not until 1986
when a reliable and general purpose procedure was 
proposed to train them \autocite{Rumelhart1986}.

It is possible to consider MLPs as a directed acyclic graph, where each node consists
of a \emph{neuron} or \emph{unit} with the direction of the edges indicating the direction information flows from
the input to the output. For an MLP this graph is divided into layers.
The first layer represents the input (with one unit per coordinate) while subsequent layers
represent individual neurons. In a standard fully-connected network, every node is connected to all
nodes in the preceding layer. When an input is presented to the network, each neuron computes
an activation by applying a non-linear function to a weighted sum of its inputs. 
%Specifically a unit that receives inputs \((x_1, x_2, \ldots, x_n)\) will compute its
%activation \(h\) as:
%\begin{equation}\label{eq:perceptron}
%	h = f\left(\sum_{i=1}^n x_iw_i\right)
%\end{equation} where \((w_1, w_2, \ldots, w_n) + b\) are the weights the neuron gives its inputs and
%\(b\) is a bias, commonly added to enhance the expressive power.
%\(f : \mathbb{R} \to \mathbb{R}\) is some nonlinear function, common examples would be a threshold,
%the logistic sigmoid \(\sigma(x) = \frac{1}{1 + e^{-x}}\) or a linear rectifier
%\(\rho(x) = \max(0, x)\) \autocite{Nair}.

Computationally and
notationally it is efficient to express many neurons at once using vectors and matrices.
Consider the input to a network as a vector of features \(\vec{x}\)
and let \(\mat{W}\) be a matrix
whose rows contain the weights for each neuron. The output of the layer is given by
%\begin{equation}\label{eq:vecperceptron}
%	h = f\left(\tran{\vec{w}}\vec{x} + b\right)
%\end{equation} where \(\vec{w}\) is the weights from eq. \eqref{eq:perceptron} as a vector.
%We can generalise this to compute an entire layer of activations at once. Let \(\mat{W}\) be a matrix
%whose rows contain the weights for each neuron and \(\vec{b}\) a vector
%containing all of their biases. We can then compute their
%activations at once with
\begin{equation} \label{eq:matperceptron}
	\vec{h} = f(\mat{W}\vec{x} + \vec{b})
\end{equation} where \(f: \mathbb{R}^n \mapsto \mathbb{R}^n\) is the non-linear function applied
element-wise. Common examples of \(f\)
include the logistic sigmoid \(\sigma(x) = 1/(1 + e^{-x})\) or
a linear rectifier \(\rho(x) = \max(0, x)\) \autocite{Nair}. We include a bias term \(\vec{b}\)
which is also learned and enhances the range of the possible transformations greatly.

This gives us a single layer of neurons, a classic MLP will perform this process twice. 
The last layer
is often linear:
\begin{equation}
	\hat{\vec{y}} = \mat{W}^{(2)}f(\mat{W}^{(1)}\vec{x} + \vec{b}^{(1)}) + \vec{b}^{(2)}.
\end{equation} This can be though of as proceeding in stages -- first compute a hidden represenation
\(\vec{h}\) as before
and then with another set of weights transform the hidden units to get a final output.

Although such neural networks are universal function approximators \autocite{Hornik1989} it is often
beneficial in practice to increase the depth as this can allow the network to express more complex
relationships
with only a moderate increase in learned parameters  \autocite{Telgarsky2016}. Depth is increased
by recursively applying the above transform with fresh weights and biases. Omitting the
biases for clarity this gives us the following network with \(l-1\) hidden layers:
\begin{equation}
	\hat{\vec{y}} = \mat{W}^{(l)}f(\mat{W}^{(l-1)}f(\dots f(\mat{W}^{(1)}\vec{x}))).
\end{equation} Adding too much depth in this fashion can cause the network to become very challenging
to train \autocite{Glorot2010, Saxe2013}. Several methods have been proposed which successfully
alleviate this, such as layer-wise pre-training \autocite{Hinton2006,Vincent2010} and more
recently residual or skip connections which change the structure of the graph to allow better flow of 
information \autocite{He2015, Huang2016}.

\subsection{Training}
Neural networks are typically trained in a supervised manner. Denote the parameters of the network
\(\theta = \{\mat{W}^{(l)}, \ldots, \mat{W}^{(1)}, \vec{b}^{(l)}, \ldots, \vec{b}^{(l)}\}\) and
the output of the network for a given input \(\vec{x}\) and set of parameters as 
\(\hat{\vec{y}} = \mathcal{F}(\vec{x};\; \theta)\). Suppose we have a set of \(m\) data items
\(\mathcal{D} = \{(\vec{x}_i, \vec{y}_i)\}_{i=1}^m\) where each item is an input-output pair. 
Let \(\mathcal{X}\) be the set of feasible \(\vec{x}\) values and \(\mathcal{Y}\) the set of
feasible \(\vec{y}\) values.
We
wish to learn a mapping which will transform each  \(\vec{x}_i\) into \(\vec{y}_i\).

To evaluate such mapping we define a loss function
\(\mathcal{L} : \mathcal{Y} \times \mathcal{Y} \mapsto \mathbb{R}\) which maps pairs of valid outputs
to a real number, such that \(\mathcal{L}(\vec{y}, \hat{\vec{y}}) = 0\) implies 
\(\vec{y} = \hat{\vec{y}}\). Typical loss functions are the squared error
\( \label{eq:squarederror}
	\mathcal{L}_{SE}(\vec{y}, \hat{\vec{y}}) 
	%= ||\vec{y} - \hat{\vec{y}}||^2_2 
	= \sum_d (y_d - \hat{y}_d)^2
\) or the cross entropy
\(
	\mathcal{L}_{CE}(\vec{y}, \hat{\vec{y}}) = -\sum_d y_d \log(\hat{y}_d)
\) (ensuring some safeguards are put in place to avoid attempting to compute \(\log 0\)).
The cross entropy is preferred when the targets can be interpreted as
probabilities.\footnote{For example, discrete class labels -- we can convert them to a one-of-\(n\)
(often termed ``one-hot'') taking discrete labels between 1 and \(n\) to \(n\) dimensional vectors
with \(0\) in all positions except one, which has its element set to \(1\). We can then view these
vectors as a (very certain) probability distribution over classes. In this case the cross entropy
corresponds exactly to the negative log-likelihood of the correct class.} This requires converting
the output of the network to a probability, typically done with either the sigmoid function 
mentioned above or the softmax \(s_i(\vec{x}) = e^{x_i}/\sum_j e^{x_j}\).

The best set of parameters for the network, \(\theta^*\), is the set which minimises the
loss across the data items. Training the network amounts to solving the following
optimisation problem:
\begin{equation}\label{eq:nnoptimise}
	\theta^* = 
	\argmin_{\theta} \sum_{i=1}^m \mathcal{L}(\vec{y_i}, \mathcal{F}(\vec{x}_i;\; \theta)).
\end{equation} In general this problem is non-convex and contains many local minima and saddle points
\autocite{Dauphin2014, Kawaguchi2016} although in practice, local methods work remarkably well
\autocite{Goodfellow2015}.

The weights are typically learned using gradient descent -- in its simplest
form, each weight is updated repeatedly in a direction which should reduce the loss:
\begin{equation} \label{eq:graddescent}
	\mat{W} \gets \mat{W} - \eta \cdot \nabla_{\mat{W}}\mathcal{L}
\end{equation} where \(\eta\) is a hyper parameter adjusting the step size (called the
\emph{learning rate}) and \(\nabla_{\mat{W}}\mathcal{L}\) is the gradient of the loss with respect
to \(\mat{W}\). Typically the gradient is evaluated over a small subset or \emph{mini-batch}
 of the data per iteration which gives a noisy estimate of the true gradient.
The gradients of deep networks can be calculated using \emph{back-propagation}, which amounts
to the chain rule of calculus and the observation that neural networks consist of many
composed functions \autocite{Rumelhart1986}. In practice this is assisted greatly by software
packages which can determine the derivatives of a given function using automatic or symbolic
differentiation such as Tensorflow \autocite{Abadi2015} or Theano
 \autocite{TheTheanoDevelopmentTeam2016}.

\section{Recurrent Neural Networks}
The Recurrent Neural Networks (RNNs) considered here extend feed-forward networks to
address problems where we wish to map a sequence of input vectors
\((\vec{x}_1, \vec{x}_2, \dots \vec{x}_T) \) to a sequence of output vectors
\((\vec{y}_1, \vec{y}_2, \dots \vec{y}_T) \). They have been applied successfuly to a
wide range of tasks which including statistical language modelling
\autocite{Mikolov2012}, machine translation \autocite{Cho2014, Wu2016a}, speech 
recognition \autocite{Graves2006}, music classification \autocite{Choi2016},
image generation \autocite{Gregor2015}, automatic captioning \autocite{Vinyals2016, Xu2015}  and more
with incredible success.

\subsection{Classical Formulation}
An RNN maintains context over a sequence by transferring its hidden state from one
time step to the next. We refer to the vector of states at time \(t\) as \(\vec{h}_t\).
The classic RNN (often termed ``Vanilla'') originally proposed in \autocite{Elman1990}
computes its hidden states with the following recurrence:
\begin{equation}
	\vec{h}_t = f(\mat{W}\vec{h}_{t-1} + \mat{U}\vec{x}_t +  \vec{b}).
\label{eq:vanillarnn}
\end{equation} \(f(\cdot)\) is again some element-wise non-linearity, often the hyperbolic tangent:
\(\tanh(x) = (1 - e^{-2x}) / (1 + e^{-2x})\). Final outputs are computed from these
states in the same manner as feed-forward networks. This remains the same for the
extended architectures considered below, the difference is the manner in which the states
are produced.

Equation \eqref{eq:vanillarnn} is closely related to the building block of a 
feed-forward network. The key difference is the (square) matrix \(\mat{W}\) which contains weights
controlling how the previous state affects the computation of the new activations.

\subsection{Training}
We can train this (or any of the variants we will see subsequently) using back-propagation.
Often termed ``Back Propagation Through Time'' \autocite{Werbos1990}, this requires using the
chain rule to determine the gradients of the loss with respect to the network parameters in
the same manner as feed-forward networks.

To illustrate this, consider a loss function for the whole sequence of
the form
\begin{equation}
	\mathcal{L}(\hat{\vec{y}}_1, \hat{\vec{y}}_2, \dots, \hat{\vec{y}}_T,
				\vec{y}_1, \vec{y}_2, \dots, \vec{y}_T)
	= \sum_{i=1}^T \mathcal{L}_i(\hat{\vec{y}}_i, \vec{y}_i),
	\label{eq:seqloss}
\end{equation} which is a sum of the loss incurred at each time step. This captures all common
cases including sequence classification in which the \(\mathcal{L}_i\) may simply return
\(0\) for all but the last time step. To find gradients of the loss with respect the parameters
which generate the hidden states, we must first find the gradient of the loss with respect to the
hidden states themselves. Choosing a hidden state \(i\) somewhere in the sequence we have:
\begin{equation}
	\nabla_{\vec{h}_i}\mathcal{L} = \sum_{j=i}^t \nabla_{\vec{h}_i}\mathcal{L}_j
	\label{eq:delhL}
\end{equation} from the definition of the loss and the fact that a hidden state may affect all
future losses. To determine each \(\nabla_{\vec{h_i}}\mathcal{L}_j\) (noting \(j \geq i\)), we
apply the chain rule to back-propagate the error from time \(j\) to time \(i\). This is the step
from which the algorithm derives its name, and simply requires multiplying through adjacent
time steps. Let \(\vec{z}_k = \mat{W}\vec{h}_{t-1} + \mat{U}\vec{x}_t +  \vec{b}\) be the
pre-activation of the hidden states. Then
\begin{align}
	\nabla_{\vec{h}_i} \mathcal{L}_j &= 
	\left(\prod_{k=i+1}^j \nabla_{\vec{h}_{k-1}} \vec{h}_k\right)
	(\nabla_{\vec{h}_j}\mathcal{L}_j) \\
	&=  \left(\prod_{k=i+1}^j \nabla_{\vec{z}_k}f \cdot \mat{W}\right)
	(\nabla_{\vec{h}_j}\mathcal{L}_j).
	\label{eq:bptt-v}
\end{align} This has two key components: \(\nabla_{\vec{h}_j}\mathcal{L}_j\) quantifies the degree
to which the hidden states at time \(j\) affect the loss while the term in parentheses
measures how much the hidden state at time \(i\) affects the hidden
state at time \(j\).

We can derive an update rule for the parameters by observing\footnote{For clarity, when we
consider the gradient of anything with respect to a matrix, such as \(\nabla_{\mat{W}}\mathcal{L}\)
we implicitly vectorise the matrix. More precise notation would be
\(\nabla_{\vect(\mat{W})}\mathcal{L}\), but this adds extra distraction into already unfortunately
 cluttered equations. For a definition of the vectorisation operation see 
section~\ref{sec:tensorinterps}. This means \(\nabla_{\mat{W}}\mathcal{L}\) has shape \(nn \times 1\),
where \(n\) is the number of hidden states. Correspondingly, \(\nabla_{\mat{W}}\vec{h_j}\) has
shape \(nn \times n\), so the shapes of the matrix multiplication in eq.~\eqref{eq:vanillaupdate}
are appropriately defined.
}
\begin{align}
	\nabla_{\mat{W}} \mathcal{L} &= \sum_{i=1}^T \nabla_{\mat{W}} \mathcal{L}_i \\
	&= \sum_{i=1}^T\sum_{j=1}^i(\nabla_{\mat{W}} \vec{h}_j)(\nabla_{\mat{h}_j} \mathcal{L}_i )
	\label{eq:vanillaupdate}
\end{align} 
and applying the above. For the input matrix \(\mat{U}\) and the bias the process is the same.

\subsection{Issues with Vanilla RNNs}
\subsubsection{Vanishing and Exploding Gradients}
Equation~\eqref{eq:bptt-v} reveals a key pathology of the vanilla RNN -- vanishing gradients.
This occurs when the gradient of the loss vanishes to a negligibly small value as we
propagate it backward in time, leading to a negligible update to the weights.
 \(\nabla_{\vec{h}_j}\mathcal{L}_j\) (a column vector) is multiplied by a long product of
 matrices, alternating between \(\nabla_{\vec{z}_k} f\) and \(\mat{W}\). If we assume for
 illustrative purposes that \(f(\cdot)\) is the identity function (so we have a linear network),
then the loss vector is multiplied by \(\mat{W}\) taken to the \((j-i)\)-th power. If the largest
eigenvalue of \(\mat{W}\) is large, then this will cause the gradient to rapidly explode.
If the largest eigenvalue is small, then the gradient will vanish. 

Vanishing gradients were diagnosed
as early as 1994 where and shown to be unavoidable if the network needs to both store
information and be robust to noise \autocite{Bengio1994}. 
However, this has not precluded further research into alleviating
the main symptoms; for a thorough treatment see \autocite{Pascanu2012}.
In the non-linear case, this remains a serious issue. While exploding gradients are often
mitigated by using a \emph{saturating} non-linearity where the gradient tends to zero as the
hidden states grow, this only exacerbates the vanishing problem. 

Vanishing gradients in RNNs make it much harder to learn to
store information for long time periods. This causes RNNs struggle to
solve simple tasks if the solution requires remembering an input for many
time steps.

\subsubsection{Butterfly Effect}
A second issue is shared with many iterated non-linear	
dynamical systems: the \emph{butterfly effect}. This is that seemingly negligible changes
in initial conditions can lead to catastrophic changes after many iterations
\autocite{Lorenz1963}. In RNNs this manifests as near-discontinuity of the loss surface
which can make gradient descent struggle.
Some
partial solutions exist such as clipping the norm of gradients \autocite{Pascanu2012},
using a regulariser to encourage gradual changes in hidden state \autocite{Krueger2016}
as well as more sophisticated optimisation techniques \autocite{Martens2011}.

\section{Popular Alternatives: LSTM and GRU}
To address these fundamental problems a number of alternate architectures have been proposed.
Here we will outline two popular variants: the Long Short Term Memory (LSTM) and the Gated
Recurrent Unit (GRU). Both of these belong to a class of \emph{gated} RNNs, which have a
markedly different method of computing a new state.

The key shared component of these architectures is an \emph{additive} state update. 
While the vanilla RNN attempts to learn an recurrent function
\(\vec{h}_t = \mathcal{F}(\vec{x}_t, \vec{h}_{t-1})\), these gated architectures instead learn
something like
a \emph{residual} mapping \(\vec{h}_t = \mathcal{F}(\vec{x}_t, \vec{h}_{t-1}) + \vec{h}_{t-1}\).
By itself, this would assist greatly with vanishing gradients 
\autocite{Jozefowicz2015, Hochreiter1997}. This is equivalent to adding a skip connection,
allowing the state to skip a time step and is directly analogous to the connections
now commonly used to address the vanishing gradient problem in very deep feed-forward networks
\autocite{He2015, Duvenaud2014, Szegedy2016}.

\subsection{LSTM}
The LSTM was proposed precisely to address the vanishing gradient problem. It uses a number of gates
to control the flow of information during the computation of new states. Although a number
of subtle variants exist, the standard LSTM we will consider here has the form
\autocite{Hochreiter1997, Graves2013}:
\begin{align}\label{eq:LSTM}
	\vec{h}_t &= \vec{o}_i \odot \tau(\vec{c}_t)\\
	\vec{c}_t &= \vec{f}_t \odot \vec{c}_{t-1} + \vec{i}_t \odot \vec{g}_t\\
	\vec{g}_t &= \tau(\mat{W}_g \vec{c}_{t-1} + \mat{U}_g \vec{x}_t + \vec{b}_g)\\
	\vec{o}_t &= \sigma(\mat{W}_o \vec{c}_{t-1} + \mat{U}_o \vec{x}_t + \vec{b}_o)\\
	\vec{f}_t &= \sigma(\mat{W}_f \vec{c}_{t-1} + \mat{U}_f \vec{x}_t + \vec{b}_f)\\
	\vec{i}_t &= \sigma(\mat{W}_i \vec{c}_{t-1} + \mat{U}_i \vec{x}_t + \vec{b}_i)
\end{align} where \(\tau(\cdot)\) refers to the \(\tanh\), \(\sigma(\cdot)\) the
sigmoid and \(\odot\)
is used to denote element-wise multiplication between vectors. 
The key elements are \(\vec{i}_t\), \(\vec{o}_t\) and \(\vec{f}_t\), termed the
\emph{input}, \emph{output} and \emph{forget} gates respectively. These are computed in the same fashion
as the activations of a vanilla neural network but use a sigmoid activation function which varies
smoothly between zero and one. Combining this with element-wise multiplication has the eponymous
gating effect, attenuating various components. 

The forget and input gates together control the acceptance or rejection of new information by 
modulating the
degree to which the new candidate state \(\vec{g}_t\) is accepted into the hidden state
\(\vec{c}_t\). The output gate simply allows the network to prevent its hidden
state from being exposed. 

\subsection{GRU}
A closely related architecture proposed much more recently is the
GRU \autocite{Cho2014}:
\begin{align}\label{eq:GRU}
	\vec{h}_t &= \vec{f}_t \odot \vec{h}_{t-1} + (1 - \vec{f}_t)\odot\vec{z}_t\\
	\vec{z}_t &= \tau(\mat{W}_z(\vec{r}_t \odot \vec{h}_{t-1}) + \mat{U}_z\vec{x}_t + \vec{b}_z)\\
	\vec{f}_t &= \sigma(\mat{W}_f\vec{h}_{t-1} + \mat{U}_f\vec{x}_t + \vec{b}_f)\\
	\vec{r}_t &= \sigma(\mat{W}_r\vec{h}_{t-1} + \mat{U}_r\vec{x}_t + \vec{b}_r).
\end{align} This simpler than the LSTM and makes some interesting alterations.
Notably, the forget gate now controls both parts of the state
update. Further, in the computation of \(\vec{z}_t\) there is a departure from the vanilla
RNN-style building block that makes up all of the LSTM's components. 
Half of the model's parameters are
dedicated towards computing state updates. The mechanism of their computation places
significant emphasis on using temporal context to do so -- the \emph{reset} gate
\(\vec{r}_t\) provides the model with the ability to ignore parts of its state but is itself a function of
the previous state.

These architectures are often used at large scale to solve very hard real world tasks. However,
the synthetic tasks on which the LSTM was originally validated \autocite{Hochreiter1997}, and which have
become popular benchmarks,
 overlook a key requirement of a successful RNN architecture. The tasks focus on problems involving long
time dependencies, but all involve remember either symbols from a fixed, small vocabulary or a single
real number. We suggest that it is also necessary to evaluate the ability of RNNs to learn to use
all of their memory cells at once to store distributed representations of the current context from which
elements of the sequence can be reconstructed. While this ability is used implicitly, especially in
architectural advancements targeted at language modelling \autocite{Cho2014b, Chan2015, Luong2016},
no specific empirical validation of this ability is regularly carried out.


\subsection{Alternative Architectures}
While the LSTM and the GRU are the most popular, many other solutions
have been proposed. In this section we outline some key lines of thought which help gain
understanding of the problems identified with the above models.

An early attempt to solve the vanishing gradient problem involves feeding inputs to different units
at different
rates. The idea is to force the network to attend to longer
time-scales by passing some information at a slower rate inducing a hierarchical structure
 \autocite{Hihi1995}. 
This idea has been revisited recently in
the form of the Clockwork RNN \autocite{Koutnik2014} which flattens the network, simply forcing
the various hidden states to be updated at different predefined rates. Further approaches in this
direction attempt to have the network learn the time scale at which it should update -- recent
examples include Hierarchical Multiscale RNNs \autocite{Chung2016} and Gated Feedback RNNs
\autocite{Chung2015} which take a standard LSTM and adjust the temporal connectivity pattern
with further multiplicative gates.

These approaches have had some success, although it is not clear how adding extra
multiplicative gates could alleviate the vanishing gradient problem. Also many of these
approaches add significant extra structures to the network
to force it to behave in a manner which
it was already fundamentally capable of. 
That this is necessary suggests more fundamental changes to the underlying model are in order.

%A closely related approach is augmenting RNNs to add an attention mechanism. This is especially
%popular in translation tasks, in which a common approach is to use two separate RNNs in an
%\emph{encoder-decoder} arrangement where the first network encodes the sentence into a fixed length
%vector and the second expands the vector into a new sentence in a different language
%\autocite{Cho2014b, Luong2016}. Content-based attention is often used to improve these models, 
%by allowing the decoder access to all previous states of the encoder via a weighted sum. Weights
%are derived from similarities between each encoder state and a vector produced by the decoder at
%each step \autocite{Chan2015, Bahdanau2015}.
%
%While this achieves excellent performance at
%extraordinary scale \autocite{Wu2016a}, it also requires computing similarities for every step of the
%output with every step of the input, which will scale quadratically in sentence length. Consequently,
%it seems like a useful avenue of research to allow the encoder RNN to learn a more useful reduced
%representation of the sentence to remove the need for the attention mechanism.

A simpler recent architecture of note is the Unitary Evolution RNN
\autocite{Arjovsky2015} which guarantee the eigenvalues of the recurrent weight matrix have a
magnitude of one. While this leads to provably non-vanishing or exploding gradients, in
practice they still struggle to learn to store information for very long time periods.
This is may be due to the somewhat ad-hoc composition of unitary operators used to allow
unconstrained optimisation of parameters while maintaining the desired properties of the
recurrent matrix.

Another principled way to approach the design of RNNs is to use \emph{strongly typed} architectures
\autocite{Balduzzi2016}. This approach is inspired by the desire to alter the main source of problems
in classic RNNs -- the recurrent weight matrix. The argument is that continually applying the same
weight matrix to the hidden states leads to incoherent features in the hidden state. The solution is
to attempt to \emph{diagonalise} the state updates by ensuring all operations on the
hidden state apply element-wise, and that the input should be the only item projected through weight
matrices.

Another line of enquiry which can help with long time-dependencies focuses on the initialisation
of the network. IRNNs \autocite{Le2015}, which simply initialise the recurrent weight matrix to the
identity (presaged by the nearly diagonal initialisation in \autocite{Mikolov2015}), perform
remarkably well on pathological tasks. The importance of the initialisation of the recurrent weights
matrix was further emphasised in \autocite{Henaff2016} where marked differences were found between
initialising a slightly modified vanilla RNN with the identity matrix or a random orthogonal matrix.
While simply initialising the network to have certain beneficial properties provides no guarantees
on final performance after training, it is important to note the significant results reported
especially as they are likely to at least be partially transferable to any given architecture.
%
%\subsubsection{Algorithmic Solutions}
%The second class of attempts to address the issue focuses on techniques for training the network.
% The first of these is to use approximate
%second-order methods to try and rescale the gradients appropriately. A number of notable attempts
%used Hessian-Free Optimization \autocite{Martens2011, Boulanger-Lewandowski2012} and achieved
%remarkable results. Perhaps more interestingly, it was subsequently found that many of the results
%could in fact be achieved with very careful tuning of standard stochastic gradient descent with
%momentum and careful initialisation \autocite{Sutskever2013a}.
%
%Another interesting approach which seems to help learn networks learn tasks requiring longer time
%dependencies is to add a regularisation term into the loss to penalise the difference between
%successive hidden states \autocite{Krueger2016}. This encourages the network to learn smooth
%transitions and may help it ``bootstrap'' itself past the vanishing gradients.
%
%A notable recent work applies the recently proposed path-SGD \autocite{Neyshabur2015} to Vanilla
%RNNs with ReLU activations. This involves re-casting the RNN as a very deep feed-forward network with
%shared weights across times-steps. The authors then regularise the update based on the scale of
%the individual paths through the network. This allows them to achieve significant results with an
%architecture that is otherwise very challenging to train -- the linear rectifier is unbounded in
%one direction which typically leads rapidly to exploding gradients \autocite{Neyshabur2016}.


\subsection{Memory}
This line of research attempts to enhance the capabilities of RNNs by focusing on the way they
interact with their memory. There are two key motivations for these works. 
The first is to decouple the memory from the network so that it can store and interact with arbitrary
quantities of information. The second is the hope that a novel
method of storing or addressing information will encourage the networks to solve more complex
tasks than is typically feasible with hidden states alone.

A notable effort to decouple RNNs from their memory is the Neural Turing Machine proposed in
\autocite{Graves2014}. This uses a neural network (which may or may not be recurrent) to
attend to a large matrix of memory. Reading and writing is done via a
address mechanism mostly based on the softmax.

Similarly Grefenstette et al. propose a set of neural Deques, Queues and Stacks which attempt to
address the limitations of the fixed size memory of the Neural Turing Machine 
\autocite{Grefenstette2015}. A similar approach is Joulin and Mikolov's Stack Augmented
RNN \autocite{Joulin2015}. These allow for unbounded, efficient memory at the cost of
no random access. Primarily these are proposed as augmentations to an RNN,
with the idea being that a standard RNN could operate as a small working memory while the more
complex data structure (with more awkward access semantics) can be used for long-term, exact
storage. This is an interesting direction as it enables something akin to a learnable Von Neumann
architecture \autocite{Graves2014}.

While these approaches can be highly successful, very few of the tasks 
they are evaluated on are tasks that an RNN should inherently be
incapable of solving. Nearly all of the tasks addressed require a fixed size memory, and solutions
can be imagined composed of carefully adding and removing information from a running representation.
Hence it is worth revisiting the basic architecture before searching for distinct 
modules to add on.

An approach in this direction is Danihelka et al.'s Associative LSTM \autocite{Danihelka2016}. 
This work incorporates an associative memory model into the LSTM by introducing
 something close to a Holographic Reduced Representation \autocite{Plate1995}.
This is a very encouraging angle -- to re-think how the RNN uses its memory at a basic
level is an efficient and sensible way to address the issues.
Unfortunately simply adjusting the network to use such a reduced representation was insufficient for
good performance and a number of complicating factors had to be incorporated such as keeping many
redundant copies of memory which diminishes the elegance of the solution.

In
general, the task of an RNN is to learn a mapping from one sequence to another, one step at
a time using an intermediate memory (the hidden states). Intuitively a solution must consist of
writing to and reading from memory, as new inputs arrive. In an RNN, reading from the memory is
straightforward as the entire memory is used to produce the output at each time
step. Writing can be separated into two operations: accumulating
and forgetting.
We consider an accumulation to be an updating of the state, typically additively, to incorporate
new information. Forgetting is simply wiping clean a section of the memory -- this tends to be
achieved multiplicatively. Different tasks will require a different balance of the two. Instinctively,
classification tasks in which the network is only evaluated on its final output would favour
more accumulative solutions. Conversely, tasks which may not even have a fixed length and
require constant prediction should be better suited to networks with an ability to forget information
that is no longer relevant.

The solutions in this section sidestep difficulties current RNN architectures have with
achieving fully general memory semantics when limited to these operations by allowing additional
operations on auxiliary data structures. Aside from capacity they
do not fundamentally change the set of available solutions but rather impose structures to guide
the network into particular modes of behaviour.

\section{Tensors in Neural Networks}
The final set of related works provide the key motivation for the next section. In general, many of
the above architectures contain multiplicative gates. Multiplicative gating structures have a long
history,
introduced by Hinton as early as 1981 \autocite{Hinton1981}. Sigaud et al. separate modern uses into
two classes based on the intention \autocite{Sigaud2015}. The first class
seeks to use the gate in a manner akin to a transistor in a digital circuit, looking to switch the
flow of information between computational units on or off. This captures the way such connections are
used in LSTMs as well as their feed-forward cousins Highway Networks \autocite{Srivastava2015}. The
second class of gated networks aims to implement a multiplicative connection between inputs, often
before any non-linearity. In general, the
latter class strictly generalises the former. Two excellent reviews can be
found in \autocite{Memisevic2011, Sigaud2015} and rather than repeat in detail the history we
attempt to bring out the most salient examples.

Of particular note is the
Gated Boltzmann Machine \autocite{Memisevic2007} which generalises Restricted
Boltzmann Machines \autocite{Smolensky1986} (a class of probabilistic graphical models very closely
related to neural networks) to model conditional relationships between pairs of images. These 
parameterise a multiplicative connection with a three-way tensor \(\tensor{W}\).
The likelihood of a hidden unit
being \(1\) given a pair of inputs \(\vec{x}\) and \(\vec{y}\) is defined as
\begin{equation}\label{eq:grbm}
	p(h_k | \vec{x},\vec{y}) = \sigma\left(\sum_{i,\;j} W_{ijk}x_iy_j \right)
\end{equation} where \(\sigma(\cdot)\) is the sigmoid discussed earlier \autocite{Memisevic2007}.
This architecture  naturally extends the operation of a feed-forward network to deal with
two inputs.
Unfortunately the size of the
tensor \(\tensor{W}\) is a serious limitation. This work outlines three key points: 
multiplicative interactions are a natural way of handling multiple distinct inputs,
multiplicative interactions are properly parameterised by tensors
and tensors can be prohibitively large.

The Multiplicative RNN \autocite{Martens2011a, Sutskever2013} (MRNN) picks up this idea, by observing
that RNNs naturally have two inputs -- the previous state and the current input. They apply the
factorisation proposed in the Conditional Restricted Boltzmann Machine 
\autocite{Taylor, Memisevic2010} to manage the size of the tensor.
Applied in a language modelling context, the reasoning is that it
makes sense to allow the current input symbol to affect the recurrent transition matrix. These MRNNs
achieved significant results, especially when combined with Hessian Free \autocite{Martens2011},
an approximate second order optimisation method.

Closely related is the recently proposed Multiplicative Integration RNN (MI-RNN)
 which adjusts the building block of eq.~\eqref{eq:vanillarnn} to use element-wise multiplication instead
of addition \autocite{Wu2016}.
 This corresponds to factorising the tensor into two matrices and has an extreme gating
effect, especially during back-propagation.
In order to successfully learn, it was found necessary to add additional bias terms such that
the final model required four weight matrices and five bias vectors per unit.

Whether explicitly acknowledged or not, both the MRNN and MI-RNN use a \emph{tensor decomposition}
to manage the parameters of a bilinear 
multiplicative relationship. Where they differ is the precise form of
the decomposition and how they apply biases to enhance the expressive power. We find that investigating
tensor products and decompositions in general allows us to generalise both of these models into a single
more flexible family. 

This style of gating is also used to modulate the flow of information between layers of deep 
architectures, being employed in Gated-Attention Readers
to combine representations of questions and potential answers \autocite{Dhingra2016}. It is also used
in PixelCNN, WaveNet and Highway Networks \autocite{VandenOord2016, Oord2016, Srivastava2015}, 
allowing the model to choose how
particular features propagate upwards through layers.

It is important to realise that all of these multiplicative structures, including the LSTM and GRU
have this inherent tensor
structure. In chapter~\ref{C:tens} we investigate with more
rigour the types of tensor products which generalise these multiplicative interactions.
