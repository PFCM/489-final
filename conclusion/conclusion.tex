% !TEX root = ../proj_report_outline.tex
\chapter{Conclusions}\label{C:con}
The first key observation is that gated RNNs have an inherent tensorial structure. Detailed
analysis of this structure indicates that it contributes greatly to their expressive power.
This leads to the idea of designing new networks that draw this out, allowing
for control over the balance between representative capacity and practical requirements.
By fully realising this important underlying processes we can 
gain much more of an understanding into how the network arrives at solutions to sequence
problems.

The second key observation is that networks with a gated, additive state update are very appealing.
This structure allows the network to sit on the border between vanishing and
exploding gradients and should therefore allow excellent learning performance. 
Intuitively it helps force the network to compute a sensible reduced representation of the
entire sequence. Correspondingly we
retain this structure when designing novel architectures.

This means we have three choices to make: how to perform
the gating, how to compute the gate signal and how to compute the additive update. By reasoned analysis,
we arrive at clear conclusions for all of them, including taking the novel step of computing an
additive update which does not depend on the previous hidden state. This is a key novelty in our
proposed class of architectures which allows them to allocate more expressive power to controlling
the flow of information.  

Empirically, we find our conclusions are justified. Our architecture
demonstrates the ability to store items in
memory for extraordinarily
long time periods and the ability to store and reproduce arbitrary inputs well in advance
of existing architectures. This illustrates the benefits of carefully
simplifying the architecture -- as each
component has a clearly defined role we observe rapid learning even on highly challenging tasks.

We also investigate the role of the extra hyper-parameter introduced by utilising a tensor decomposition.
Reducing the rank is found to have a useful regularising effect, helping to improve generalisation 
performance on real-world data when
set appropriately. This again points to the utility of making explicit the underlying tensor structure
in gated networks. Once we have control over the decomposition we can use it to our advantage to
trade off representative power with generalisation ability.

We have presented a new class of promising architectures for recurrent neural networks.
Existing approaches are either seriously flawed or opaque and over-complicated.
This necessitates finding more elegant solutions which maintain the performance but keep
only components with clearly defined roles and a minimum of confusion and complexity.
Further, the analysis and intuitions that lead to these architectures provide a new way
of thinking about the building blocks of RNNs which could inform much novel work beyond
our own.