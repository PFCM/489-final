% !TEX root = ../proj_report_outline.tex

\chapter{Tensors}\label{C:tens}

In order to represent functions of two arguments with neural networks, three-way tensors are 
unavoidable. In this chapter discuss the vector-tensor-vector bilinear product. We look at various
low-rank tensor decompositions to avoid having to store the tensor in full with a view towards
assessing their suitability to be learnt in situ with gradient descent.

It is also important to understand conceptually what the bilinear product does. It admits a surprising
number of interpretations including theorem~\ref{thm:xor} -- that it allows us to operate on a
pairwise exclusive-or of features. Additionally, using different tensor decompositions to represent
the tensor corresponds to emphasising different modes of interpretation. Making these relationships
clear is then an important step in making a principled choice of decomposition to insert into a 
network architecture.

Finally, we undertake preliminary experiments to investigate empirically the feasibility of learning
tensor decompositions in practice.

\section{Definitions}
Formally, we refer to a multi-dimensional array which requires \(n\) indices to address a single
(scalar) element as a \(n\)-way tensor and occasionally refer to \(n\) as the number of dimensions of
the tensor. In this sense a matrix is a two-way tensor and a vector
a one-way tensor, although we will use their usual names for clarity. Notationally, we attempt to
stick to the notation used in \autocite{Kolda2009} deviating only where it would become unwieldy.
We denote tensors with three or more dimensions with single calligraphic boldface letters such
as \(\tensor{W}\). Matrices and vectors will be denoted with upper and lower case boldface letters
while scalars will be standard lower case letters. Commonly we will need to address particular
substructures of tensors. This is analogous to pulling out individual rows or columns of a matrix.
To perform this we fix some of the indices to specific values and allow the remaining indices to vary
across their range. We denote by \(\midbullet\) the indices allowed to vary, the rest will be provided
with a specific value. For example, a single row of a matrix \(\mat{A}\) would be denoted 
\(\mat{A}_{i\midbullet}\).
For three-way tensors we refer to the vector elements produced by fixing two indices as 
\emph{threads}. It is also possible to form matrices by fixing only one index -- we refer to these
as \emph{slices}. Table~\ref{tab:notation} provides examples of the possibilities.

\begin{table}
\centering
\begin{tabu} to 0.5\linewidth {|r|l|}
\hline 
\textit{Description} & \textit{Example} \\
\hline
scalar & \(a\) \\
vector & \(\vec{b}\) \\
matrix & \(\mat{C}\) \\
higher order tensor & \(\tensor{D}\) \\
element of vector (scalar) & \(b_i\) \\
element of matrix (scalar) & \(C_{ij}\) \\
element of 3-tensor (scalar) & \(D_{ijk}\) \\
row of matrix (vector) & \(\vec{C}_{i\midbullet}\) \\
column of matrix (vector) & \(\vec{C}_{\midbullet i}\)\\
\textit{fiber} of 3-tensor (vector) & \(\vec{D}_{\midbullet jk}\)\\
\textit{slice} of 3-tensor (matrix) & \(\mat{D}_{\midbullet\midbullet k}\)\\
\hline
\end{tabu}
\caption{Example of notation for tensors.}
\label{tab:notation}
\end{table}

When dealing with tensor-tensor products in general, it is important to be precise as there are often
a number of possible permutations of indices that would lead to a valid operation. The downside of
this is that it leads to unwieldy notation. Fortunately, we are only concerned with a a couple of
special cases. In particular, we need to multiply a three-tensors by vectors and a matrices by
vectors. Matrix-vector multiplication consists of taking the dot product of the vector with
each row of the matrix. For example, with a matrix \(\mat{A} \in \mathbb{R}^{m \times n}\) and a
vector \(\vec{x} \in \mathbb{R}^n\), if \(\vec{y} = \mat{A}\vec{x}\) (with \(\vec{y}\) necessarily
in \(\mathbb{R}^m\)), then
\begin{align}\label{eq:matmul}
	y_i &= \sum_j^n A_{ij} x_j \\
		&= \langle \vec{A}_i, \vec{x}\rangle
\end{align} where \(\langle \cdot, \cdot \rangle\) denotes the inner (dot) product. This can be viewed
as taking all of the vectors formed by fixing the first index of \(\mat{A}\) while allowing the second
to vary and computing their inner product with \(\vec{x}\). To perform the same operation using the
columns of \(\mat{A}\) we need to fix the second index, the would typically be done by exchanging the
order: \(\tran{\vec{x}}\mat{A}\).

We can generalise the operation to tensors: choose an index over which to perform the product,
collect every thread formed by fixing all but that one index and compute their inner product with the
given vector. If the tensor has \(n\) indices, the result will have \(n-1\). A three tensor is in
this way reduced to a matrix. Kolda and Bader introduce the operator
 \(\;\cdot \bar{\times}_i \cdot\;\)
for this, where \(i\) represents the index to vary \autocite{Kolda2009}. For the bilinear forms
we are concerned with, this leads to the following notation:
\begin{align}
	\vec{z} &= \tensor{W} \;\bar{\times}_1\; \vec{x}\; \bar{\times}_2\; \vec{y} 
	\label{eq:bilinearkolda} \\
	&= \tensor{W} \; \bar{\times}_3\; \vec{y}\; \bar{\times}_1\; \vec{x}.
	\label{eq:bilinearkolda2}
\end{align}

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
	\centering
	\begin{tikzpicture}
		\draw (0,0) node {} -- (-1,0);
	\end{tikzpicture}
	\caption{Vector \(\vec{a}\in\mathbb{R}^{n_1}\)}\label{fig:tnd:vec}
	\end{subfigure} ~
	\begin{subfigure}[t]{0.45\textwidth}
	\centering
	\begin{tikzpicture}
		\draw (1,0) -- (0,0) node {} -- (-1,0);
	\end{tikzpicture}
	\caption{Matrix \(\mat{B}\in\mathbb{R}^{n_1 \times n_2}\)}\label{fig:tnd:mat}
	\end{subfigure} \\
	
	\begin{subfigure}[t]{0.5\textwidth}
	\centering
	\begin{tikzpicture}
		\draw (0,0) node {}
			\foreach \x in {0,90,180}
			{
				(\x:1) -- (0,0)
			};
	\end{tikzpicture}
	\caption{Three-way tensor 
		\(\tensor{C}\in\mathbb{R}^{n_1\times n_2\times n_3}\)}\label{fig:tnd:3ten}
	\end{subfigure}
	\caption{Example tensor network diagrams.}
	\label{fig:tnegs}
\end{figure}

When \(\tensor{W}\) is a three-way tensor, we prefer a more compact notation
\begin{equation}\label{eq:bilintensor}
	\vec{z} = \tran{\vec{x}}\tensor{W}\vec{y}.
\end{equation} This loses none of the precision of the more verbose notation, provided we make clear
that we intend \(\vec{x}\) to operate along the first dimension of the tensor and \(\vec{y}\) the
third. That is to say, \((\tran{\vec{x}}\tensor{W})\vec{y}\) exactly corresponds with 
equation~\eqref{eq:bilinearkolda} while \(\tran{\vec{x}}(\tensor{W}\vec{y})\) corresponds to
equation~\eqref{eq:bilinearkolda2}. With either notation, for a tensor 
\(\tensor{W} \in \mathbb{R}^{n_1 \times n_2 \times n_3}\), we must have that 
\(\vec{x}\in \mathbb{R}^{n_1}\), \(\vec{y} \in \mathbb{R}^{n_3}\) and the result 
\(\vec{z}\in\mathbb{R}^{n_2}\).

An intuitive way to illustrate these ideas is using Tensor Network Diagrams
\autocite{Cichocki2016, Orus2014}. In these diagrams, each object is represented
as a circle, with each free `arm' representing an index used to address elements.
A vector therefore has one free arm, a matrix two and so on. Scalars will have
no arms. Figure~\ref{fig:tnegs} has examples for these simple objects. 

Where these diagrams are especially useful is for representing contractive products
where we sum over the range of a shared index. We represent this by joining the
respective arms. As an example, a matrix-vector product \(\vec{y} = \mat{Ax}\)
has such a contraction: \(y_{i} = \sum_{j}A_{ij}x_{j}\). This is shown in
figure~\ref{fig:tnmatvec} -- it is clear that there is only a single arm, so the
result is a vector as it should be. Figure~\ref{fig:tnprods} shows some examples
of these kinds of products.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{tikzpicture}
			\draw
				(1,0) node{} -- (0,0) node {} -- (-1,0);
		\end{tikzpicture}
		\caption{Matrix-vector product \(\mat{A}\vec{y}\), one free arm indicates
		 the result is a vector.}
		 \label{fig:tnmatvec}
	\end{subfigure} ~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{tikzpicture}
			\draw
				(1,0) node{} -- (0,0) node {} -- (-1,0) node{};
		\end{tikzpicture}
		\caption{Vector-matrix-vector bilinear form \(\tran{\vec{x}}\mat{A}
				 \vec{y}\). No free arms so the result is a scalar.}
	\end{subfigure}\\
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{tikzpicture}
			\draw (0,0) node {}
				\foreach \x in {0,90,180}
				{
					(\x:1) -- (0,0)
				}
				(0:1) node{};
		\end{tikzpicture}
		\caption{Tensor-vector product produces a matrix.}
	\end{subfigure} ~
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\begin{tikzpicture}
			\draw (0,0) node {}
				\foreach \x in {0,90,180}
				{
					(\x:1) -- (0,0)
				}
				(0:1) node{}
				(180:1) node{};
		\end{tikzpicture}
		\caption{Vector-tensor-vector product produces a vector.}
	\end{subfigure}
	\caption{Various products expressed as Tensor Network Diagrams.}
	\label{fig:tnprods}
\end{figure}

\section{Bilinear Products}
There are several ways to describe the operation performed by the bilinear products we are
concerned with. These correspond to different interpretations of the results. The following
interpretations provide insight both into what is actually being calculated and how the product might
be applicable to a neural network setting. In all of the below we use the above definitions of
\(\vec{x}, \vec{y}, \vec{z}\) and \(\tensor{W}\).

\subsection{Interpretations}
\subsubsection{Stacked bilinear forms}
If we consider the expression for a single element of \(\vec{z}\), we get
\begin{equation} \label{eq:singlezsum}
	z_j = \sum_i^{n_1} \sum_k^{n_3} W_{ijk} x_i y_k
\end{equation} as expected. We can re-write this in terms of the slices of \(\tensor{W}\):
\begin{equation}\label{eq:singlezslice}
	z_j = \tran{\vec{x}}\tensor{W}_{\cdot j \cdot}\vec{y}
\end{equation} which reveals the motivation behind the notation in equation~\eqref{eq:bilintensor}.
It also reveals that each element of \(\vec{z}\) is itself linear in \(\vec{x}\) or \(\vec{y}\) if
the other is held constant.

This provides an interpretation
in terms of similarities. If we consider the standard dot product of two vectors 
\(\vec{a}\) and
\(\vec{b}\) of size \(m\): 
\begin{equation}
\langle\vec{a}, \vec{b}\rangle = 
\vec{a}^\mathsf{T}\vec{b}
= \sum_{i=1}^ma_ib_i
 = {\cos\theta}{||\vec{a}||_2||\vec{b}||_2} 
\end{equation} where
\(\theta\) is the angle in the angle between the vectors. If the product
is positive, the two vectors are pointing in a similar direction and if it is negative they
are in opposite directions. If it is exactly zero, they must be orthogonal. The dot product
therefore provides us with some notion of the similarity between two vectors.
Indeed if we normalise the vectors by dividing each component by their \(l_2\) norm we
recover exactly the cosine similarity widely used, especially in information retrieval 
\autocite{Singhal2001, Tan2006} . Note that
we can generalise this idea by inserting a matrix of (potentially learned)
weights \(\mat{U}\) which enables us
to define general scalar bilinear forms
\begin{equation}
	\langle\vec{a}, \mat{U}\vec{b}\rangle = \langle\vec{a}^\mathsf{T}\mat{U}, \vec{b}\rangle
	= \vec{a}^\mathsf{T}\mat{U}\vec{b}.
\end{equation}

In a bilinear tensor product, each component of the result takes this form.
We can therefore think of the
product as computing a series of distinct similarity measures between the two input
vectors. With this in mind the obvious question is: what is the role of the matrix? The first
thing to note is that inserting a matrix into the inner product allows the two vectors to be
of different dimension. We also observe that a matrix-vector multiplication consists of
taking the dot product of the vector with each row or column of the matrix. Given our current
interpretation of the dot product as an un-normalised similarity measure, we can also
interpret a matrix-vector multiplication as computing the similarity of the vector with each
row or column of the matrix. 

We can then think of the rows of the matrix \(\mat{U}\) in the
above as containing patterns to look for in the \(\vec{b}\) vector and the columns to
contain patterns to test for in the \(\vec{a}\) vector. If we consider the vectors 
\(\vec{a}\) and \(\vec{b}\) to come from different feature spaces, the matrix \(\mat{U}\)
provides a conversion between them allowing us to directly compare the two. We can then
interpret each coordinate of the result of the bilinear tensor product as being an
independent similarity measure based on different interpretations of the underlying
feature space. In this sense, where a matrix multiplication looks for patterns in a single input
space, a bilinear product looks for \emph{joint} patterns in the combined input spaces of
\(\vec{x}\) and \(\vec{y}\).


\subsubsection{Choosing a matrix}
Following on from the above discussion we claim that for each coordinate of the output we are
computing a \emph{similarity vector} which we compare to the remaining input to generate a
scalar value. If we consider all coordinates at once, we see that this amounts to having
one input choose a matrix, which we then multiply by the remaining input vector. We have
refrained from making these points in terms of the specific vectors referenced above to make
the point that the operation is completely symmetrical. While it aids interpretation to think
of one vector choosing a matrix for the other vector, we can always achieve the same
intuition after switching the vectors, give or take some transposes.

This interpretation is very clear from the expression of the product in
equation~\ref{eq:bilinearkolda}. Simply by inserting parentheses we observe that we are
first generating a matrix in a way somehow dependent on the first input and multiplying
the second input by that matrix. In this sense we allow one input to choose patterns to look for
in the other input.

This intuition of choosing a matrix is suggested in \autocite{Sutskever2013} in the
context language modelling with RNNs. It is suggested that allowing the current input
character to choose the hidden-to-hidden weights matrix should confer benefits. 
This intuition (and the factorisation of the implicit tensor) was put to use earlier in the context
of Conditional Restricted Boltzmann Machines \autocite{Taylor} which actively seek to model the
conditional dependencies between two types of input.

Although this provides a powerful insight into the bilinear product, it is worth reinforcing that
the product is entirely symmetrical. We can not think purely about it as \(\vec{x}\) choosing a matrix
for \(\vec{y}\) as the converse is equally true.

\subsubsection{Tensor as an independent basis}
Extending the above to try and capture the symmetricity of the operation, we introduce
the notion that the coefficients of the tensor represent a basis in which to compare the
two inputs, independent of both of them. This idea is mentioned in
 \autocite{Tenenbaum2000} which considered the problem of data that can be described by two
independent factors.
The tensor then contains a basis which characterises the interaction by which the factors
in the input vectors combine to create an output observation.

This is a somewhat abstract interpretation -- to attempt to create a more concrete
 intuition
consider the case when both vectors have a single element set to \(1\) and the remainder
\(0\). In this case the first tensor-vector product corresponds to taking a slice of the
tensor, resulting in a matrix. The final matrix-vector product corresponds to picking a
row or column of the matrix. Consequently the whole operation is precisely looking up
a fibre of the tensor. To generalise from such a ``one-hot'' encoding of the inputs
to vectors of real coefficients, we simply replace the idea of a
\emph{lookup} with that of a \emph{linear combination}. The vectors then represent the coefficients
of weighted sums; first over slices and then over rows or columns. The final product is then a
distinct representation of both vectors in terms of the independent basis expressed by the tensor.

\subsubsection{Operation on the outer product}
n this section we describe the bilinear tensor product as operating on pairwise
products of inputs. This interpretation is essential for understanding the expressive
power of the operation as it gives rise to an obvious XOR-like behaviour. It also serves
as a useful reminder that a bilinear form is linear in each input only when the other
is held constant -- when both are allowed to vary we can represent complex non-linear
relations.
A way to approach this interpretation arises from a method of implementing the bilinear 
product in terms of straightforward matrix operations.

To discuss
this we introduce the notion of the \emph{matricisation} of a tensor. Intuitively this
operation is a way of \emph{unfolding} or \emph{flattening} a tensor into a matrix,
preserving its elements. Specifically the mode-\(n\)
matricisation of a tensor is defined as an operation which takes all mode-\(n\) fibres
of the tensor and places them as columns to create a matrix. We denote an mode-\(n\)
matricisation of a tensor \(\tensor{W}\) as \(\matr_n(\tensor{W})\). While the operation is
fairly straightforward, describing the exact permutation of the indices is awkard -- 
for a robust treatment of the general case (and the source of the above definition) see
\autocite{Kolda2009}. 

Although this notion captures and generalises the vectorisation operator often
encountered in linear algebra, we retain the classical \(\vect\) operator for clarity.
This flattens a matrix into a vector by stacking its columns. For some matrix \(\mat{A}\) with
\(n\) columns:
\begin{equation}\label{eq:vec}
	\vect(\mat{A}) = \begin{bmatrix}
		\vec{A}_{\midbullet 1}\\
		\vec{A}_{\midbullet 2}\\
		\vdots\\
		\vec{A}_{\midbullet n}
	\end{bmatrix}.
\end{equation}

For our purposes is is sufficient to note that a 
mode-2 matricisation of a
three-way tensor \(\tensor{W} \in \mathbb{R}^{n_1\times n_2\times n_3}\) must have shape
\(n_2 \times n_1n_3\). This gives us the following lemma, which shifts this line of
thinking slightly sideways.

\begin{lem}[Matricisation/vectorisation]
The \(j\)-th row of the mode-2 matricisation of the three-way tensor \(\tensor{W}\)
is equivalent to the vectorisation of the slice formed by fixing the second index at \(j\):
\begin{equation}\label{eq:matlemstatement}
	\matr_2 (\tensor{W})_{j \midbullet} = \vect(\mat{W}_{\midbullet j \midbullet}).
\end{equation}
\label{lem:matricise}
\end{lem}
\begin{proof}
By the above definition of the vectorisation operator, each index \((i, k)\) in some matrix
\(\mat{U} \in \mathbb{R}^{n_1\times n_3}\) maps to element \(i + (k-1)n_1\) in
\(\vect(\mat{U})\). By the definition of the mode-2 matricisation we would expect to
find tensor element \(W_{ijk}\) at index \((j, i + (k-1)n_3)\). Hence if we fix \(j\), we have
precisely the vectorisation of the \(j\)-th slice of the tensor.

The indices into \(\matr_2(\tensor{W})\) can be though of as arising from the following construction
process: first fix all indices to 1. Construct a column by sweeping the second index, \(j\),
through its full range. Then increment the first index \(i\) and repeat the procedure, placing
the generated columns with index \(i\). Only when \(i\) has swept through its full range
increment the final index \(k\) and repeat the procedure. The generated columns should then be
at positions \(i + (k-1)n_1\).
\end{proof}

These flattenings are important as they allow us to
implement many operations involving tensors in terms of a small number of larger matrix
operations when compared to the naive approach.

\begin{lem}[Matricised product]\label{lem:outerprod}
For a tensor \(\tensor{W}\) and vectors \(\vec{x}, \vec{y}\) as above,
we can describe the product \(\vec{z} = \tran{\vec{x}}\tensor{W}\vec{y}\) in terms of the
mode-2 matricisation of \(\tensor{W}\) as 
follows:
\begin{align}
	\vec{z} = \matr_2(\tensor{W})\mathrm{vec}\left[\vec{y}\tran{\vec{x}}\right]
	\label{eq:bilinearouter}
\end{align}
\end{lem}

\begin{proof}
To
prove this we can compare the expressions for a single element of the result.
An element \(z_j\) from equation~\ref{eq:bilinearouter}
is formed as the inner product of the \(j\)-th
row of the flattened tensor and the vectorised outer product of the inputs. By 
lemma~\ref{lem:matricise}:
\begin{equation}
	z_j = 
	\sum_{s=1}^{n_1n_3} (\matr(\tensor{W})_{js} 
	\left( \vect\left[\vec{y}\vec{x}^\mathsf{T}\right]\right)_s
\end{equation}
We replace the sum over \(s\) with a sum over two
indices, \(i\) and \(k\), and using them to appropriately re-index the flattenings 
as described in lemma~\ref{lem:matricise} we 
derive
\begin{align}
	z_j &= \sum_{i=1}^{n_1}\sum_{k=1}^{n_3} W_{ijk} x_i y_k \\
		&= \vec{x}^\mathsf{T}\mat{W}_{\midbullet j \midbullet}\vec{y}
\end{align}

\end{proof}


Therefore to understand the bilinear product it helps to understand the matrix 
\(\vec{y}\vec{x}^\mathsf{T}\). Each element is of the following form:
\[
	(\vec{y}\vec{x}^\mathsf{T})_{ki} = x_iy_k,
\] it contains all possible products of pairs of elements, one from each vector. 
This captures some interesting interactions.

\begin{thm} [Bilinear exclusive-or] \label{thm:xor}
Bilinear tensor products operate on the pair-wise exclusive-or of the
signs of the inputs.
\end{thm}
\begin{proof}
Consider the sign of a scalar product
\(c = a\cdot b\). If one of the operands \(a\) or \(b\) is positive and the other negative,
then the sign of \(c\) is negative. If both are positive or both are negative, then the
result is positive. This captures precisely the ``one or the other but not both'' structure
of the exclusive-or operation.

By lemma~\ref{lem:outerprod} a bilinear product can be viewed as a matrix operation on the flattened
outer product of the two inputs. As each element in the outer product is the product of two scalars,
the signs have an exclusive-or structure.
\end{proof}

\begin{cor}[Bilinear conjunctions]\label{cor:and}
If the inputs are binary, bilinear products operate on pair-wise conjunctions of the inputs.
\end{cor}
\begin{proof}
If \(a, b \in \{0, 1\}\), then \(a \cdot b = 1\) if and only if both \(a\) and \(b\) are \(1\). If
either or both are \(0\) then their product must be zero. Following the same structure as the proof
of theorem~\ref{thm:xor}, for binary inputs we must have this conjunctive relationship.
\end{proof}

There are a number of remarks worth making about these results. Firstly they suggest considering
the case where both inputs are the same: \(\vec{z} = \tran{\vec{x}}\tensor{W}\vec{x}\). Replacing
bilinear forms with quadratic forms may invalidate some of the similarity based interpretations,
but it might also provide an interesting class of very powerful feed-forward networks. As an example,
note that a plain perceptron has been long known to be incapable of learning the exclusive-or
mapping \autocite{Minsky1969} -- a neural network to solve the problem requires either a hidden layer
or an additional input feature (specifically the conjunction of the inputs) \autocite{Rumelhart1986}.
By corollary~\ref{cor:and} this quadratic tensor form would implicitly and naturally capture the
additional conjunctive feature and be capable of solving the exclusive or problem without hidden
layers or hand-engineered features.

\section{Tensor Decompositions}
\subsection{CANDECOMP/PARAFAC}
\subsection{Tensor Train, Tucker}

\section{Learning decompositions by gradient descent}
Multiplicative dynamics = instability?