Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Gal2015,
abstract = {A long strand of empirical research has claimed that dropout cannot be applied between the recurrent connections of a recurrent neural network (RNN). The reasoning has been that the noise hinders the network's ability to model sequences, and instead should be applied to the RNN's inputs and outputs alone. But dropout is a vital tool for regularisation, and without dropout in recurrent layers our models overfit quickly. In this paper we show that a recently developed theoretical framework, casting dropout as approximate Bayesian inference, can give us mathematically grounded tools to apply dropout within the recurrent layers. We apply our new dropout technique in long short-term memory (LSTM) networks and show that the new approach significantly outperforms existing techniques.},
archivePrefix = {arXiv},
arxivId = {1512.05287},
author = {Gal, Yarin},
eprint = {1512.05287},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Gal - Unknown - A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.pdf:pdf},
isbn = {9789537619084},
issn = {0302-9743},
journal = {arXiv preprint},
title = {{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1512.05287},
year = {2015}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural$\backslash$rnetwork with a small central layer to reconstruct high-dimensional input vectors. Gradient descent$\backslash$r$\backslash$ncan be used for fine-tuning the weights in such ‘‘autoencoder'' networks, but this works well only if$\backslash$r$\backslash$nthe initial weights are close to a good solution. We describe an effective way of initializing the$\backslash$r$\backslash$nweights that allows deep autoencoder networks to learn low-dimensional codes that work much$\backslash$r$\backslash$nbetter than principal components analysis as a tool to reduce the dimensionality of data.$\backslash$r$\backslash$n},
archivePrefix = {arXiv},
arxivId = {20},
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
doi = {10.1126/science.1127647},
eprint = {20},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Hinton, Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Networks.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {1095-9203},
journal = {Science},
number = {5786},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
volume = {313},
year = {2006}
}
@article{Breiman1997,
abstract = {Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman[1996]. The first successful arcing procedure was introduced by Freund and Schapire[1995,1996] and called Adaboost. In an effort to explain why Adaboost works, Schapire et.al. [1997] derived a bound on the generalization error of a convex combination of classifiers in terms of the margin. We introduce a function called the edge, which differs from the margin only if there are more than two classes. A framework for understanding arcing algorithms is defined. In this framework, we see that the arcing algorithms currently in the literature are optimization algorithms which minimize some function of the edge. A relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner. Two algorithms are described which achieve the optimal reduction. Tests on both synthetic and real data cast doubt on the Schapire et.al. explanation.},
author = {Breiman, Leo},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - Unknown - ARCING THE EDGE.pdf:pdf},
journal = {Statistics},
pages = {1--14},
title = {{Arcing the edge}},
volume = {4},
year = {1997}
}
@inproceedings{Jozefowicz2015,
abstract = {The Recurrent Neural Network (RNN) is an ex-tremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's archi-tecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thor-ough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.},
author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
booktitle = {ICML},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Jozefowicz, Zaremba, Sutskever - Unknown - An Empirical Exploration of Recurrent Network Architectures.pdf:pdf},
title = {{An Empirical Exploration of Recurrent Network Architectures}},
year = {2015}
}
@article{Friedman2002,
abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current “pseudo”-residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner.},
author = {Friedman, Jerome H.},
doi = {10.1016/S0167-9473(01)00065-2},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Friedman - 2002 - Stochastic gradient boosting.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
number = {4},
pages = {367--378},
title = {{Stochastic gradient boosting}},
volume = {38},
year = {2002}
}
@article{Hitchcock1928,
author = {Hitchcock, Frank L},
doi = {10.1002/sapm19287139},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Hitchcock - 1928 - Multiple Invariants and Generalized Rank of a P-Way Matrix or Tensor.pdf:pdf},
issn = {00971421},
journal = {Journal of Mathematics and Physics},
month = {apr},
number = {1-4},
pages = {39--79},
title = {{Multiple Invariants and Generalized Rank of a P-Way Matrix or Tensor}},
url = {http://doi.wiley.com/10.1002/sapm19287139},
volume = {7},
year = {1928}
}
@article{Goodfellow2013,
abstract = {We consider the problem of designing mod-els to leverage a recently introduced ap-proximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a nat-ural companion to dropout) designed to both facilitate optimization by dropout and im-prove the accuracy of dropout's fast approxi-mate model averaging technique. We empir-ically verify that the model successfully ac-complishes both of these tasks. We use max-out and dropout to demonstrate state of the art classification performance on four bench-mark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
archivePrefix = {arXiv},
arxivId = {1302.4389},
author = {Goodfellow, Ian J and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
eprint = {1302.4389},
file = {:Users/pfcmathews/Downloads/1302.4389v4.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
pages = {1319--1327},
title = {{Maxout Networks}},
volume = {28},
year = {2013}
}
@article{Rezende2014,
abstract = {Abstract We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep , directed generative models , endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition ... $\backslash$n},
author = {Rezende, D J and Mohamed, S and Wierstra, D},
journal = {Proceedings of The 31st {\ldots}},
pages = {1278--1286},
title = {{Stochastic backpropagation and approximate inference in deep generative models}},
url = {http://jmlr.org/proceedings/papers/v32/rezende14.html$\backslash$npapers3://publication/uuid/F2747569-7719-4EAC-A5A7-9ECA9D6A8FE6},
volume = {32},
year = {2014}
}
@article{Cichocki2016,
abstract = {Machine learning and data mining algorithms are becoming increasingly important in analyzing large volume, multi-relational and multi--modal datasets, which are often conveniently represented as multiway arrays or tensors. It is therefore timely and valuable for the multidisciplinary research community to review tensor decompositions and tensor networks as emerging tools for large-scale data analysis and data mining. We provide the mathematical and graphical representations and interpretation of tensor networks, with the main focus on the Tucker and Tensor Train (TT) decompositions and their extensions or generalizations. Keywords: Tensor networks, Function-related tensors, CP decomposition, Tucker models, tensor train (TT) decompositions, matrix product states (MPS), matrix product operators (MPO), basic tensor operations, multiway component analysis, multilinear blind source separation, tensor completion, linear/multilinear dimensionality reduction, large-scale optimization problems, symmetric eigenvalue decomposition (EVD), PCA/SVD, huge systems of linear equations, pseudo-inverse of very large matrices, Lasso and Canonical Correlation Analysis (CCA) (This is Part 1)},
archivePrefix = {arXiv},
arxivId = {1609.00893},
author = {Cichocki, A. and Lee, N. and Oseledets, I. V. and Phan, A-H. and Zhao, Q. and Mandic, D.},
eprint = {1609.00893},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Cichocki et al. - 2016 - Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale Optimization Problems Perspectives and Ch.pdf:pdf},
journal = {arXiv preprint},
pages = {100},
title = {{Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale Optimization Problems: Perspectives and Challenges PART 1}},
url = {http://arxiv.org/abs/1609.00893},
year = {2016}
}
@article{VanderMaaten2008,
abstract = {We present a new technique called " t-SNE " that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.},
author = {van der Maaten, Laurens and Hinton, Geoffrey},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/van der Maaten LVANDERMAATEN, Hinton - 2008 - Visualizing Data using t-SNE.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Visualization,dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling},
pages = {1--48},
title = {{Visualizing Data using t-SNE}},
volume = {1},
year = {2008}
}
@article{Memisevic2010,
abstract = {To allow the hidden units of a restricted Boltzmann machine to model the transformation between two successive images, Memisevic and Hinton (2007) introduced three-way multiplicative interactions that use the intensity of a pixel in the first image as a multiplicative gain on a learned, symmetric weight between a pixel in the second image and a hidden unit. This creates cubically many parameters, which form a three-dimensional interaction tensor. We describe a low-rank approximation to this interaction tensor that uses a sum of factors, each of which is a three-way outer product. This approximation allows efficient learning of transformations between larger image patches. Since each factor can be viewed as an image filter, the model as a whole learns optimal filter pairs for efficiently representing transformations. We demonstrate the learning of optimal filter pairs from various synthetic and real image sequences. We also show how learning about image transformations allows the model to perform a simple visual analogy task, and we show how a completely unsupervised network trained on transformations perceives multiple motions of transparent dot patterns in the same way as humans.},
author = {Memisevic, Roland and Hinton, Geoffrey E},
doi = {10.1162/neco.2010.01-09-953},
file = {:Users/pfcmathews/Downloads/UTML-TR-2009-003.pdf:pdf},
isbn = {1530-888X (Electronic)$\backslash$r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {6},
pages = {1473--1492},
pmid = {20141471},
title = {{Learning to represent spatial transformations with factored higher-order Boltzmann machines.}},
volume = {22},
year = {2010}
}
@article{Larsson2016,
abstract = {We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a single expansion rule generates an extremely deep network whose structural layout is precisely a truncated fractal. Such a network contains interacting subpaths of different lengths, but does not include any pass-through connections: every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. This property stands in stark contrast to the current approach of explicitly structuring very deep networks so that training is a residual learning problem. Our experiments demonstrate that residual representation is not fundamental to the success of extremely deep convolutional neural networks. A fractal design achieves an error rate of 22.85{\%} on CIFAR-100, matching the state-of-the-art held by residual networks. Fractal networks exhibit intriguing properties beyond their high performance. They can be regarded as a computationally efficient implicit union of subnetworks of every depth. We explore consequences for training, touching upon connection with student-teacher behavior, and, most importantly, demonstrating the ability to extract high-performance fixed-depth subnetworks. To facilitate this latter task, we develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. With such regularization, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.},
archivePrefix = {arXiv},
arxivId = {1605.07648},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
eprint = {1605.07648},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Larsson, Maire, Shakhnarovich - 2016 - FractalNet Ultra-Deep Neural Networks without Residuals(2).pdf:pdf},
journal = {arXiv preprint},
month = {may},
title = {{FractalNet: Ultra-Deep Neural Networks without Residuals}},
url = {http://arxiv.org/abs/1605.07648},
year = {2016}
}
@phdthesis{Sutskever2013,
abstract = {Recurrent Neural Networks (RNNs) are powerful sequence models that were believed to be difficult to train, and as a result they were rarely used in machine learning applications. This thesis presents methods that overcome the difficulty of training RNNs, and applications of RNNs to challenging problems. We first describe a newprobabilistic sequence model that combines Restricted Boltzmann Machines and RNNs. The new model is more powerful than similar models while being less difficult to train. Next, we present a new variant of the Hessian-free (HF) optimizer and show that it can train RNNs on tasks that have extreme long-range temporal dependencies, which were previously considered to be impossibly hard. We then apply HF to character-level language modelling and get excellent results. We also apply HF to optimal control and obtain RNN control laws that can successfully operate under conditions of delayed feedback and unknown disturbances. Finally, we describe a random parameter initialization scheme that allows gradient descent with mo- mentum to train RNNs on problems with long-term dependencies. This directly contradicts widespread beliefs about the inability of first-order methods to do so, and suggests that previous attempts at training RNNs failed partly due to flaws in the random initialization},
archivePrefix = {arXiv},
arxivId = {arXiv:submit/1456339},
author = {Sutskever, Ilya},
booktitle = {PhD thesis},
eprint = {1456339},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever - Unknown - TRAINING RECURRENT NEURAL NETWORKS.pdf:pdf},
isbn = {978-0-499-22066-0},
pages = {101},
primaryClass = {arXiv:submit},
title = {{Training Recurrent neural Networks}},
year = {2013}
}
@article{Vincent2010,
abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1111/1467-8535.00290},
eprint = {0-387-31073-8},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Vincent et al. - 2010 - Stacked Denoising Autoencoders Learning Useful Representations in a Deep Network with a Local Denoising Criterio.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
number = {3},
pages = {3371--3408},
pmid = {17348934},
title = {{Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion}},
volume = {11},
year = {2010}
}
@article{Sussillo2014,
abstract = {Training very deep networks is an important open problem in machine learning. One of many difficulties is that the norm of the back-propagated error gradient can grow or decay exponentially. Here we show that training very deep feed-forward networks (FFNs) is not as difficult as previously thought. Unlike when back-propagation is applied to a recurrent network, application to an FFN amounts to multiplying the error gradient by a different random matrix at each layer. We show that the successive application of correctly scaled random matrices to an initial vector results in a random walk of the log of the norm of the resulting vectors, and we compute the scaling that makes this walk unbiased. The variance of the random walk grows only linearly with network depth and is inversely proportional to the size of each layer. Practically, this implies a gradient whose log-norm scales with the square root of the network depth and shows that the vanishing gradient problem can be mitigated by increasing the width of the layers. Mathematical analyses and experimental results using stochastic gradient descent to optimize tasks related to the MNIST and TIMIT datasets are provided to support these claims. Equations for the optimal matrix scaling are provided for the linear and ReLU cases.},
archivePrefix = {arXiv},
arxivId = {1412.6558},
author = {Sussillo, David and Abbott, L. F.},
eprint = {1412.6558},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Sussillo, Abbott - 2014 - Random Walk Initialization for Training Very Deep Feedforward Networks.pdf:pdf},
journal = {arXiv},
pages = {10},
title = {{Random Walk Initialization for Training Very Deep Feedforward Networks}},
url = {http://arxiv.org/abs/1412.6558},
year = {2014}
}
@article{VandenOord2016,
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder, creating . Additionally, the gated convolutional layers in the proposed model improve the loglikelihood of PixelCNN to match the stateoftheart performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
archivePrefix = {arXiv},
arxivId = {1606.05328},
author = {van den Oord, A{\"{a}}ron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1606.05328},
file = {:Users/pfcmathews/Downloads/1606.05328v2.pdf:pdf},
journal = {arXiv},
title = {{Conditional Image Generation with PixelCNN Decoders}},
year = {2016}
}
@article{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
doi = {10.1.1.208.6449},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - Unknown - Deep Sparse Rectifier Neural Networks.pdf:pdf},
issn = {15324435},
journal = {AISTATS},
pages = {315--323},
title = {{Deep Sparse Rectifier Neural Networks}},
volume = {15},
year = {2011}
}
@article{Dean2004,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.},
archivePrefix = {arXiv},
arxivId = {10.1.1.163.5292},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
eprint = {10.1.1.163.5292},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Dean, Ghemawat - Unknown - MapReduce Simplified Data Processing on Large Clusters.pdf:pdf},
isbn = {9781595936868},
issn = {00010782},
journal = {Proceedings of 6th Symposium on Operating Systems Design and Implementation},
pages = {137--149},
pmid = {11687618},
title = {{MapReduce: Simplied Data Processing on Large Clusters}},
year = {2004}
}
@article{Le2015,
abstract = {Learning long term dependencies in recurrent networks is difficult due to van-ishing and exploding gradients. To overcome this difficulty, researchers have de-veloped sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to a standard implementation of LSTMs on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00941v1},
author = {Le, Quoc V. and Jaitly, Navdeep and Hinton, Geoffrey E.},
doi = {10.1109/72.279181},
eprint = {arXiv:1504.00941v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Le, Jaitly, Hinton - 2015 - A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.pdf:pdf},
isbn = {9781461268758},
issn = {1045-9227},
journal = {arXiv preprint arXiv:1504.00941},
pages = {1--9},
pmid = {17756722},
title = {{A Simple Way to Initialize Recurrent Networks of Rectified Linear Units}},
year = {2015}
}
@article{Henaff2016,
abstract = {Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.},
archivePrefix = {arXiv},
arxivId = {1602.06662},
author = {Henaff, Mikael and Szlam, Arthur and LeCun, Yann},
eprint = {1602.06662},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - henaff - orthogonal RNNs and long-memory tasks.pdf:pdf},
journal = {arxiv},
title = {{Orthogonal RNNs and Long-Memory Tasks}},
url = {http://arxiv.org/abs/1602.06662},
year = {2016}
}
@article{Duvenaud2014,
archivePrefix = {arXiv},
arxivId = {1402.5836},
author = {Duvenaud, David and Rippel, Oren and Adams, Ryan P. and Ghahramani, Zoubin},
eprint = {1402.5836},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Duvenaud et al. - 2014 - Avoiding pathologies in very deep networks.pdf:pdf},
issn = {15337928},
journal = {AISTATS},
month = {feb},
pages = {202----210},
title = {{Avoiding pathologies in very deep networks}},
url = {http://arxiv.org/abs/1402.5836},
year = {2014}
}
@inproceedings{Telgarsky2016,
abstract = {For any positive integer {\$}k{\$}, there exist neural networks with {\$}\backslashTheta(k{\^{}}3){\$} layers, {\$}\backslashTheta(1){\$} nodes per layer, and {\$}\backslashTheta(1){\$} distinct parameters which can not be approximated by networks with {\$}\backslashmathcal{\{}O{\}}(k){\$} layers unless they are exponentially large --- they must possess {\$}\backslashOmega(2{\^{}}k){\$} nodes. This result is proved here for a class of nodes termed "semi-algebraic gates" which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, and boosted decision trees (in this last case with a stronger separation: {\$}\backslashOmega(2{\^{}}{\{}k{\^{}}3{\}}){\$} total tree nodes are required).},
archivePrefix = {arXiv},
arxivId = {1602.04485},
author = {Telgarsky, Matus},
booktitle = {29th Annual Conference on Learning Theory},
eprint = {1602.04485},
file = {:Users/pfcmathews/Downloads/1602.04485v2.pdf:pdf},
keywords = {1,a model of real-valued,a neural network is,approximation,as follows,computation defined by a,connected directed graph,depth hierarchy,neural networks,nodes await real numbers,on their incoming edges,representation,setting and main results,thereafter computing a function},
number = {1},
pages = {1--19},
title = {{Benefits of Depth In Neural Networks}},
url = {http://arxiv.org/abs/1602.04485},
volume = {49},
year = {2016}
}
@article{Lorenz1963,
author = {Lorenz, Edward N.},
doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
issn = {0022-4928},
journal = {Journal of the Atmospheric Sciences},
month = {mar},
number = {2},
pages = {130--141},
title = {{Deterministic Nonperiodic Flow}},
url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0469{\%}281963{\%}29020{\%}3C0130{\%}3ADNF{\%}3E2.0.CO{\%}3B2},
volume = {20},
year = {1963}
}
@article{Novikov2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.03795v1},
author = {Novikov, Alexander and Trofimov, Mikhail and Oseledets, Ivan},
eprint = {arXiv:1605.03795v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Novikov, Trofimov, Oseledets - 2016 - Tensor Train polynomial models via Riemannian optimization.pdf:pdf},
journal = {arXiv},
title = {{Tensor Train polynomial models via Riemannian optimization}},
year = {2016}
}
@article{Jaeger2004,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jaeger, H.},
doi = {10.1126/science.1091277},
eprint = {arXiv:1011.1669v3},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Jaeger - 2004 - Harnessing Nonlinearity Predicting Chaotic Systems and Saving Energy in Wireless Communication.pdf:pdf},
isbn = {9788578110796},
issn = {0036-8075},
journal = {Science},
number = {5667},
pages = {78--80},
pmid = {15064413},
title = {{Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1091277},
volume = {304},
year = {2004}
}
@article{Choi2016,
abstract = {We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with two CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
archivePrefix = {arXiv},
arxivId = {1609.04243},
author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark and Cho, Kyunghyun},
eprint = {1609.04243},
journal = {arXiv preprint},
month = {sep},
title = {{Convolutional Recurrent Neural Networks for Music Classification}},
url = {http://arxiv.org/abs/1609.04243},
year = {2016}
}
@article{Ho1998,
abstract = {Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy},
author = {Ho, Tin Kam},
doi = {10.1109/34.709601},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Ho - 1998 - The Random Subspace Method for Constructing Decision Forest.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {832--844},
title = {{The Random Subspace Method for Constructing Decision Forest}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=709601 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=709601},
volume = {20},
year = {1998}
}
@article{Hornik1989,
author = {Hornik, K and Stinchcombe, M and White, H},
file = {:Users/pfcmathews/Downloads/Kornick{\_}et{\_}al.pdf:pdf},
journal = {Neural Networks},
pages = {359--366},
title = {{Multilayer Feedforward Networks Are Universal Function Approximators}},
volume = {2},
year = {1989}
}
@article{Carroll1970,
abstract = {An individual differences model for multidimensional scaling is outlined in which individuals are assumed differentially to weight the several dimensions of a common “psychological space”. A corresponding method of analyzing similarities data is proposed, involving a generalization of “Eckart-Young analysis” to decomposition of three-way (or higher-way) tables. In the present case this decomposition is applied to a derived three-way table of scalar products between stimuli for individuals. This analysis yields a stimulus by dimensions coordinate matrix and a subjects by dimensions matrix of weights. This method is illustrated with data on auditory stimuli and on perception of nations.},
author = {Carroll, J. Douglas and Chang, Jih Jie},
doi = {10.1007/BF02310791},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Carroll, Chang - 1970 - Analysis of individual differences in multidimensional scaling via an n-way generalization of “Eckart-Young”.pdf:pdf},
isbn = {0033-3123},
issn = {00333123},
journal = {Psychometrika},
month = {sep},
number = {3},
pages = {283--319},
publisher = {Springer-Verlag},
title = {{Analysis of individual differences in multidimensional scaling via an n-way generalization of "Eckart-Young" decomposition}},
url = {http://link.springer.com/10.1007/BF02310791},
volume = {35},
year = {1970}
}
@incollection{Hochreiter2001,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"{u}}rgen},
booktitle = {A Field Guide to Dynamical Recurrent Networks},
chapter = {Gradient F},
doi = {10.1109/9780470544037.ch14},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-7803-5369-5},
issn = {1098-6596},
pmid = {25246403},
publisher = {IEEE Press},
title = {{Gradient Flow in Recurrent Nets: The Difficulty of Learning LongTerm Dependencies}},
url = {http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=5264952},
year = {2001}
}
@article{Kurach2016,
abstract = {In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions.},
archivePrefix = {arXiv},
arxivId = {1511.06392},
author = {Kurach, Karol and Andrychowicz, Marcin and Sutskever, Ilya},
eprint = {1511.06392},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Kurach, Andrychowicz, Sutskever - 2015 - Neural Random-Access Machines.pdf:pdf},
journal = {ICLR},
pages = {17},
title = {{Neural Random-Access Machines}},
url = {http://arxiv.org/abs/1511.06392},
year = {2016}
}
@article{Thivierge2007,
abstract = {A hallmark feature of vertebrate brain organization is ordered topography, wherein sets of neuronal connections preserve the relative organization of cells between two regions. Although topography is often found in projections from peripheral sense organs to the brain, it also seems to participate in the anatomical and functional organization of higher brain centers, for reasons that are poorly understood. We propose that a key function of topography might be to provide computational underpinnings for precise one-to-one correspondences between abstract cognitive representations. This perspective offers a novel conceptualization of how the brain approaches difficult problems, such as reasoning and analogy making, and suggests that a broader understanding of topographic maps could be pivotal in fostering strong links between genetics, neurophysiology and cognition. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Thivierge, Jean Philippe and Marcus, Gary F.},
doi = {10.1016/j.tins.2007.04.004},
file = {:Users/pfcmathews/Downloads/thivierge marcus tins 2007.pdf:pdf},
isbn = {0166-2236 (Print)},
issn = {01662236},
journal = {Trends in Neurosciences},
number = {6},
pages = {251--259},
pmid = {17462748},
title = {{The topographic brain: from neural connectivity to cognition}},
volume = {30},
year = {2007}
}
@article{Martens2011,
abstract = {In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-the-art method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of sch (2002) which is used within the HF approach of Martens. Copyright 2011 by the author(s)/owner(s).},
author = {Martens, James and Sutskever, Ilya},
doi = {10.1145/346152.346166},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Martens, Sutskever - Unknown - Learning Recurrent Neural Networks with Hessian-Free Optimization.pdf:pdf},
isbn = {978-1-4503-0619-5},
issn = {01635980},
journal = {Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
keywords = {Learning systems,Optimization},
pages = {1033--1040},
title = {{Learning recurrent neural networks with Hessian-free optimization}},
year = {2011}
}
@article{Sutskever2013a,
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
author = {Sutskever, Ilya and Martens, James and Dahl, George E and Hinton, Geoffrey E},
doi = {10.1109/ICASSP.2013.6639346},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever et al. - 2013 - On the importance of initialization and momentum in deep learning.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {15206149},
journal = {Jmlr},
keywords = {dblp},
number = {2010},
pages = {1139--1147},
title = {{On the importance of initialization and momentum in deep learning}},
url = {http://dblp.uni-trier.de/db/conf/icml/icml2013.html{\#}SutskeverMDH13},
volume = {28},
year = {2013}
}
@article{Mansimov2015,
abstract = {Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.},
archivePrefix = {arXiv},
arxivId = {1511.02793},
author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
doi = {10.1088/0004-6256/150/6/203},
eprint = {1511.02793},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mansimov et al. - 2015 - Generating Images from Captions with Attention.pdf:pdf},
journal = {arXiv preprint},
pages = {1--12},
title = {{Generating Images from Captions with Attention}},
url = {http://arxiv.org/abs/1511.02793},
year = {2015}
}
@article{Chan2015,
abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1{\%} without a dictionary or a language model, and 10.3{\%} with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0{\%}.},
archivePrefix = {arXiv},
arxivId = {1508.01211},
author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
doi = {10.1109/72.279181},
eprint = {1508.01211},
file = {:Users/pfcmathews/Downloads/1508.01211.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {arXiv preprint},
pages = {1--16},
pmid = {18267787},
title = {{Listen, attend and spell}},
url = {http://arxiv.org/abs/1508.01211},
year = {2015}
}
@article{Graves,
abstract = {Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.},
author = {Graves, Alex},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Graves - Unknown - Practical Variational Inference for Neural Networks.pdf:pdf},
isbn = {9781618395993},
journal = {Nips},
pages = {1--9},
title = {{Practical Variational Inference for Neural Networks.}},
url = {https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf},
year = {2011}
}
@inproceedings{Freund1996,
author = {Freund, Yoav and Schapire, Robert E},
booktitle = {Thirteenth International Conference on Machine Learning},
doi = {10.1.1.133.1040},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Freund, Schapire - 1996 - Experiments with a new boosting algorithm.pdf:pdf},
isbn = {1558604197},
issn = {0706-652X, 1205-7533},
pages = {148--156},
title = {{Experiments with a new boosting algorithm}},
url = {http://www.public.asu.edu/{~}jye02/CLASSES/Fall-2005/PAPERS/boosting-icml.pdf},
year = {1996}
}
@article{Courbariaux2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.02830v2},
author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
eprint = {arXiv:1602.02830v2},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Courbariaux et al. - 2016 - Binarized Neural Networks Training Neural Networks with Weights and Activations Constrained to {\$}1{\$} or {\$}-1{\$}.pdf:pdf},
journal = {arXiv:1602.02830},
pages = {1--29},
title = {{Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to {\$}+1{\$} or {\$}-1{\$}}},
year = {2016}
}
@article{Ahmed2014,
abstract = {Biomarker identification, i.e., detecting the features that indicate differences between two or more classes, is an important task in omics sciences. Mass spectrometry (MS) provide a high throughput analysis of proteomic and metabolomic data. The number of features of the MS data sets far exceeds the number of samples, making biomarker identification extremely difficult. Feature construction can provide a means for solving this problem by transforming the original features to a smaller number of high-level features. This paper investigates the construction of multiple features using genetic programming (GP) for biomarker identification and classification of mass spectrometry data. In this paper, multiple features are constructed using GP by adopting an embedded approach in which Fisher criterion and p-values are used to measure the discriminating information between the classes. This produces nonlinear high-level features from the low-level features for both binary and multi-class mass spectrometry data sets. Meanwhile, seven different classifiers are used to test the effectiveness of the constructed features. The proposed GP method is tested on eight different mass spectrometry data sets. The results show that the high-level features constructed by the GP method are effective in improving the classification performance in most cases over the original set of features and the low-level selected features. In addition, the new method shows superior performance in terms of biomarker detection rate.},
author = {Ahmed, Soha and Zhang, Mengjie and Peng, Lifeng and Xue, Bing},
doi = {doi:10.1145/2576768.2598292},
file = {:Users/pfcmathews/Downloads/p249.pdf:pdf},
isbn = {9781450326629},
pages = {249--256},
title = {{Multiple Feature Construction for Effective Biomarker Identification and Classification using Genetic Programming}},
year = {2014}
}
@article{Hopfield1982,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
author = {Hopfield, J J},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Hopfield - 1982 - Neural networks and physical systems with emergent collective computational abilities.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {apr},
number = {8},
pages = {2554--8},
pmid = {6953413},
publisher = {National Academy of Sciences},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6953413 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC346238},
volume = {79},
year = {1982}
}
@inproceedings{Luong2016,
abstract = {Sequence to sequence learning has recently emerged as a new paradigm in super-vised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the one-to-many setting – where the encoder is shared between several tasks such as ma-chine translation and syntactic parsing, (b) the many-to-one setting – useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting – where multiple encoders and de-coders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Further-more, we have established a new state-of-the-art result in constituent parsing with 93.0 F 1 . Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06114v4},
author = {Luong, Minh-Thang and Le, Quoc V. and Sutskever, Ilya and Vinyals, Oriol and Kaiser, Lukasz and Brain, Google},
booktitle = {International Conference On Learning Representations},
eprint = {arXiv:1511.06114v4},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Luong et al. - 2015 - Multi-task Sequence to Sequence Learning(2).pdf:pdf},
keywords = {()},
month = {nov},
title = {{MULTI-TASK SEQUENCE TO SEQUENCE LEARNING}},
url = {http://arxiv.org/abs/1511.06114},
year = {2016}
}
@article{Barone2016,
abstract = {Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. Effective learning in this setting is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem of naive deep networks. Many of these architectures, such as LSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single structural principle: the state passthrough. We observe that these architectures, hereby characterized as Passthrough Networks, in addition to the mitigation of the vanishing gradient problem, enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on synthetic tasks and a state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.},
archivePrefix = {arXiv},
arxivId = {1603.03116},
author = {Barone, Antonio Valerio Miceli},
eprint = {1603.03116},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Barone - 2016 - Low-rank passthrough neural networks.pdf:pdf},
journal = {Arxiv},
month = {mar},
pages = {16},
title = {{Low-rank passthrough neural networks}},
url = {http://arxiv.org/abs/1603.03116},
year = {2016}
}
@inproceedings{Mikolov2011,
author = {Mikolov, Tomas and Kombrink, Stefan and Burget, Lukas and Cernocky, Jan and Khudanpur, Sanjeev},
booktitle = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2011.5947611},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2011 - Extensions of recurrent neural network language model.pdf:pdf},
isbn = {978-1-4577-0538-0},
keywords = {Artificial neural networks,Backpropagation,Computational modeling,Probability distribution,Recurrent neural networks,Training,Vocabulary,backpropagation,competitive language modeling techniques,computational complexity,feedforward network,feedforward neural nets,language modeling,natural language processing,recurrent neural nets,recurrent neural network language model,recurrent neural networks,speech recognition},
month = {may},
pages = {5528--5531},
publisher = {IEEE},
title = {{Extensions of recurrent neural network language model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5947611},
year = {2011}
}
@article{Salimans2016a,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.07868v1},
author = {Salimans, Tim},
eprint = {arXiv:1602.07868v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - salimans - weight normalization, a simple reparameterization to accelerate training of DNNs.pdf:pdf},
journal = {arXiv},
title = {{Weight Normalization : A Simple Reparameterization to Accelerate Training of Deep Neural Networks}},
year = {2016}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - Unknown - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.pdf:pdf},
title = {{ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION}},
year = {2014}
}
@article{Friedman2001,
author = {Friedman, Jerome H.},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Friedman - 2001 - Greedy function approximation A gradient boosting machine.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Function estimation,boosting,decision trees,robust nonparametric regression},
number = {5},
pages = {1189--1232},
publisher = {Institute of Mathematical Statistics},
title = {{Greedy function approximation: A gradient boosting machine.}},
volume = {29},
year = {2001}
}
@article{Lecun1998,
author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
doi = {10.1109/5.726791},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Lecun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {2D shape variability,Character recognition,Feature extraction,GTN,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis,back-propagation,backpropagation,cheque reading,complex decision surface synthesis,convolution,convolutional neural network character recognizers,document recognition,document recognition systems,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,handwritten character recognition,handwritten digit recognition task,high-dimensional patterns,language modeling,multilayer neural networks,multilayer perceptrons,multimodule systems,optical character recognition,performance measure minimization,segmentation recognition},
number = {11},
pages = {2278--2324},
publisher = {IEEE},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=726791},
volume = {86},
year = {1998}
}
@article{Meinshausen2006,
abstract = {The pattern of zero entries in the inverse covariance matrix of a multivariate normal distribution corresponds to conditional independence restrictions between variables. Covariance selection aims at estimating those structural zeros from data. We show that neighborhood selection with the Lasso is a computationally attractive alternative to standard covariance selection for sparse high-dimensional graphs. Neighborhood selection estimates the conditional independence restrictions separately for each node in the graph and is hence equivalent to variable selection for Gaussian linear models. We show that the proposed neighborhood selection scheme is consistent for sparse high-dimensional graphs. Consistency hinges on the choice of the penalty parameter. The oracle value for optimal prediction does not lead to a consistent neighborhood estimate. Controlling instead the probability of falsely joining some distinct connectivity components of the graph, consistent estimation for sparse graphs is achieved (with exponential rates), even when the number of variables grows as the number of observations raised to an arbitrary power.},
archivePrefix = {arXiv},
arxivId = {math/0608017},
author = {Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
doi = {10.1214/009053606000000281},
eprint = {0608017},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Meinshausen, B{\"{u}}hlmann - 2006 - High-dimensional graphs and variable selection with the Lasso.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Covariance selection,Gaussian graphical models,Linear regression,Penalized regression},
number = {3},
pages = {1436--1462},
pmid = {239471300013},
primaryClass = {math},
title = {{High-dimensional graphs and variable selection with the Lasso}},
volume = {34},
year = {2006}
}
@book{Sutton2012,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
address = {Cambridge},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
doi = {10.1109/MED.2013.6608833},
edition = {2},
eprint = {1603.02199},
file = {:Users/pfcmathews/Downloads/SuttonBook.pdf:pdf},
isbn = {0262193981},
issn = {18726240},
pmid = {18255791},
publisher = {MIT Press},
title = {{Reinforcement learning: An Introduction}},
url = {https://books.google.com/books?id=CAFR6IBF4xYC{\&}pgis=1$\backslash$nhttp://incompleteideas.net/sutton/book/the-book.html$\backslash$nhttps://www.dropbox.com/s/f4tnuhipchpkgoj/book2012.pdf},
year = {2010}
}
@article{Krueger2016,
abstract = {We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modelling and phoneme recognition, and outperforming weight noise. With this penalty term, IRNN can achieve similar performance to LSTM on language modelling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences.},
archivePrefix = {arXiv},
arxivId = {1511.08400},
author = {Krueger, David and Memisevic, Roland},
eprint = {1511.08400},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Krueger, Memisevic - 2015 - Regularizing RNNs by Stabilizing Activations(2).pdf:pdf},
journal = {International Conference On Learning Representations},
month = {nov},
pages = {1--8},
title = {{Regularizing RNNs by Stabilizing Activations}},
url = {http://arxiv.org/abs/1511.08400},
year = {2016}
}
@article{Wu2016a,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144},
file = {:Users/pfcmathews/Downloads/1609.08144v1.pdf:pdf},
journal = {arXiv preprint},
pages = {1--23},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{Burges2010,
abstract = {LambdaMART is the boosted tree version of LambdaRank, which is based on RankNet. RankNet, LambdaRank, and LambdaMART have proven to be very suc- cessful algorithms for solving real world ranking problems: for example an ensem- ble of LambdaMART rankers won Track 1 of the 2010 Yahoo! Learning To Rank Challenge. The details of these algorithms are spread across several papers and re- ports, and so here we give a self-contained, detailed and complete description of them.},
author = {Burges, Christopher J C},
doi = {10.1111/j.1467-8535.2010.01085.x},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Burges - Unknown - From RankNet to LambdaRank to LambdaMART An Overview.pdf:pdf},
issn = {00071013},
journal = {Learning},
pages = {23--581},
title = {{From ranknet to lambdarank to lambdamart: An overview}},
volume = {11},
year = {2010}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
author = {Bengio, Yoshua},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Bengio, Simard, Frasconi - 1994 - Learning Long-Term Dependencies with Gradient Descent is Difficult.pdf:pdf},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
url = {http://www.dsi.unifi.it/{~}paolo/ps/tnn-94-gradient.pdf http://dsii.dsi.unifi.it/{~}paolo/ps/tnn-94-gradient.pdf},
volume = {5},
year = {1994}
}
@article{Breiman2001,
author = {Breiman, L},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 2001 - Random forests(2).pdf:pdf},
journal = {Machine learning},
title = {{Random forests}},
url = {http://link.springer.com/article/10.1023/A:1010933404324},
year = {2001}
}
@article{Kuleshov2010,
abstract = {The stochasticmulti-armed bandit problem is an important model for studying the exploration- exploitation tradeoff in reinforcement learning. Although many algorithms for the problem are well-understood theoretically, empirical confirmation of their effectiveness is generally scarce. This paper presents a thorough empirical study of the most popular multi-armed bandit algorithms. Three important observations can be made from our results. Firstly, simple heuristics such as -greedy and Boltzmann exploration outperform theoretically sound algorithms on most settings by a significant margin. Secondly, the performance of most algorithms varies dramatically with the parameters of the bandit problem. Our study identifies for each algorithm the settings where it performs well, and the settings where it performs poorly. These properties are not described by current theory, even though they can be exploited in practice in the design of heuristics. Thirdly, the algorithms' perfor- mance relative each to other is affected only by the number of bandit arms and the variance of the rewards. This finding may guide the design of subsequent empirical evaluations. In the second part of the paper, we turn our attention to an important area of ap- plication of bandit algorithms: clinical trials. Although the design of clinical trials has been one of the principal practical problems motivating research on multi-armed bandits, bandit algorithms have never been evaluated as potential treatment allocation strategies. Using data from a real study, we simulate the outcome that a 2001-2002 clinical trial would have had if bandit algorithms had been used to allocate patients to treatments. We find that an adaptive trial would have successfully treated at least 50{\%} more patients, while significantly reducing the number of adverse effects and increasing patient retention. At the end of the trial, the best treatment could have still been identified with a high level of statistical confidence. Our findings demonstrate that bandit algorithms are attractive alternatives to current adaptive treatment allocation strategies.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.6028v1},
author = {Kuleshov, Volodymyr and Precup, D},
eprint = {arXiv:1402.6028v1},
file = {:Volumes/olossur/Documents/COMP422/project2/report/bandits.pdf:pdf},
journal = {Journal of Machine Learning},
title = {{Algorithms for the multi-armed bandit problem}},
url = {http://www.cs.mcgill.ca/{~}vkules/bandits.pdf},
year = {2000}
}
@article{Burns2016,
abstract = {acmqueue | january-february 2016 70 system evolution T hough widespread interest in software containers is a relatively recent phenomenon, at Google we have been managing Linux containers at scale for more than ten years and built three different container-management systems in that time. Each system was heavily influenced by its predecessors, even though they were developed for different reasons. This article describes the lessons we've learned from developing and operating them. The first unified container-management system developed at Google was the system we internally call Borg. 7 It was built to manage both long-running services and batch jobs, which had previously been handled by two separate systems: Babysitter and the Global Work Queue. The latter's architecture strongly influenced Borg, but was focused on batch jobs; both predated Linux control groups. Borg shares machines between these two types of applications as a way of increasing resource utilization and thereby reducing costs. Such sharing was possible because container support in the Linux kernel was becoming available (indeed, Google contributed much of the container code to the Linux kernel), which enabled better isolation between latency-sensitive user-facing services and CPU-hungry batch processes.},
author = {Burns, Brendan and Grant, Brian and Oppenheimer, David and Brewer, Eric and Wilkes, John},
doi = {10.1145/2898442.2898444},
file = {:Users/pfcmathews/Downloads/p10-burns.pdf:pdf},
issn = {15427730},
journal = {Acmqueue},
number = {february},
pages = {24},
title = {{Lessons learned from three container- management systems over a decade}},
year = {2016}
}
@article{Cho2014a,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - Unknown - Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
pages = {1724--1734},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Sigaud2015,
abstract = {Gated networks are networks that contain gating connections, in which the outputs of at least two neurons are multiplied. Initially, gated networks were used to learn relationships between two input sources, such as pixels from two images. More recently, they have been applied to learning activity recognition or multi-modal representations. The aims of this paper are threefold: 1) to explain the basic computations in gated networks to the non-expert, while adopting a standpoint that insists on their symmetric nature. 2) to serve as a quick reference guide to the recent literature, by providing an inventory of applications of these networks, as well as recent extensions to the basic architecture. 3) to suggest future research directions and applications.},
archivePrefix = {arXiv},
arxivId = {1512.03201},
author = {Sigaud, Olivier and Masson, Cl{\'{e}}ment and Filliat, David and Stulp, Freek},
eprint = {1512.03201},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Sigaud et al. - 2015 - GATED NETWORKS AN INVENTORY.pdf:pdf},
journal = {arXiv},
keywords = {()},
title = {{Gated networks: an inventory}},
url = {http://arxiv.org/abs/1512.03201},
year = {2015}
}
@article{Rousseeuw1987,
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate' number of clusters.},
author = {Rousseeuw, Peter J.},
doi = {10.1016/0377-0427(87)90125-7},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Rousseeuw - 1987 - Silhouettes A graphical aid to the interpretation and validation of cluster analysis.pdf:pdf},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
pages = {53--65},
title = {{Silhouettes: A graphical aid to the interpretation and validation of cluster analysis}},
volume = {20},
year = {1987}
}
@article{Tenenbaum2000,
abstract = {Perceptual systems routinely separate "content" from "style," classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, {\&} Bibby, 1979; Hinton {\&} Zemel, 1994; Ghahramani, 1995; Bell {\&} Sejnowski, 1995; Hinton, Dayan, Frey, {\&} Neal, 1995; Dayan, Hinton, Neal, {\&} Zemel, 1995; Hinton {\&} Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants.},
author = {Tenenbaum, Joshua B and Freeman, W T},
doi = {10.1162/089976600300015349},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Tenenbaum, Freeman MERL - Unknown - Separating Style and Content.pdf:pdf},
isbn = {0899-7667 (Print)$\backslash$r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {6},
pages = {1247--1283},
pmid = {10935711},
title = {{Separating style and content with bilinear models.}},
volume = {12},
year = {2000}
}
@inproceedings{Eldan2016,
abstract = {We show that there are simple functions expressible by small 3-layer feedforward neural networks, which cannot be approximated by a 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for most continuous activation functions, such as rectified linear units and sigmoids, and formally demonstrates that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks.},
archivePrefix = {arXiv},
arxivId = {1512.03965},
author = {Eldan, Ronen and Shamir, Ohad},
booktitle = {29th Annual Conference on Learning Theory},
eprint = {1512.03965},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Eldan, Shamir - 2015 - The Power of Depth for Feedforward Neural Networks.pdf:pdf},
keywords = {Neural Network},
month = {dec},
pages = {907----940},
title = {{The Power of Depth for Feedforward Neural Networks}},
url = {http://arxiv.org/abs/1512.03965},
year = {2016}
}
@article{Im2016,
abstract = {Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual "canvas". We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other.},
archivePrefix = {arXiv},
arxivId = {1602.05110},
author = {Im, Daniel Jiwoong and Kim, Chris Dongjoo and Jiang, Hui and Memisevic, Roland},
eprint = {1602.05110},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Im et al. - 2016 - Generating images with recurrent adversarial networks.pdf:pdf},
journal = {arxiv},
month = {feb},
title = {{Generating images with recurrent adversarial networks}},
url = {http://arxiv.org/abs/1602.05110},
year = {2016}
}
@inproceedings{Vinyals2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03134v1},
author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {arXiv:1506.03134v1},
file = {:Users/pfcmathews/Downloads/1506.03134v1.pdf:pdf},
title = {{Pointer Networks}},
year = {2015}
}
@article{Abadi2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.04467v2},
author = {Abadi, Mart$\backslash$'{\{}$\backslash$i{\}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man$\backslash$'{\{}e{\}}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi$\backslash$'{\{}e{\}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {arXiv:1603.04467v2},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Abadi et al. - 2015 - TensorFlow Large-scale machine learning on heterogeneous systems.pdf:pdf},
journal = {arXiv preprint},
title = {{TensorFlow: Large-scale machine learning on heterogeneous systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
year = {2015}
}
@article{Chen2016,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
archivePrefix = {arXiv},
arxivId = {1603.02754},
author = {Chen, Tianqi and Guestrin, Carlos},
eprint = {1603.02754},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:pdf},
month = {mar},
title = {{XGBoost: A Scalable Tree Boosting System}},
url = {http://arxiv.org/abs/1603.02754},
year = {2016}
}
@article{Orus2014,
abstract = {This is a partly non-technical introduction to selected topics on tensor network methods, based on several lectures and introductory seminars given on the subject. It should be a good place for newcomers to get familiarized with some of the key ideas in the field, specially regarding the numerics. After a very general introduction we motivate the concept of tensor network and provide several examples. We then move on to explain some basics about Matrix Product States (MPS) and Projected Entangled Pair States (PEPS). Selected details on some of the associated numerical methods for 1. d and 2. d quantum lattice systems are also discussed. ?? 2014 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {1306.2164},
author = {Or{\'{u}}s, Rom{\'{a}}n},
doi = {10.1016/j.aop.2014.06.013},
eprint = {1306.2164},
issn = {1096035X},
journal = {Annals of Physics},
keywords = {Entanglement,MPS,PEPS,Tensor networks},
month = {jun},
pages = {117--158},
title = {{A practical introduction to tensor networks: Matrix product states and projected entangled pair states}},
url = {http://arxiv.org/abs/1306.2164 http://dx.doi.org/10.1016/j.aop.2014.06.013},
volume = {349},
year = {2014}
}
@article{Boulanger-Lewandowski2012a,
abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
archivePrefix = {arXiv},
arxivId = {1206.6392},
author = {Boulanger-Lewandowski, Nicolas and Vincent, Pascal and Bengio, Yoshua},
eprint = {1206.6392},
file = {:Users/pfcmathews/Downloads/1206.6392v1.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
number = {Cd},
pages = {1159--1166},
title = {{Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription}},
year = {2012}
}
@article{He,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94{\%} top-5 test error on the ImageNet 2012 clas-sification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [33]). To our knowledge, our result is the first 1 to surpass the reported human-level performance (5.1{\%}, [26]) on this dataset.},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
file = {:Users/pfcmathews/Downloads/1502.01852v1.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {arXiv preprint},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{Nair,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Nair, Hinton - Unknown - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Hinton1990,
abstract = {Three different ways of mapping part-whole hierarchies into connectionist networks are described. The simplest scheme uses a fixed mapping and is inadequate for most tasks because it fails to share units and connections between different pieces of the part-whole hierarchy. Two alternative schemes are described, each of which involves a different method of time-sharing connections and units. The scheme we finally arrive at suggests that neural networks have two quite different methods for performing inference. Simple "intuitive" inferences can be performed by a single settling of a network without changing the way in which the world is mapped into the network. More complex "rational" inferences involve a sequence of such settlings with mapping changes after each settling. ?? 1990.},
author = {Hinton, Geoffrey E.},
doi = {10.1016/0004-3702(90)90004-J},
file = {:Users/pfcmathews/Downloads/AIJmapping.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-2},
pages = {47--75},
title = {{Mapping part-whole hierarchies into connectionist networks}},
volume = {46},
year = {1990}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, QVV},
eprint = {1409.3215},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
isbn = {1409.3215},
journal = {Advances in Neural Information Processing Systems},
pages = {3104--3112},
title = {{Sequence to sequence learning with neural networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
@article{Chung2016,
abstract = {Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.},
archivePrefix = {arXiv},
arxivId = {1609.01704},
author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
eprint = {1609.01704},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Chung, Ahn, Bengio - 2016 - Hierarchical Multiscale Recurrent Neural Networks.pdf:pdf},
journal = {arXiv preprint},
title = {{Hierarchical Multiscale Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1609.01704},
year = {2016}
}
@article{Srivastava2015,
abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
archivePrefix = {arXiv},
arxivId = {1505.00387},
author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"{u}}rgen},
eprint = {1505.00387},
file = {:Users/pfcmathews/Downloads/1505.00387v2.pdf:pdf},
journal = {arXiv:1505.00387 [cs]},
title = {{Highway Networks}},
url = {http://arxiv.org/abs/1505.00387$\backslash$nhttp://www.arxiv.org/pdf/1505.00387.pdf},
year = {2015}
}
@article{Li2015,
abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
archivePrefix = {arXiv},
arxivId = {1511.05493},
author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
eprint = {1511.05493},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2015 - Gated Graph Sequence Neural Networks.pdf:pdf},
journal = {ICLR},
keywords = {Gated Feedback,Graph Structure},
number = {1},
pages = {1--19},
title = {{Gated Graph Sequence Neural Networks}},
url = {http://arxiv.org/abs/1511.05493},
year = {2016}
}
@article{Schapire1990,
abstract = {The problem of improving the accuracy of a hypothesis output by a$\backslash$nlearning algorithm in the distribution-free learning model is$\backslash$nconsidered. A concept class is learnable (or strongly learnable) if,$\backslash$ngiven access to a source of examples from the unknown concept, the$\backslash$nlearner with high probability is able to output a hypothesis that is$\backslash$ncorrect on all but an arbitrarily small fraction of the instances. The$\backslash$nconcept class is weakly learnable if the learner can produce a$\backslash$nhypothesis that forms only slightly better than random guessing. It is$\backslash$nshown that these two notions of learnability are equivalent. An explicit$\backslash$nmethod is described for directly converting a weak learning algorithm$\backslash$ninto one that achieves arbitrarily high accuracy. This construction may$\backslash$nhave practical applications as a tool for efficiently converting a$\backslash$nmediocre learning algorithm into one that performs extremely well. In$\backslash$naddition, the construction has some interesting theoretical consequences$\backslash$n},
author = {Schapire, Robert E},
doi = {10.1023/A:1022648800760},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Schapire - 1990 - The Strength of Weak Learnability.pdf:pdf},
isbn = {0-8186-1982-1},
issn = {15730565},
journal = {Machine Learning},
keywords = {Machine learning,PAC learning,learnability theory,learning from examples,polynomial-time identification},
number = {2},
pages = {197--227},
title = {{The Strength of Weak Learnability}},
volume = {5},
year = {1990}
}
@book{Tan2006,
abstract = {1st ed. 1. Introduction -- 2. Data -- 3. Exploring data -- 4. Classification : basic concepts, decision trees, and model evaluation -- 5. Classification : alternative techniques -- 6. Association analysis : basic concepts and algorithms -- 7. Association analysis : advanced concepts -- 8. Cluster analysis : basic concepts and algorithms -- 9. Cluster analysis : additional issues and algorithms -- 10. Anomaly detection -- App. A. Linear algebra -- App. B. Dimensionality reduction -- App. C. Probability and statistics -- App. D. Regression -- App. E. Optimization.},
author = {Tan, Pang-Ning. and Steinbach, Michael. and Kumar, Vipin},
isbn = {978-0-321-32136-7},
keywords = {Data mining},
pages = {769},
publisher = {Pearson Addison Wesley},
title = {{Introduction to data mining}},
year = {2006}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - Unknown - Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
month = {jun},
pages = {1724--1734},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Chung2015,
abstract = {In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.},
archivePrefix = {arXiv},
arxivId = {1502.02367},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1145/2661829.2661935},
eprint = {1502.02367},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Chung et al. - 2015 - Gated feedback recurrent neural networks.pdf:pdf},
isbn = {9781634393973},
issn = {18792782},
journal = {Proceedings of the 32nd International Conference on Machine Learning},
pages = {2067----2075},
pmid = {23459267},
title = {{Gated feedback recurrent neural networks}},
url = {http://arxiv.org/abs/1502.02367},
volume = {37},
year = {2015}
}
@inproceedings{Mikolov2010,
author = {Mikolov, Tomas and Karafi{\'{a}}t, Martin and Burget, Luk{\'{a}}s and Cernock{\'{y}}, Jan and Khudanpur, Sanjeev},
booktitle = {Proceedings of the Conference of the International Speech Communication Association},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2010 - Recurrent neural network based language model.pdf:pdf},
pages = {1045--1048},
title = {{Recurrent neural network based language model}},
year = {2010}
}
@article{Neyshabur2015,
abstract = {We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.},
archivePrefix = {arXiv},
arxivId = {1506.02617},
author = {Neyshabur, Behnam and Salakhutdinov, Ruslan and Srebro, Nathan},
eprint = {1506.02617},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Neyshabur, Salakhutdinov, Srebro - 2015 - Path-SGD Path-Normalized Optimization in Deep Neural Networks.pdf:pdf},
issn = {10495258},
journal = {arXiv},
pages = {1--12},
title = {{Path-SGD: Path-Normalized Optimization in Deep Neural Networks}},
url = {http://arxiv.org/abs/1506.02617},
year = {2015}
}
@article{Wu2016,
abstract = {We introduce a general and simple structural design called Multiplicative Integration (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.},
archivePrefix = {arXiv},
arxivId = {1606.06630},
author = {Wu, Yuhuai and Zhang, Saizheng and Zhang, Ying and Bengio, Yoshua and Salakhutdinov, Ruslan},
eprint = {1606.06630},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - On Multiplicative Integration with Recurrent Neural Networks.pdf:pdf},
isbn = {9789537619084},
journal = {arXiv},
month = {jun},
title = {{On Multiplicative Integration with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1606.06630},
year = {2016}
}
@article{Zagoruyko2016,
abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN.},
archivePrefix = {arXiv},
arxivId = {1605.07146},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
eprint = {1605.07146},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Zagoruyko, Komodakis - 2016 - Wide Residual Networks.pdf:pdf},
journal = {Arxiv},
month = {may},
title = {{Wide Residual Networks}},
url = {http://arxiv.org/abs/1605.07146},
year = {2016}
}
@inproceedings{Mukherjee2013,
abstract = {We describe a new, simplified, and general analysis of a fusion of Nesterov's accelerated gradient with parallel coordinate de-scent. The resulting algorithm, which we call BOOM, for boosting with momentum, enjoys the merits of both techniques. Namely, BOOM re-tains the momentum and convergence properties of the accelerated gra-dient method while taking into account the curvature of the objective function. We describe an distributed implementation of BOOM which is suitable for massive high dimensional datasets. We show experimentally that BOOM is especially effective in large scale learning problems with rare yet informative features.},
author = {Mukherjee, Indraneel and Canini, Kevin and Frongillo, Rafael and Singer, Yoram},
booktitle = {ECML PKDD 2013, Part III, LNAI 8190},
doi = {10.1007/978-3-642-40994-3_2},
isbn = {9783642409936},
issn = {03029743},
keywords = {accelerated gradient,boosting,coordinate descent},
number = {PART 3},
pages = {17--32},
title = {{Parallel Boosting with Momentum}},
volume = {8190 LNAI},
year = {2013}
}
@article{Mishkin2015,
abstract = {Layer-sequential unit-variance (LSUV) initialization - a simple strategy for weight initialization for deep net learning - is proposed. The strategy proceeds from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. We show that with the strategy, learning of very deep nets via standard stochastic gradient descent is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance that is state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR, ImageNet datasets.},
archivePrefix = {arXiv},
arxivId = {1511.06422},
author = {Mishkin, Dmytro and Matas, Jiri},
doi = {10.1016/0898-1221(96)87329-9},
eprint = {1511.06422},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mishkin, Matas - 2015 - All you need is a good init.pdf:pdf},
isbn = {0262560992},
issn = {87550229},
journal = {ICLR},
keywords = {Initialization,Optimization},
pages = {1--8},
pmid = {21595383},
title = {{All you need is a good init}},
url = {http://arxiv.org/abs/1511.06422},
year = {2015}
}
@book{Minsky1969,
author = {Minsky, Marvin and Papert, Seymour},
publisher = {M.I.T. Press},
title = {{Perceptrons.}},
year = {1969}
}
@article{Smolensky1986,
abstract = {Smolensky, P. (1986). Information processing in dynamical systems: Foundations of harmony theory. In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing: Volume 1: Foundations, pages 194-281. MIT Press, Cambridge, MA.},
author = {Smolensky, Paul},
file = {:Users/pfcmathews/Downloads/Smolensky.1986.pdf:pdf},
isbn = {026268053X},
journal = {Parallel Distributed Processing Explorations in the Microstructure of Cognition},
number = {1},
pages = {194--281},
title = {{Information processing in dynamical systems: Foundations of harmony theory}},
url = {http://portal.acm.org/citation.cfm?id=104279.104290},
volume = {1},
year = {1986}
}
@inproceedings{Zhang2016,
abstract = {In this paper, we systematically analyse the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the "depth" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems, as we improve the state-of-the-art for sequential MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1602.08210},
author = {Zhang, Saizheng and Wu, Yuhuai and Che, Tong and Lin, Zhouhan and Memisevic, Roland and Salakhutdinov, Ruslan and Bengio, Yoshua},
booktitle = {ICML},
eprint = {1602.08210},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Architectural Complexity Measures of Recurrent Neural Networks.pdf:pdf},
issn = {10495258},
keywords = {Complexity Measure,RNN},
pages = {19},
title = {{Architectural Complexity Measures of Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1602.08210},
year = {2016}
}
@article{Cooijmans2016,
abstract = {We propose a reparameterization of LSTM that brings the benefits of batch nor-malization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transi-tion, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classi-fication, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and im-proved generalization.},
archivePrefix = {arXiv},
arxivId = {1603.09025},
author = {Cooijmans, Tim and Ballas, Nicolas and Laurent, C{\'{e}}sar and Courville, Aaron},
eprint = {1603.09025},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Cooijmans et al. - 2016 - Recurrent Batch Normalization.pdf:pdf},
journal = {arXiv preprint},
pages = {1--10},
title = {{Recurrent Batch Normalization}},
year = {2016}
}
@article{Gregor2015,
abstract = {This paper introduces the Deep Recurrent Atten-tive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin-guished from real data with the naked eye.},
archivePrefix = {arXiv},
arxivId = {1502.04623},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and {Jimenez Rezende}, Danilo and Wierstra, Daan},
eprint = {1502.04623},
journal = {Icml-2015},
month = {feb},
pages = {1462--1471},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
url = {http://arxiv.org/abs/1502.04623},
year = {2015}
}
@inproceedings{Allan2004,
author = {Allan, Moray and Williams, Christopher K I},
booktitle = {Advances in Neural Information Processing Systems, 17},
file = {:Users/pfcmathews/Downloads/2714-harmonising-chorales-by-probabilistic-inference.pdf:pdf},
pages = {25--32},
title = {{Harmonising Chorales by Probabilistic Inference}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2005{\_}270.pdf},
year = {2005}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
file = {:Users/pfcmathews/Downloads/Rosenblatt1958.pdf:pdf},
isbn = {0033-295X},
issn = {0033-295X},
journal = {Psychological review},
keywords = {PERCEPTION},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: a probabilistic model for information storage and organization in the brain.}},
volume = {65},
year = {1958}
}
@article{Dai2015,
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
archivePrefix = {arXiv},
arxivId = {1511.01432},
author = {Dai, Andrew M. and Le, Quoc V.},
eprint = {1511.01432},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Dai, Le - 2015 - Semi-supervised Sequence Learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS '15)},
month = {nov},
pages = {1--9},
title = {{Semi-supervised Sequence Learning}},
url = {http://arxiv.org/abs/1511.01432},
year = {2015}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {3111--3119},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Poliner2007,
abstract = {We present a discriminative model for polyphonic piano transcription. Support vector machines trained on spectral features are used to classify frame-level note instances. The classifier outputs are temporally constrained via hidden Markov models, and the proposed system is used to transcribe both synthesized and real piano recordings. A frame-level transcription accuracy of 68{\%} was achieved on a newly generated test set, and direct comparisons to previous approaches are provided.},
author = {Poliner, Graham E. and Ellis, D. P W},
doi = {10.1155/2007/48317},
file = {:Users/pfcmathews/Downloads/piano.pdf:pdf},
issn = {11108657},
journal = {EURASIP Journal on Advances in Signal Processing},
title = {{A discriminative model for polyphonic piano transcription}},
year = {2007}
}
@misc{Hinton1981,
abstract = {A viewpoint-independent description of the shape of an object can be generated by imposing a canonical frame of reference on the object and describing the spatial dispositions of the parts relative to this object-based frame. When a familiar object is in an unusual orientation, the deciding factor in the choice of the canonical object-based frame may be the fact that relative to this frame the object has a familiar shape description. This may suggest that we first hypothesise an object-based frame and then test the resultant shape description for familiarity. However, it is possible to organise the interactions between units in a parallel network so that the pattern of activity in the network simultaneously converges on a representation of the shape and a representation of the object-based frame of reference. The connections in the network are determined by the constraints inherent in the image formation process.},
author = {Hinton, Geoffrey E.},
booktitle = {Proceedings of the 7th international joint conference on Artificial intelligence},
file = {:Users/pfcmathews/Downloads/object-based81.pdf:pdf},
pages = {683--685},
title = {{A Parallel Computation that Assigns Canonical Object-Based Frames of Reference}},
url = {http://portal.acm.org/citation.cfm?id=1623282},
year = {1981}
}
@article{Schaefers2014,
abstract = {Monte-Carlo Tree Search (MCTS) has brought about great success regarding the evaluation of stochastic and deterministic games in recent years. We present and empirically analyze a data-driven parallelization approach for MCTS targeting large HPC clusters with Infiniband interconnect. Our implementation is based on OpenMPI and makes extensive use of its RDMA based asynchronous tiny message communication capabilities for effectively overlapping communication and computation. We integrate our parallel MCTS approach termed UCTTreesplit in our state-of-the-art Go engine Gomorra and measure its strengths and limitations in a real-world setting. Our extensive experiments show that we can scale up to 128 compute nodes and 2048 cores in self-play experiments and, furthermore, give promising directions for additional improvement. The generality of our parallelization approach advocates its use to significantly improve the search quality of a huge number of current MCTS applications.},
author = {Schaefers, Lars and Platzner, Marco},
doi = {10.1109/TCIAIG.2014.2346997},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - 06876158.pdf:pdf},
issn = {1943-068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Computational modeling,Computers,Data models,Games,Indexes,Memory management,Monte Carlo methods},
number = {99},
pages = {1--1},
title = {{Distributed Monte-Carlo Tree Search: A Novel Technique and its Application to Computer Go}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6876158},
volume = {PP},
year = {2014}
}
@article{Greff2016,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
file = {:Users/pfcmathews/Downloads/1503.04069v1.pdf:pdf},
isbn = {9788578110796},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
year = {2016}
}
@book{Quinlan2014,
abstract = {Describes a tree-based classifier. It handles un-ordered domains and null/missing values. It uses entropy arguments to choose attributes on which to discriminate. Interesting points: 1. pointer to a db repository: ml-repositoryics.uci.edu (UC Irvine) 2. comparison with statistics-based systems (p. 15) "As a general rule ... statistical techniques tend to focus on tasks in which all the attributes have continuous or ordinal values" S.M. Weiss and C.A. Kulikowski "Computer Systems that Learn", Morgan Kaufmann, San Mateo CA, 1991 provide a comparison 3. MDL principle, to decide how to prune the tree Jorman Rissanen, Anal of Statistics, 11,2, 416-431 Quinlan and Rivest, "Inferring decision trees using the Minimum Description Length principle" Information and Computation 80,3, 227-248 4. comparison of tree-based classifiers with neural nets (NN) - they are both more robust - they are about equally accurate (with NN slightly ahead); but NN require much more computation (an order of magnitude more) (p. 102) 5. CART is a statistics-based program L. Breiman, J.H. Friedman, R.A. Olshen and C.J. Stone "Classification and Regression Trees" Belmont, CA: Wadsworth (1984) 6. citation: Hunt 75 "Artificial Intelligence" NY, Academic Press (pioneered the tree-based classification methods)},
author = {Quinlan, JR},
booktitle = {Machine Learning},
doi = {10.1016/S0019-9958(62)90649-6},
isbn = {1558602380},
issn = {08856125},
pages = {302},
pmid = {21786264},
title = {{C4. 5: programs for machine learning}},
url = {https://books.google.co.nz/books?hl=en{\&}lr={\&}id=b3ujBQAAQBAJ{\&}oi=fnd{\&}pg=PP1{\&}dq=related:O9AvHGeAh{\_}{\_}1sM:scholar.google.com/{\&}ots=sP6vTKJmG8{\&}sig=zDWOEdzpX1IBuRGtKyqGPR5XWuU},
volume = {240},
year = {1993}
}
@article{Danihelka2016,
abstract = {We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.},
archivePrefix = {arXiv},
arxivId = {1602.03032},
author = {Danihelka, Ivo and Wayne, Greg and Uria, Benigno and Kalchbrenner, Nal and Graves, Alex},
eprint = {1602.03032},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Danihelka et al. - 2016 - Associative Long Short-Term Memory.pdf:pdf},
journal = {arXiv},
month = {feb},
title = {{Associative Long Short-Term Memory}},
url = {http://arxiv.org/abs/1602.03032},
year = {2016}
}
@inproceedings{Mason1999,
abstract = {We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theo-retical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions.},
author = {Mason, Llew and Bartlett, Peter and Baxter, Jonathan and Frean, Marcus},
booktitle = {NIPS},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mason et al. - Unknown - Boosting Algorithms as Gradient Descent.pdf:pdf},
title = {{Boosting Algorithms as Gradient Descent}},
year = {1999}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@article{Breiman2001a,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength og the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 2001 - Random Forests.pdf:pdf},
journal = {Machine Learning},
number = {1},
pages = {5--32},
title = {{Random Forests}},
url = {http://link.springer.com/article/10.1023/A:1010933404324},
volume = {45},
year = {2001}
}
@article{Vinyals2016,
abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset.We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.},
archivePrefix = {arXiv},
arxivId = {1609.06647},
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
doi = {10.1109/TPAMI.2016.2587640},
eprint = {1609.06647},
file = {:Users/pfcmathews/Downloads/1609.06647v1.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {PP},
pages = {1--1},
title = {{Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7505636},
volume = {99},
year = {2016}
}
@article{Huang2016a,
abstract = {Very deep convolutional networks with hundreds or more layers have lead to significant reductions in error on competitive benchmarks like the ImageNet or COCO tasks. Although the unmatched expressiveness of the many deep layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes and the training time can be painfully slow even on modern computers. In this paper we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and obtain deep networks. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. The resulting networks are short (in expectation) during training and deep during testing. Training Residual Networks with stochastic depth is compellingly simple to implement, yet effective. We show that this approach successfully addresses the training difficulties of deep networks and complements the recent success of Residual and Highway Networks. It reduces training time substantially and improves the test errors on almost all data sets significantly (CIFAR-10, CIFAR-100, SVHN). Intriguingly, we show that with stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91{\%}) on CIFAR-10.},
archivePrefix = {arXiv},
arxivId = {1603.09382},
author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian},
eprint = {1603.09382},
file = {:Users/pfcmathews/Downloads/1603.09382v3.pdf:pdf},
journal = {arXiv},
title = {{Deep Networks with Stochastic Depth}},
url = {http://arxiv.org/abs/1603.09382},
year = {2016}
}
@inproceedings{Balduzzi2016,
abstract = {Recurrent neural networks are increasing popular models for sequential learning. Unfortunately, although the most effective RNN architectures are perhaps excessively complicated, extensive searches have not found simpler alternatives. This paper imports ideas from physics and functional programming into RNN design to provide guiding principles. From physics we introduce type constraints, analogous to the constraints that disqualify adding meters to seconds in physics. From functional programming, we require that strongly-typed architectures factorize into stateless learnware and state-dependent firmware, thereby ameliorating the impact of side-effects. The features learned by strongly-typed nets have a simple semantic interpretation via dynamic average-pooling on one-dimensional convolutions. We also show that strongly-typed gradients are better behaved than in classical architectures, and characterize the representational power of strongly-typed nets. Finally, experiments show that, despite being more constrained, strongly-typed architectures achieve lower training error and comparable generalization error to classical architectures.},
archivePrefix = {arXiv},
arxivId = {1602.02218},
author = {Balduzzi, David and Ghifary, Muhammad},
booktitle = {International Conference on Machine Learning - ICML 2014},
eprint = {1602.02218},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Balduzzi, Ghifary - 2016 - Strongly-Typed Recurrent Neural Networks.pdf:pdf},
pages = {9},
title = {{Strongly-Typed Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1602.02218},
year = {2016}
}
@article{Harshman1970,
abstract = {Simple structure and other common principles of factor rotation do not in general provide strong grounds for attributing explanatory significance to the factors which they select. In contrast, it is shown that an extension of Cattell's principle of rotation to Proportional Profiles (PP) offers a basis for determining explanatory factors for three-way or higher order multi-mode data. Conceptual models are developed for two basic patterns of multi-mode data variation, system- and object-variation, and PP analysis is found to apply in the system-variation case. Although PP was originally formulated as a principle of rotation to be used with classic two-way factor analysis, it is shown to embody a latent three-mode factor model, which is here made explicit and generalized frown two to N "parallel occasions". As originally formulated, PP rotation was restricted to orthogonal factors. The generalized PP model is demonstrated to give unique "correct" solutions with oblique, non-simple structure, and even non-linear factor structures. A series of tests, conducted with synthetic data of known factor composition, demonstrate the capabilities of linear and non-linear versions of the model, provide data on the minimal necessary conditions of uniqueness, and reveal the properties of the analysis procedures when these minimal conditions are not fulfilled. In addition, a mathematical proof is presented for the uniqueness of the solution given certain conditions on the data. Three-mode PP factor analysis is applied to a three-way set of real data consisting of the fundamental and first three formant frequencies of 11 persons saying 8 vowels. A unique solution is extracted, consisting of three factors which are highly meaningful and consistent with prior knowledge and theory concerning vowel quality. The relationships between the three-mode PP model and Tucker's multi-modal model, McDonald's non-linear model and Carroll and Chang's multi-dimensional scaling model are explored.},
author = {Harshman, Richard A},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Harshman - 1970 - FOUNDATIONS OF THE PARAFAC PROCEDURE MODELS AND CONDITIONS FOR AN {\&}quotEXPLANATORY{\&}quot MULTIMODAL FACTOR ANALYSIS.pdf:pdf},
journal = {UCLA Working Papers in Phonetics},
number = {10},
pages = {1-- 84},
title = {{Foundations of the PARAFAC procedure: Models and conditions for an ``explanatory'' multimodal factor analysis}},
url = {http://www.psychology.uwo.ca/faculty/harshman/wpppfac0.pdf},
volume = {16},
year = {1970}
}
@article{Elman1990,
author = {Elman, Jeffrey l},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Elman - 1990 - Finding Structure in Time.pdf:pdf},
journal = {COGNITIVE SCIENCE},
pages = {179--211},
title = {{Finding Structure in Time}},
volume = {14},
year = {1990}
}
@article{Werbos1990,
abstract = {Basic backpropagation, which is a simple method now being widely$\backslash$nused in areas like pattern recognition and fault diagnosis, is reviewed.$\backslash$nThe basic equations for backpropagation through time, and applications$\backslash$nto areas like pattern recognition involving dynamic systems, systems$\backslash$nidentification, and control are discussed. Further extensions of this$\backslash$nmethod, to deal with systems other than neural networks, systems$\backslash$ninvolving simultaneous equations, or true recurrent networks, and other$\backslash$npractical issues arising with the method are described. Pseudocode is$\backslash$nprovided to clarify the algorithms. The chain rule for ordered$\backslash$nderivatives-the theorem which underlies backpropagation-is briefly$\backslash$ndiscussed. The focus is on designing a simpler version of$\backslash$nbackpropagation which can be translated into computer code and applied$\backslash$ndirectly by neutral network users},
author = {Werbos, Paul J},
doi = {10.1109/5.58337},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Werbos - Unknown - Backpropagation Through Time What It Does and How to Do It.pdf:pdf},
isbn = {0018-9219},
issn = {15582256},
journal = {Proceedings of the IEEE},
number = {10},
pages = {1550--1560},
title = {{Backpropagation Through Time: What It Does and How to Do It}},
volume = {78},
year = {1990}
}
@article{Mikolov2015,
abstract = {Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming a kind of longer term memory. We evaluate our model on language modeling tasks on benchmark datasets, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter {\&} Schmidhuber, 1997).},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.7753v1},
author = {Mikolov, Tomas and Joulin, Armand and Chopra, Sumit and Mathieu, Michael},
eprint = {arXiv:1412.7753v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2014 - Learning Longer Memory in Recurrent Neural Networks.pdf:pdf},
journal = {Iclr},
month = {dec},
pages = {1--9},
title = {{Learning Longer Memory in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1412.7753 http://arxiv.org/pdf/1412.7753v1.pdf},
year = {2015}
}
@article{Memisevic2007,
abstract = {We describe a probabilistic model for learning rich, distributed representations of image transformations. The basic model is defined as a gated conditional random field that is trained to predict transformations of its inputs using a factorial set of latent variables. Inference in the model consists in extracting the transformation, given a pair of images, and can be performed exactly and efficiently. We show that, when trained on natural videos, the model develops domain specific motion features, in the form of fields of locally transformed edge filters. When trained on affine, or more general, transformations of still images, the model develops codes for these transformations, and can subsequently perform recognition tasks that are invariant under these transformations. It can also fantasize new transformations on previously unseen images. We describe several variations of the basic model and provide experimental results that demonstrate its applicability to a variety of tasks.},
author = {Memisevic, Roland and Hinton, Geoffrey},
doi = {http://doi.ieeecomputersociety.org/10.1109/CVPR.2007.383036},
file = {:Users/pfcmathews/Downloads/cvpr07.pdf:pdf},
isbn = {1-4244-1179-3},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition, IEEE Computer Society Conference on},
pages = {1--8},
title = {{Unsupervised Learning of Image Transformations}},
volume = {0},
year = {2007}
}
@article{Oord2016,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:Users/pfcmathews/Downloads/1609.03499.pdf:pdf},
isbn = {9783901882760},
pages = {1--15},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@book{Magnus2007,
author = {Magnus, J. and Neudecker, H.},
edition = {3},
file = {:Users/pfcmathews/Downloads/mdc2007-3rdedition.pdf:pdf},
isbn = {0471986321},
issn = {00401706},
publisher = {Wiley},
title = {{Matrix Differential Calculus with Applications in Statistics and Econometrics}},
url = {http://www.jstor.org/stable/1270024?origin=crossref},
year = {2007}
}
@article{Boulanger-Lewandowski2012,
abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
archivePrefix = {arXiv},
arxivId = {1206.6392},
author = {Boulanger-Lewandowski, Nicolas and Vincent, Pascal and Bengio, Yoshua},
eprint = {1206.6392},
isbn = {978-1-4503-1285-1},
journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
month = {jun},
number = {Cd},
pages = {1159--1166},
title = {{Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription}},
url = {http://arxiv.org/abs/1206.6392},
year = {2012}
}
@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555v1},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
eprint = {1412.3555v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:pdf},
journal = {arXiv},
month = {dec},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
url = {http://arxiv.org/abs/1412.3555},
year = {2014}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory re-sources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demon-strate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {arXiv:1410.5401v2},
file = {:Users/pfcmathews/Downloads/1410.5401v2.pdf:pdf},
journal = {arXiv preprint},
pages = {1--26},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at rel-atively low computational cost. Recently, the introduction of residual connections in conjunction with a more tradi-tional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Incep-tion networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These varia-tions improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We fur-ther demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent},
eprint = {1602.07261},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy, Ioffe, Vanhoucke - 2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf:pdf},
journal = {Arxiv},
pages = {12},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
year = {2016}
}
@article{Joulin2015,
abstract = {Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01007v4},
author = {Joulin, Armand and Mikolov, Tomas},
eprint = {arXiv:1503.01007v4},
file = {:Users/pfcmathews/Downloads/1503.01007v4.pdf:pdf},
isbn = {1503.01007},
issn = {10495258},
journal = {arXiv},
pages = {1--10},
title = {{Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets}},
year = {2015}
}
@article{Gao2016,
abstract = {This paper introduces two recurrent neural network structures called Simple Gated Unit (SGU) and Deep Simple Gated Unit (DSGU), which are general structures for learning long term dependencies. Compared to traditional Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), both structures require fewer parameters and less computation time in sequence classification tasks. Unlike GRU and LSTM, which require more than one gates to control information flow in the network, SGU and DSGU only use one multiplicative gate to control the flow of information. We show that this difference can accelerate the learning speed in tasks that require long dependency information. We also show that DSGU is more numerically stable than SGU. In addition, we also propose a standard way of representing inner structure of RNN called RNN Conventional Graph (RCG), which helps analyzing the relationship between input units and hidden units of RNN.},
archivePrefix = {arXiv},
arxivId = {1604.02910},
author = {Gao, Yuan and Glowacka, Dorota},
eprint = {1604.02910},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Gao, Glowacka - 2016 - Deep Gate Recurrent Neural Network.pdf:pdf},
journal = {arXiv preprint},
pages = {7},
title = {{Deep Gate Recurrent Neural Network}},
url = {http://arxiv.org/abs/1604.02910},
volume = {1},
year = {2016}
}
@article{Hihi1995,
abstract = {We have already shown that extracting long-term dependencies from sequential data is difficult, both for deterministic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.},
author = {Hihi, Salah El and Bengio, Yoshua},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Hihi, Bengio - 1995 - Hierarchical Recurrent Neural Networks for Long-Term Dependencies.pdf:pdf},
journal = {Nips},
pages = {493--499},
title = {{Hierarchical Recurrent Neural Networks for Long-Term Dependencies.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.7574{\&}rep=rep1{\&}type=pdf},
year = {1995}
}
@article{Bahdanau2015,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {{Dzmitry Bahdana} and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:Users/pfcmathews/Downloads/1409.0473.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
journal = {International Conference On Learning Representations},
keywords = {Neural machine translation is a recently proposed,Unlike the traditional statistical machine transla,a source sentence into a fixed-length vector from,and propose to extend this by allowing a model to,bottleneck in improving the performance of this ba,for parts of a source sentence that are relevant t,having to form these parts as a hard segment expli,machine translation often belong to a family of en,maximize the translation performance. The models p,phrase-based system on the task of English-to-Fren,qualitative analysis reveals that the (soft-)align,the neural machine,translation aims at building a single neural netwo,translation. In this paper,we achieve a translation performance comparable to,we conjecture that the use of a fixed-length vecto,well with our intuition,without},
pages = {1--15},
pmid = {14527267},
title = {{Neural Machine Translation By Jointly Learning To Align and Translate}},
url = {http://arxiv.org/abs/1409.0473v3},
year = {2015}
}
@article{Potamitis2016,
author = {Potamitis, Ilyas},
file = {:Users/pfcmathews/Downloads/1609.08408v1.pdf:pdf},
journal = {arXiv},
keywords = {bird sound,computational ecology,deep learning,u-net},
pages = {1--13},
title = {{Deep learning for detection of bird vocalisations}},
year = {2016}
}
@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
doi = {10.1145/1143844.1143891},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Graves et al. - 2006 - Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Qv and Mikolov, Tomas},
eprint = {1405.4053},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Le, Mikolov - Unknown - Distributed Representations of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
journal = {International Conference on Machine Learning - ICML 2014},
pages = {1188--1196},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{XuanVinh2010,
abstract = {Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing pop-ular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then high-light to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants.},
author = {{Xuan Vinh}, Nguyen and {Julien Epps}, Unsweduau and Bailey, James},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Xuan Vinh, Julien Epps, Bailey - 2010 - Information Theoretic Measures for Clusterings Comparison Variants, Properties, Normalization an.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {adjustment for chance,clustering comparison,information theory,normalized infor-mation distance},
pages = {2837--2854},
title = {{Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance}},
volume = {11},
year = {2010}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition(2).pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {arXiv},
keywords = {deep learning,denoising auto-encoder,image denoising},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385 http://arxiv.org/pdf/1512.03385v1.pdf},
year = {2015}
}
@article{TheTheanoDevelopmentTeam2016,
abstract = {Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.},
archivePrefix = {arXiv},
arxivId = {1605.02688},
author = {{The Theano Development Team} and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Fr{\'{e}}d{\'{e}}ric and Bayer, Justin and Belikov, Anatoly and Belopolsky, Alexander and Bengio, Yoshua and Bergeron, Arnaud and Bergstra, James and Bisson, Valentin and Snyder, Josh Bleecher and Bouchard, Nicolas and Boulanger-Lewandowski, Nicolas and Bouthillier, Xavier and de Br{\'{e}}bisson, Alexandre and Breuleux, Olivier and Carrier, Pierre-Luc and Cho, Kyunghyun and Chorowski, Jan and Christiano, Paul and Cooijmans, Tim and C{\^{o}}t{\'{e}}, Marc-Alexandre and C{\^{o}}t{\'{e}}, Myriam and Courville, Aaron and Dauphin, Yann N. and Delalleau, Olivier and Demouth, Julien and Desjardins, Guillaume and Dieleman, Sander and Dinh, Laurent and Ducoffe, M{\'{e}}lanie and Dumoulin, Vincent and Kahou, Samira Ebrahimi and Erhan, Dumitru and Fan, Ziye and Firat, Orhan and Germain, Mathieu and Glorot, Xavier and Goodfellow, Ian and Graham, Matt and Gulcehre, Caglar and Hamel, Philippe and Harlouchet, Iban and Heng, Jean-Philippe and Hidasi, Bal{\'{a}}zs and Honari, Sina and Jain, Arjun and Jean, S{\'{e}}bastien and Jia, Kai and Korobov, Mikhail and Kulkarni, Vivek and Lamb, Alex and Lamblin, Pascal and Larsen, Eric and Laurent, C{\'{e}}sar and Lee, Sean and Lefrancois, Simon and Lemieux, Simon and L{\'{e}}onard, Nicholas and Lin, Zhouhan and Livezey, Jesse A. and Lorenz, Cory and Lowin, Jeremiah and Ma, Qianli and Manzagol, Pierre-Antoine and Mastropietro, Olivier and McGibbon, Robert T. and Memisevic, Roland and van Merri{\"{e}}nboer, Bart and Michalski, Vincent and Mirza, Mehdi and Orlandi, Alberto and Pal, Christopher and Pascanu, Razvan and Pezeshki, Mohammad and Raffel, Colin and Renshaw, Daniel and Rocklin, Matthew and Romero, Adriana and Roth, Markus and Sadowski, Peter and Salvatier, John and Savard, Fran{\c{c}}ois and Schl{\"{u}}ter, Jan and Schulman, John and Schwartz, Gabriel and Serban, Iulian Vlad and Serdyuk, Dmitriy and Shabanian, Samira and Simon, {\'{E}}tienne and Spieckermann, Sigurd and Subramanyam, S. Ramana and Sygnowski, Jakub and Tanguay, J{\'{e}}r{\'{e}}mie and van Tulder, Gijs and Turian, Joseph and Urban, Sebastian and Vincent, Pascal and Visin, Francesco and de Vries, Harm and Warde-Farley, David and Webb, Dustin J. and Willson, Matthew and Xu, Kelvin and Xue, Lijun and Yao, Li and Zhang, Saizheng and Zhang, Ying},
eprint = {1605.02688},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/The Theano Development Team et al. - 2016 - Theano A Python framework for fast computation of mathematical expressions.pdf:pdf},
journal = {arXiv},
keywords = {Computer Science - Learning,Computer Science - Mathematical Software,Computer Science - Symbolic Computation},
pages = {19},
title = {{Theano: A Python framework for fast computation of mathematical expressions}},
url = {http://arxiv.org/abs/1605.02688},
volume = {abs/1605.0},
year = {2016}
}
@article{Karpathy2015,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2014}
}
@inproceedings{Taylor,
abstract = {The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factor the three-way weight tensor implied by the multiplicative model, reducing the number of parameters from O(N3) to O(N2). The result is an efficient, compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM, our model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly among them.},
author = {Taylor, Graham W. and Hinton, Geoffrey E.},
booktitle = {Proceedings of the 26th International Conference on Machine Learning (ICML 09)},
doi = {10.1145/1553374.1553505},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Taylor, Hinton - Unknown - Factored Conditional Restricted Boltzmann Machines for Modeling Motion Style.pdf:pdf},
isbn = {9781605585161},
issn = {1520510X},
pages = {1025--1032},
title = {{Factored conditional restricted Boltzmann Machines for modeling motion style}},
url = {http://dl.acm.org/citation.cfm?id=1553374.1553505},
year = {2009}
}
@article{Vinh2009,
author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
doi = {10.1145/1553374.1553511},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Vinh, Epps, Bailey - 2009 - Information theoretic measures for clusterings comparison is a correction for chance necessary.pdf:pdf},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1073--1080},
publisher = {ACM},
title = {{Information theoretic measures for clusterings comparison: is a correction for chance necessary?}},
year = {2009}
}
@phdthesis{Mikolov2012,
abstract = {Statistical language models are crucial part of many successful applications, such as au- tomatic speech recognition and statistical machine translation (for example well-known Google Translate). Traditional techniques for estimating these models are based on N- gram counts. Despite known weaknesses of N-grams and huge efforts of research commu- nities across many fields (speech recognition, machine translation, neuroscience, artificial intelligence, natural language processing, data compression, psychology etc.), N-grams remained basically the state-of-the-art. The goal of this thesis is to present various archi- tectures of language models that are based on artificial neural networks. Although these models are computationally more expensive than N-gram models, with the presented techniques it is possible to apply them to state-of-the-art systems efficiently. Achieved reductions of word error rate of speech recognition systems are up to 20{\%}, against state- of-the-art N-gram model. The presented recurrent neural network based model achieves the best published performance on well-known Penn Treebank setup. Kl´cov´},
archivePrefix = {arXiv},
arxivId = {1312.3005},
author = {Mikolov, Tomas},
booktitle = {PhD thesis},
doi = {10.1016/j.csl.2015.07.001},
eprint = {1312.3005},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov - 2012 - Statistical Language Models Based on Neural Networks.pdf:pdf},
isbn = {0885-2308},
issn = {08852308},
number = {April},
pages = {1--129},
pmid = {18244602},
title = {{Statistical Language Models Based on Neural Networks}},
url = {http://www.fit.vutbr.cz/research/pubs/diss.php.cs?id=10158},
year = {2012}
}
@article{Palit2012,
abstract = {In this era of data abundance, it has become critical to process large volumes of data at much faster rates than ever before. Boosting is a powerful predictive model that has been successfully used in many real-world applications. However, due to the inherent sequential nature, achieving scalability for boosting is nontrivial and demands the development of new parallelized versions which will allow them to efficiently handle large-scale data. In this paper, we propose two parallel boosting algorithms, AdaBoost.PL and LogitBoost.PL, which facilitate simultaneous participation of multiple computing nodes to construct a boosted ensemble classifier. The proposed algorithms are competitive to the corresponding serial versions in terms of the generalization performance. We achieve a significant speedup since our approach does not require individual computing nodes to communicate with each other for sharing their data. In addition, the proposed approach also allows for preserving privacy of computations in distributed environments. We used MapReduce framework to implement our algorithms and demonstrated the performance in terms of classification accuracy, speedup and scaleup using a wide variety of synthetic and real-world data sets.},
author = {Palit, Indranil and Reddy, Chandan K},
doi = {10.1109/TKDE.2011.208},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Palit, Reddy - Unknown - Scalable and Parallel Boosting with MapReduce.pdf:pdf},
isbn = {2011030145},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Boosting,MapReduce,classification,distributed computing,parallel algorithms},
number = {10},
pages = {1904--1916},
title = {{Scalable and Parallel Boosting with MapReduce}},
volume = {24},
year = {2012}
}
@article{Trelea2003,
abstract = {The particle swarm optimization algorithm is analyzed using standard results from the dynamic system theory. Graphical parameter selection guidelines are derived. The exploration-exploitation tradeoff is discussed and illustrated. Examples of performance on benchmark functions superior to previously published results are given. ?? 2002 Elsevier Science B.V. All rights reserved.},
author = {Trelea, Ioan Cristian},
doi = {10.1016/S0020-0190(02)00447-7},
file = {:Users/pfcmathews/Downloads/1-s2.0-S0020019002004477-main.pdf:pdf},
isbn = {0020-0190},
issn = {00200190},
journal = {Information Processing Letters},
keywords = {Analysis of algorithms,Parallel algorithms,Particle swarm optimization,Stochastic optimization},
number = {6},
pages = {317--325},
title = {{The particle swarm optimization algorithm: Convergence analysis and parameter selection}},
volume = {85},
year = {2003}
}
@article{Ioffe2015,
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift(2).pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {arXiv},
month = {feb},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{TinKamHo1995,
abstract = {Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits},
author = {{Tin Kam Ho}},
doi = {10.1109/ICDAR.1995.598994},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(2).pdf:pdf},
isbn = {0-8186-7128-9},
journal = {Proceedings of 3rd International Conference on Document Analysis and Recognition},
keywords = {Classification tree analysis,Decision trees,Handwriting recognition,Hidden Markov models,Multilayer perceptrons,Optimization methods,Stochastic processes,Testing,Tin,Training data,complexity,decision theory,generalization accuracy,handwritten digits,optical character recognition,random decision forests,stochastic modeling,suboptimal accuracy,tree-based classifiers},
pages = {278--282},
title = {{Random decision forests}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=598994},
volume = {1},
year = {1995}
}
@article{Dhingra2016,
abstract = {In this paper we study the problem of answering cloze-style questions over short documents. We introduce a new attention mechanism which uses multiplicative interactions between the query embedding and intermediate states of a recurrent neural network reader. This enables the reader to build query-specific representations of tokens in the document which are further used for answer selection. Our model, the Gated-Attention Reader, outperforms all state-of-the-art models on several large-scale benchmark datasets for this task---the CNN $\backslash${\&} Dailymail news stories and Children's Book Test. We also provide a detailed analysis of the performance of our model and several baselines over a subset of questions manually annotated with certain linguistic features. The analysis sheds light on the strengths and weaknesses of several existing models.},
archivePrefix = {arXiv},
arxivId = {1606.01549},
author = {Dhingra, Bhuwan and Liu, Hanxiao and Cohen, William W. and Salakhutdinov, Ruslan},
eprint = {1606.01549},
file = {:Users/pfcmathews/Downloads/1606.01549v1.pdf:pdf},
journal = {arXiV},
month = {jun},
title = {{Gated-Attention Readers for Text Comprehension}},
url = {http://arxiv.org/abs/1606.01549},
year = {2016}
}
@article{Neyshabur2016,
abstract = {We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.},
archivePrefix = {arXiv},
arxivId = {1605.07154},
author = {Neyshabur, Behnam and Wu, Yuhuai and Salakhutdinov, Ruslan and Srebro, Nathan},
eprint = {1605.07154},
file = {:Users/pfcmathews/Downloads/1605.07154v1.pdf:pdf},
journal = {arXiv preprint},
month = {may},
title = {{Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations}},
url = {http://arxiv.org/abs/1605.07154},
year = {2016}
}
@article{Hitchcock1927,
author = {Hitchcock, Frank L.},
doi = {10.1002/sapm192761164},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Hitchcock - 1927 - The Expression of a Tensor or a Polyadic as a Sum of Products.pdf:pdf},
issn = {00971421},
journal = {Journal of Mathematics and Physics},
month = {apr},
number = {1-4},
pages = {164--189},
title = {{The Expression of a Tensor or a Polyadic as a Sum of Products}},
url = {http://doi.wiley.com/10.1002/sapm192761164},
volume = {6},
year = {1927}
}
@article{Singhal2001,
abstract = {For thousands of years people have realized the importance of archiving and finding information. With the advent of computers, it became possible to store large amounts of information; and finding useful information from such collections became a necessity. The field of Information Retrieval (IR) was born in the 1950s out of this necessity. Over the last forty years, the field has matured considerably. Several IR systems are used on an everyday basis by a wide variety of users. This article is a brief overview of the key advances in the field of Information Retrieval, and a description of where the state-of-the-art is at in the field.},
author = {Singhal, Amit},
doi = {10.1.1.117.7676‎},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Singhal - 2001 - Modern Information Retrieval A Brief Overview.pdf:pdf},
issn = {00218979},
journal = {Bulletin of the Ieee Computer Society Technical Committee on Data Engineering},
number = {4},
pages = {1--9},
title = {{Modern Information Retrieval: A Brief Overview}},
url = {http://160592857366.free.fr/joe/ebooks/ShareData/Modern Information Retrieval - A Brief Overview.pdf},
volume = {24},
year = {2001}
}
@article{Jaeger2005,
abstract = {This tutorial is a worked-out version of a 5-hour course originally held at AIS in September/October 2002. It has two distinct components. First, it contains a mathematically-oriented crash course on traditional training methods for recurrent neural networks, covering back-propagation through time (BPTT), real-time recurrent learning (RTRL), and extended Kalman filtering approaches (EKF). This material is covered in Sections 2 – 5. The remaining sections 1 and 6 – 9 are much more gentle, more detailed, and illustrated with simple examples. They are intended to be useful as a stand-alone tutorial for the echo state network (ESN) approach to recurrent neural network training. The},
author = {Jaeger, Herbert},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Jaeger - 2005 - A tutorial on training recurrent neural networks , covering BPPT , RTRL , EKF and the echo state network approach.pdf:pdf},
isbn = {159},
journal = {ReVision},
pages = {1--46},
title = {{A tutorial on training recurrent neural networks , covering BPPT , RTRL , EKF and the " echo state network " approach}},
url = {http://www.mendeley.com/catalog/tutorial-training-recurrent-neural-networks-covering-bppt-rtrl-ekf-echo-state-network-approach/},
volume = {2002},
year = {2005}
}
@article{Dauphin2014,
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
archivePrefix = {arXiv},
arxivId = {1406.2572},
author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
eprint = {1406.2572},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.pdf:pdf},
isbn = {1406.2572},
issn = {10495258},
journal = {arXiv},
month = {jun},
pages = {1--14},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
url = {http://arxiv.org/abs/1406.2572},
year = {2014}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Arjovsky2015,
abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.},
archivePrefix = {arXiv},
arxivId = {1511.06464},
author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
eprint = {1511.06464},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Arjovsky, Shah, Bengio - 2015 - Unitary Evolution Recurrent Neural Networks.pdf:pdf},
journal = {arXiv},
month = {nov},
pages = {1--11},
title = {{Unitary Evolution Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1511.06464},
year = {2015}
}
@article{Ye2009,
abstract = {Stochastic Gradient Boosted Decision Trees (GBDT) is one of the most widely used learning algorithms in machine learning today. It is adaptable, easy to interpret, and produces highly accurate models. However, most implementations today are computationally expensive and require all training data to be in main memory. As training data becomes ever larger, there is motivation for us to parallelize the GBDT algorithm. Parallelizing decision tree training is intuitive and various approaches have been explored in existing literature. Stochastic boosting on the other hand is inherently a sequential process and have not been applied to distributed decision trees. In this work, we present two different distributed methods that generates exact stochastic GBDT models, the first is a MapReduce implementation and the second utilizes MPI on the Hadoop grid environment.},
author = {Ye, Jerry and Chow, Jyh-Herng and Chen, Jiang and Zheng, Zhaohui},
doi = {10.1145/1645953.1646301},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Ye et al. - Unknown - Stochastic Gradient Boosted Distributed Decision Trees.pdf:pdf},
isbn = {978-1-60558-512-3},
issn = {1605585122},
journal = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
keywords = {decision trees,distributed learning,gradient boosting,hadoop,learning to rank,mpi,web search ranking},
pages = {2061--2064},
title = {{Stochastic Gradient Boosted Distributed Decision Trees}},
url = {http://doi.acm.org/10.1145/1645953.1646301},
year = {2009}
}
@article{Kennedy1995,
abstract = {A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described},
author = {Kennedy, J and Eberhart, R},
doi = {10.1109/ICNN.1995.488968},
file = {:Users/pfcmathews/Downloads/{\_}reading6 1995 particle swarming.pdf:pdf},
isbn = {VO - 4},
issn = {19353812},
journal = {Neural Networks, 1995. Proceedings., IEEE International Conference on},
keywords = {Artificial neural networks,Birds,Educational institutions,Genetic algorithms,Humans,Marine animals,Optimization methods,Particle swarm optimization,Performance evaluation,Testing,artificial intelligence,artificial life,evolution,genetic algorithms,multidimensional search,neural nets,neural network,nonlinear functions,optimization,particle swarm,search problems,simulation,social metaphor},
pages = {1942--1948 vol.4},
pmid = {20371407},
title = {{Particle swarm optimization}},
volume = {4},
year = {1995}
}
@article{Kolda2009,
author = {Kolda, Tamara G. and Bader, Brett W.},
doi = {10.1137/07070111X},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Kolda, Bader - 2009 - Tensor Decompositions and Applications.pdf:pdf},
issn = {0036-1445},
journal = {SIAM Review},
month = {aug},
number = {3},
pages = {455--500},
title = {{Tensor Decompositions and Applications}},
url = {http://epubs.siam.org/doi/abs/10.1137/07070111X},
volume = {51},
year = {2009}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Rumelhart, Hinton, Williams - 1986 - Learning representations by back-propagating erros.pdf:pdf},
isbn = {0262661160},
issn = {0028-0836},
journal = {Nature},
number = {6088},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learning+representations+by+back-propagating+errors{\&}ots=zZDj2mGYVQ{\&}sig=mcyEACaE{\_}ZB4FB4xsoTgXgcbE2g$\backslash$nhttp://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learn},
volume = {323},
year = {1986}
}
@article{Dinarelli2016,
abstract = {In this paper we study different types of Recurrent Neural Networks (RNN) for sequence labeling tasks. We propose two new variants of RNNs integrating improvements for sequence labeling, and we compare them to the more traditional Elman and Jordan RNNs. We compare all models, either traditional or new, on four distinct tasks of sequence labeling: two on Spoken Language Understanding (ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the Penn Treebank (PTB) corpora. The results show that our new variants of RNNs are always more effective than the others.},
archivePrefix = {arXiv},
arxivId = {1606.02555},
author = {Dinarelli, Marco and Tellier, Isabelle},
eprint = {1606.02555},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Dinarelli, Tellier - 2016 - Improving Recurrent Neural Networks For Sequence Labelling.pdf:pdf},
month = {jun},
title = {{Improving Recurrent Neural Networks For Sequence Labelling}},
url = {http://arxiv.org/abs/1606.02555},
year = {2016}
}
@article{Salimans2016,
author = {Salimans, Tim and Kingma, Diederik P.},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Salimans, Kingma - 2016 - Weight Normalization A Simple Reparameterization to Accelerate Training of Deep Neural Networks.pdf:pdf},
title = {{salimans - weight normalization, a simple reparameterization to accelerate training of DNNs}},
year = {2016}
}
@article{Osedelets2011,
author = {Osedelets, I. V.},
doi = {10.1137/090750688},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Osedelets - 2011 - Tensor Train Decomposition.pdf:pdf},
isbn = {0001405101},
journal = {SIAM J Sci. Comput.},
number = {5},
pages = {2295--2317},
title = {{Tensor Train Decomposition}},
volume = {33},
year = {2011}
}
@article{Tucker1966,
abstract = {The model for three-mode factor analysis is discussed in terms of newer applications of mathematical processes including a type of matrix process termed the Kronecker product and the definition of combination variables. Three methods of analysis to a type of extension of principal components analysis are discussed. Methods II and III are applicable to analysis of data collected for a large sample of individuals. An extension of the model is described in which allowance is made for unique variance for each combination variable when the data are collected for a large sample of individuals.},
author = {Tucker, Ledyard R},
doi = {10.1007/BF02289464},
issn = {1860-0980},
journal = {Psychometrika},
number = {3},
pages = {279--311},
title = {{Some mathematical notes on three-mode factor analysis}},
url = {http://dx.doi.org/10.1007/BF02289464},
volume = {31},
year = {1966}
}
@article{Martens2011a,
abstract = {Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely diffi- cult to train them properly. Fortunately, re- cent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence prob- lems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free op- timizer (HF) by applying them to character-level language modeling tasks. The standard RNN ar- chitecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or gated) con- nections which allow the current input charac- ter to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character- level language modeling a hierarchical non- parametric sequence model. To our knowledge this represents the largest recurrent neural net- work application to date.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Sutskever, Ilya and Martens, James and Hinton, Geoffrey},
doi = {2},
eprint = {9809069v1},
file = {:Users/pfcmathews/Downloads/LANG-RNN.pdf:pdf},
isbn = {9781450306195},
issn = {1},
journal = {Neural Networks},
number = {1},
pages = {1017--1024},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Generating Text with Recurrent Neural Networks}},
url = {http://www.icml-2011.org/papers/524{\_}icmlpaper.pdf},
volume = {131},
year = {2011}
}
@article{Koutnik2014,
abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is required. This paper introduces a simple, yet powerful modification to the simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of SRN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification, where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.3511v1},
author = {Koutnik, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Juergen},
eprint = {arXiv:1402.3511v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Koutnik et al. - 2014 - A Clockwork RNN.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of The 31st International Conference on Machine Learning},
pages = {1863--1871},
title = {{A Clockwork RNN}},
url = {http://jmlr.org/proceedings/papers/v32/koutnik14.html},
volume = {32},
year = {2014}
}
@article{Zaremba2014,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
doi = {ng},
eprint = {1409.2329},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Dropout improves Recurrent Neural Networks for Handwriting Recognition.pdf:pdf},
isbn = {078036404X},
journal = {arXiv preprint},
keywords = {Natural Language Processing,Recurrent Neural Netw},
title = {{Recurrent Neural Network Regularization}},
url = {http://arxiv.org/abs/1409.2329},
year = {2014}
}
@article{Pascanu2013,
abstract = {In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.},
archivePrefix = {arXiv},
arxivId = {1312.6026},
author = {Pascanu, Razvan and G{\"{u}}l{\c{c}}ehre, {\c{C}}aglar and Cho, Kyunghyun and Bengio, Yoshua and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1312.6026},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Pascanu et al. - Unknown - How to Construct Deep Recurrent Neural Networks.pdf:pdf},
journal = {CoRR},
pages = {1--10},
title = {{How to Construct Deep Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1312.6026},
volume = {abs/1312.6},
year = {2013}
}
@article{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the cor- responding words in the output sequence. We validate the use of attention with state-of-the- art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO. 1.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Xu, Kelvin and Kiros, Jimmy Lei Ba Ryan and Courville, Kyunghyun Cho Aaron and Bengio, Ruslan Salakhutdinov Richard S. Zemel Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:Users/pfcmathews/Downloads/1502.03044v3.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {arXiv preprint},
pmid = {18267787},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
year = {2015}
}
@article{Memisevic2011,
abstract = {A fundamental operation in many vision tasks, including motion understanding, stereopsis, visual odometry, or invariant recognition, is establishing correspondences between images or between images and data from other modalities. We present an analysis of the role that multiplicative interactions play in learning such correspondences, and we show how learning and inferring relationships between images can be viewed as detecting rotations in the eigenspaces shared among a set of orthogonal matrices. We review a variety of recent multiplicative sparse coding methods in light of this observation. We also review how the squaring operation performed by energy models and by models of complex cells can be thought of as a way to implement multiplicative interactions. This suggests that the main utility of including complex cells in computational models of vision may be that they can encode relations not invariances.},
archivePrefix = {arXiv},
arxivId = {1110.0107},
author = {Memisevic, Roland},
doi = {10.1109/TPAMI.2013.53},
eprint = {1110.0107},
file = {:Users/pfcmathews/Downloads/1110.0107.pdf:pdf},
issn = {01628828},
journal = {arXiv preprint arXiv:1110.0107},
pages = {1--32},
pmid = {23787339},
title = {{Learning to relate images: Mapping units, complex cells and simultaneous eigenspaces}},
url = {http://arxiv.org/abs/1110.0107},
year = {2011}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Pascanu, Mikolov, Bengio - 2012 - On the difficulty of training recurrent neural networks.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}
@article{Bertschinger2004,
abstract = {Depending on the connectivity, recurrent networks of simple computational units can show very different types of dynamics, ranging from totally ordered to chaotic. We analyze how the type of dynamics (ordered or chaotic) exhibited by randomly connected networks of threshold gates driven by a time-varying input signal depends on the parameters describing the distribution of the connectivity matrix. In particular, we calculate the critical boundary in parameter space where the transition from ordered to chaotic dynamics takes place. Employing a recently developed framework for analyzing real-time computations, we show that only near the critical boundary can such networks perform complex computations on time series. Hence, this result strongly supports conjectures that dynamical systems that are capable of doing complex computational tasks should operate near the edge of chaos, that is, the transition from ordered to chaotic dynamics.},
author = {Bertschinger, Nils and Natschl{\"{a}}ger, Thomas},
doi = {10.1162/089976604323057443},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Bertschinger, Natschl{\"{a}}ger - 2004 - Real-time computation at the edge of chaos in recurrent neural networks.pdf:pdf},
isbn = {0899-7667 (Print)},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Animals,Artificial Intelligence,Chaos,Computer Simulation,Feedback,Memory,Models,Neural Networks (Computer),Neurological,Nonlinear Dynamics,Time Factors},
number = {7},
pages = {1413--36},
pmid = {15165396},
title = {{Real-time computation at the edge of chaos in recurrent neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15165396},
volume = {16},
year = {2004}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {arXiv:1308.0850v5},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Graves - 2013 - Generating Sequences With Recurrent Neural Networks.pdf:pdf},
isbn = {2000201075},
issn = {18792782},
journal = {arXiv preprint},
pages = {1--43},
pmid = {23459267},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Panda2009,
abstract = {Classification and regression tree learning on massive datasets is a common data mining task at Google, yet many state of the art tree learning algorithms require training data to reside in memory on a single machine. While more scalable implementations of tree learning have been proposed, they typically require specialized parallel computing architectures. In contrast, the majority of Google's computing infrastructure is based on commodity hardware. In this paper, we describe PLANET: a scalable distributed framework for learning tree models over large datasets. PLANET defines tree learning as a series of distributed computations, and implements each one using the MapReduce model of distributed computation. We show how this framework supports scalable construction of classification and regression trees, as well as ensembles of such models. We discuss the benefits and challenges of using a MapReduce compute cluster for tree learning, and demonstrate the scalability of this approach by applying it to a real world learning task from the domain of computational advertising.},
author = {Panda, Biswanath and Herbach, Joshua S and Basu, Sugato and Bayardo, Roberto J},
doi = {10.14778/1687553.1687569},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Panda et al. - 2009 - PLANET massively parallel learning of tree ensembles with MapReduce.pdf:pdf},
isbn = {0000000000000},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
number = {2},
pages = {1426--1437},
title = {{PLANET: massively parallel learning of tree ensembles with MapReduce}},
url = {http://portal.acm.org/citation.cfm?id=1687553.1687569},
volume = {2},
year = {2009}
}
@article{Zamir,
abstract = {Users of Web search engines are often forced to sift through the long ordered list of document " snippets " returned by the engines. The IR community has explored document clustering as an alternative method of organizing retrieval results, but clustering has yet to be deployed on the major search engines. The paper articulates the unique requirements of Web document clustering and reports on the first evaluation of clustering methods in this domain. A key requirement is that the methods create their clusters based on the short snippets returned by Web search engines. Surprisingly, we find that clusters based on snippets are almost as good as clusters created using the full text of Web documents. To satisfy the stringent requirements of the Web domain, we introduce an incremental, linear time (in the document collection size) algorithm called Suffix Tree Clustering (STC), which creates clusters based on phrases shared between documents. We show that STC is faster than standard clustering methods in this domain, and argue that Web document clustering via STC is both feasible and potentially beneficial.},
author = {Zamir, Oren and Etzioni, Oren},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Zamir, Etzioni - Unknown - Web Document Clustering A Feasibility Demonstration.pdf:pdf},
title = {{Web Document Clustering: A Feasibility Demonstration}}
}
@article{Huang2016,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1)/2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74{\%} test error on CIFAR-10, 19.25{\%} on CIFAR-100 and 1.59{\%} on SVHN).},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q.},
eprint = {1608.06993},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Liu, Weinberger - 2016 - Densely Connected Convolutional Networks.pdf:pdf},
journal = {arXiv preprint},
pages = {1--12},
title = {{Densely Connected Convolutional Networks}},
url = {http://arxiv.org/abs/1608.06993},
year = {2016}
}
@article{Cho2014b,
abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1259v2},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
eprint = {arXiv:1409.1259v2},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - On the Properties of Neural Machine Translation Encoder-Decoder Approaches.pdf:pdf},
isbn = {9781937284961},
journal = {Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
month = {sep},
pages = {103--111},
title = {{On the Properties of Neural Machine Translation: Encoder–Decoder Approaches}},
url = {http://arxiv.org/abs/1409.1259 http://arxiv.org/pdf/1409.1259v2.pdf$\backslash$nhttp://arxiv.org/abs/1409.1259},
year = {2014}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Glorot, Bengio - Unknown - Understanding the difficulty of training deep feedforward neural networks(2).pdf:pdf},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Kaiser2015,
abstract = {Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they cannot be parallelized and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.},
archivePrefix = {arXiv},
arxivId = {1511.08228},
author = {Kaiser, {\L}ukasz and Sutskever, Ilya},
doi = {10.2307/2486811},
eprint = {1511.08228},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Kaiser, Sutskever - 2015 - Neural GPUs Learn Algorithms.pdf:pdf},
isbn = {1066100802},
issn = {1097-0266},
journal = {arXiv:1511.08228 [cs]},
pages = {1--9},
pmid = {1423975},
title = {{Neural GPUs Learn Algorithms}},
url = {http://arxiv.org/abs/1511.08228$\backslash$nhttp://www.arxiv.org/pdf/1511.08228.pdf},
year = {2015}
}
@article{Jean2014,
archivePrefix = {arXiv},
arxivId = {1412.2007},
author = {Jean, S{\'{e}}bastien and Cho, Kyunghyun and Memisevic, Roland and Bengio, Yoshua},
eprint = {1412.2007},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Jean et al. - 2014 - On Using Very Large Target Vocabulary for Neural Machine Translation.pdf:pdf},
title = {{On using very large target vocabulary for neural machine translation}},
year = {2014}
}
@article{Karpathy2016,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02078v1},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1506.02078v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {ICLR},
month = {jun},
pages = {1--13},
title = {{Visualizing and Understanding Recurrent Networks}},
url = {http://arxiv.org/abs/1506.02078},
year = {2016}
}
@article{Jaderberg2016,
abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local infor-mation. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1608.05343v1},
author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
eprint = {arXiv:1608.05343v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Jaderberg et al. - Unknown - Decoupled Neural Interfaces using Synthetic Gradients.pdf:pdf},
journal = {arXiv:1608.05343},
title = {{Decoupled neural interfaces using synthetic gradients}},
url = {http://arxiv.org/abs/1608.05343},
year = {2016}
}
@article{Plate1982,
abstract = {A solution to the problem of representing compo-sitional structure using distributed representations is described. The method uses circular convolution to associate items, which are represented by vec-tors. Arbitrary variable bindings, short sequences of various lengths, frames, and reduced represen-tations can be compressed into a fixed width vec-tor. These representations are items in their own right, and can be used in constructing compositional structures. The noisy reconstructions given by convolution memories can be cleaned up by using a separate associative memory that has good recon-structive properties.},
author = {Plate, Tony},
file = {:Users/pfcmathews/Downloads/006.pdf:pdf},
journal = {Science},
pages = {30--35},
pmid = {1000185211},
title = {{Holographic Reduced Representations: Convolutional for Compositional Distributed Representations}},
year = {1982}
}
@article{Auer2002,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we showthat the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support},
author = {Auer, P and Cesa-bianchi, N and Fischer, P},
doi = {10.1023/A:1013689704352},
file = {:Users/pfcmathews/Downloads/ml-02.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {adaptive allocation rules,bandit problems,finite horizon regret},
number = {2-3},
pages = {235--256},
title = {{Finite time analysis of the multiarmed bandit problem}},
volume = {47},
year = {2002}
}
@article{Dietterich2000,
abstract = {Publication View. 42637098. Dietterich TG: for Constructing Ensembles of Decision Trees (2000). Shuly Wintner. Abstract. While the morphology of Modem Hebrew is well accounted},
author = {Dietterich, Thomas G.},
doi = {10.1023/A:1007607513941},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Dietterich - 2000 - An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {5,bagging,boosting,c4,decision trees,ensemble learning,monte carlo methods},
pages = {139--157},
title = {{An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees}},
url = {http://en.scientificcommons.org/42637098$\backslash$nuuid/7906280C-AEF8-405A-9A94-6BAA1DDAED1E},
volume = {40},
year = {2000}
}
@article{Grefenstette2015,
abstract = {Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.},
archivePrefix = {arXiv},
arxivId = {1506.02516},
author = {Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
doi = {10.1103/PhysRevLett.115.218702},
eprint = {1506.02516},
file = {:Users/pfcmathews/Downloads/1506.02516.pdf:pdf},
issn = {10495258},
journal = {arXiv preprint},
pages = {12},
title = {{Learning to Transduce with Unbounded Memory}},
url = {http://arxiv.org/abs/1506.02516},
year = {2015}
}
@article{Marcus1993,
abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.},
author = {Marcus, Mitchell P and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
doi = {10.1162/coli.2010.36.1.36100},
file = {:Users/pfcmathews/Downloads/fulltext.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
keywords = {POS-Tagging},
number = {2},
pages = {313--330},
title = {{Building a large annotated corpus of English: The Penn Treebank.}},
volume = {19},
year = {1993}
}
@article{Breiman1996,
abstract = {Bagging predictors is a method for generating multiple versions of a pre-dictor and using these to get an aggregated predictor. The aggregation av-erages over the versions when predicting a numerical outcome and does a plurality v ote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classiication and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability o f the prediction method. If perturbing the learning set can cause signiicant changes in the predictor constructed, then bagging can improve accuracy.},
author = {Breiman, Leo},
doi = {10.1007/BF00058655},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 1996 - Bagging predictors.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {aggregation,averaging,bootstrap,combining},
pages = {123--140},
pmid = {17634459},
title = {{Bagging Predictors}},
url = {http://link.springer.com/article/10.1007/BF00058655},
volume = {24},
year = {1996}
}
@article{He2016,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which further makes training easy and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10/100, and a 200-layer ResNet on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1603.05027},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:pdf},
journal = {arXiv preprint},
month = {mar},
pages = {1--15},
title = {{Identity Mappings in Deep Residual Networks}},
url = {http://arxiv.org/abs/1603.05027},
year = {2016}
}
@article{Laurent2015,
abstract = {Recurrent Neural Networks (RNNs) are powerful models for sequential data that have the potential to learn long-term dependencies. However, they are computationally expensive to train and difficult to parallelize. Recent work has shown that normalizing intermediate representations of neural networks can significantly improve convergence rates in feedforward neural networks . In particular, batch normalization, which uses mini-batch statistics to standardize features, was shown to significantly reduce training time. In this paper, we show that applying batch normalization to the hidden-to-hidden transitions of our RNNs doesn't help the training procedure. We also show that when applied to the input-to-hidden transitions, batch normalization can lead to a faster convergence of the training criterion but doesn't seem to improve the generalization performance on both our language modelling and speech recognition tasks. All in all, applying batch normalization to RNNs turns out to be more challenging than applying it to feedforward networks, but certain variants of it can still be beneficial.},
archivePrefix = {arXiv},
arxivId = {1510.01378},
author = {Laurent, C{\'{e}}sar and Pereyra, Gabriel and Brakel, Phil{\'{e}}mon and Zhang, Ying and Bengio, Yoshua},
eprint = {1510.01378},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Laurent, Pereyra, Zhang - Unknown - Batch Normalized Recurrent Neural Networks.pdf:pdf},
journal = {arXiv},
pages = {1--9},
title = {{Batch Normalized Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1510.01378},
year = {2015}
}
@article{Saxe2013,
abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
archivePrefix = {arXiv},
arxivId = {1312.6120},
author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
eprint = {1312.6120},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Saxe, McClelland, Ganguli - 2013 - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.pdf:pdf},
isbn = {1312.6120},
journal = {Advances in Neural Information Processing Systems},
month = {dec},
pages = {1--9},
title = {{Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}},
url = {http://arxiv.org/abs/1312.6120},
year = {2013}
}
@article{Plate1995,
author = {Plate, Tony},
file = {:Users/pfcmathews/Downloads/10.1.1.33.4546.pdf:pdf},
journal = {IEEE Transactions on Neural Networks},
number = {3},
pages = {623--641},
title = {{Holographic Reduced Representations}},
volume = {6},
year = {1995}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Novikov,
abstract = {Deep neural networks currently demonstrate state-of-the-art performance in sev- eral domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06569v1},
author = {Novikov, Alexander and Vetrov, Dmitry and Podoprikhin, Dimitry and Osokin, Anton},
eprint = {arXiv:1509.06569v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Novikov et al. - Unknown - Tensorizing Neural Networks.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Tensorizing Neural Networks}},
url = {http://arxiv.org/pdf/1509.06569v1.pdf},
year = {2015}
}
