Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Cichocki2016,
abstract = {Machine learning and data mining algorithms are becoming increasingly important in analyzing large volume, multi-relational and multi--modal datasets, which are often conveniently represented as multiway arrays or tensors. It is therefore timely and valuable for the multidisciplinary research community to review tensor decompositions and tensor networks as emerging tools for large-scale data analysis and data mining. We provide the mathematical and graphical representations and interpretation of tensor networks, with the main focus on the Tucker and Tensor Train (TT) decompositions and their extensions or generalizations. Keywords: Tensor networks, Function-related tensors, CP decomposition, Tucker models, tensor train (TT) decompositions, matrix product states (MPS), matrix product operators (MPO), basic tensor operations, multiway component analysis, multilinear blind source separation, tensor completion, linear/multilinear dimensionality reduction, large-scale optimization problems, symmetric eigenvalue decomposition (EVD), PCA/SVD, huge systems of linear equations, pseudo-inverse of very large matrices, Lasso and Canonical Correlation Analysis (CCA) (This is Part 1)},
archivePrefix = {arXiv},
arxivId = {1609.00893},
author = {Cichocki, A. and Lee, N. and Oseledets, I. V. and Phan, A-H. and Zhao, Q. and Mandic, D.},
eprint = {1609.00893},
file = {:Users/pfcmathews/Downloads/1609.00893v1.pdf:pdf},
journal = {arXiv preprint},
pages = {100},
title = {{Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale Optimization Problems: Perspectives and Challenges PART 1}},
url = {http://arxiv.org/abs/1609.00893},
year = {2016}
}
@article{Schaefers2014,
abstract = {Monte-Carlo Tree Search (MCTS) has brought about great success regarding the evaluation of stochastic and deterministic games in recent years. We present and empirically analyze a data-driven parallelization approach for MCTS targeting large HPC clusters with Infiniband interconnect. Our implementation is based on OpenMPI and makes extensive use of its RDMA based asynchronous tiny message communication capabilities for effectively overlapping communication and computation. We integrate our parallel MCTS approach termed UCTTreesplit in our state-of-the-art Go engine Gomorra and measure its strengths and limitations in a real-world setting. Our extensive experiments show that we can scale up to 128 compute nodes and 2048 cores in self-play experiments and, furthermore, give promising directions for additional improvement. The generality of our parallelization approach advocates its use to significantly improve the search quality of a huge number of current MCTS applications.},
author = {Schaefers, Lars and Platzner, Marco},
doi = {10.1109/TCIAIG.2014.2346997},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - 06876158.pdf:pdf},
issn = {1943-068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Computational modeling,Computers,Data models,Games,Indexes,Memory management,Monte Carlo methods},
number = {99},
pages = {1--1},
title = {{Distributed Monte-Carlo Tree Search: A Novel Technique and its Application to Computer Go}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6876158},
volume = {PP},
year = {2014}
}
@article{Arjovsky2015,
abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.},
archivePrefix = {arXiv},
arxivId = {1511.06464},
author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
eprint = {1511.06464},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Arjovsky, Shah, Bengio - 2015 - Unitary Evolution Recurrent Neural Networks.pdf:pdf},
journal = {arXiv},
month = {nov},
pages = {1--11},
title = {{Unitary Evolution Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1511.06464},
year = {2015}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {3111--3119},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Osedelets2011,
author = {Osedelets, I. V.},
doi = {10.1137/090750688},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Osedelets - 2011 - Tensor Train Decomposition.pdf:pdf},
isbn = {0001405101},
journal = {SIAM J Sci. Comput.},
number = {5},
pages = {2295--2317},
title = {{Tensor Train Decomposition}},
volume = {33},
year = {2011}
}
@article{Laurent2015,
abstract = {Recurrent Neural Networks (RNNs) are powerful models for sequential data that have the potential to learn long-term dependencies. However, they are computationally expensive to train and difficult to parallelize. Recent work has shown that normalizing intermediate representations of neural networks can significantly improve convergence rates in feedforward neural networks . In particular, batch normalization, which uses mini-batch statistics to standardize features, was shown to significantly reduce training time. In this paper, we show that applying batch normalization to the hidden-to-hidden transitions of our RNNs doesn't help the training procedure. We also show that when applied to the input-to-hidden transitions, batch normalization can lead to a faster convergence of the training criterion but doesn't seem to improve the generalization performance on both our language modelling and speech recognition tasks. All in all, applying batch normalization to RNNs turns out to be more challenging than applying it to feedforward networks, but certain variants of it can still be beneficial.},
archivePrefix = {arXiv},
arxivId = {1510.01378},
author = {Laurent, C{\'{e}}sar and Pereyra, Gabriel and Brakel, Phil{\'{e}}mon and Zhang, Ying and Bengio, Yoshua},
eprint = {1510.01378},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Laurent et al. - 2015 - Batch Normalized Recurrent Neural Networks.pdf:pdf},
journal = {arXiv},
pages = {1--9},
title = {{Batch Normalized Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1510.01378},
year = {2015}
}
@article{Jaderberg2016,
abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local infor-mation. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1608.05343v1},
author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
eprint = {arXiv:1608.05343v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Jaderberg et al. - Unknown - Decoupled Neural Interfaces using Synthetic Gradients.pdf:pdf},
journal = {arXiv:1608.05343},
title = {{Decoupled neural interfaces using synthetic gradients}},
url = {http://arxiv.org/abs/1608.05343},
year = {2016}
}
@article{Tenenbaum2000,
abstract = {Perceptual systems routinely separate "content" from "style," classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, {\&} Bibby, 1979; Hinton {\&} Zemel, 1994; Ghahramani, 1995; Bell {\&} Sejnowski, 1995; Hinton, Dayan, Frey, {\&} Neal, 1995; Dayan, Hinton, Neal, {\&} Zemel, 1995; Hinton {\&} Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants.},
author = {Tenenbaum, Joshua B and Freeman, W T},
doi = {10.1162/089976600300015349},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Tenenbaum, Freeman MERL - Unknown - Separating Style and Content.pdf:pdf},
isbn = {0899-7667 (Print)$\backslash$r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {6},
pages = {1247--1283},
pmid = {10935711},
title = {{Separating style and content with bilinear models.}},
volume = {12},
year = {2000}
}
@book{Quinlan2014,
abstract = {Describes a tree-based classifier. It handles un-ordered domains and null/missing values. It uses entropy arguments to choose attributes on which to discriminate. Interesting points: 1. pointer to a db repository: ml-repositoryics.uci.edu (UC Irvine) 2. comparison with statistics-based systems (p. 15) "As a general rule ... statistical techniques tend to focus on tasks in which all the attributes have continuous or ordinal values" S.M. Weiss and C.A. Kulikowski "Computer Systems that Learn", Morgan Kaufmann, San Mateo CA, 1991 provide a comparison 3. MDL principle, to decide how to prune the tree Jorman Rissanen, Anal of Statistics, 11,2, 416-431 Quinlan and Rivest, "Inferring decision trees using the Minimum Description Length principle" Information and Computation 80,3, 227-248 4. comparison of tree-based classifiers with neural nets (NN) - they are both more robust - they are about equally accurate (with NN slightly ahead); but NN require much more computation (an order of magnitude more) (p. 102) 5. CART is a statistics-based program L. Breiman, J.H. Friedman, R.A. Olshen and C.J. Stone "Classification and Regression Trees" Belmont, CA: Wadsworth (1984) 6. citation: Hunt 75 "Artificial Intelligence" NY, Academic Press (pioneered the tree-based classification methods)},
author = {Quinlan, JR},
booktitle = {Machine Learning},
doi = {10.1016/S0019-9958(62)90649-6},
isbn = {1558602380},
issn = {08856125},
pages = {302},
pmid = {21786264},
title = {{C4. 5: programs for machine learning}},
url = {https://books.google.co.nz/books?hl=en{\&}lr={\&}id=b3ujBQAAQBAJ{\&}oi=fnd{\&}pg=PP1{\&}dq=related:O9AvHGeAh{\_}{\_}1sM:scholar.google.com/{\&}ots=sP6vTKJmG8{\&}sig=zDWOEdzpX1IBuRGtKyqGPR5XWuU},
volume = {240},
year = {1993}
}
@article{VanderMaaten2008,
abstract = {We present a new technique called " t-SNE " that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.},
author = {van der Maaten, Laurens and Hinton, Geoffrey},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/van der Maaten, Hinton - 2008 - Visualizing Data using t-SNE.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Visualization,dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling},
pages = {1--48},
title = {{Visualizing Data using t-SNE}},
volume = {1},
year = {2008}
}
@article{Lecun1998,
author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
doi = {10.1109/5.726791},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Lecun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {2D shape variability,Character recognition,Feature extraction,GTN,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis,back-propagation,backpropagation,cheque reading,complex decision surface synthesis,convolution,convolutional neural network character recognizers,document recognition,document recognition systems,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,handwritten character recognition,handwritten digit recognition task,high-dimensional patterns,language modeling,multilayer neural networks,multilayer perceptrons,multimodule systems,optical character recognition,performance measure minimization,segmentation recognition},
number = {11},
pages = {2278--2324},
publisher = {IEEE},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=726791},
volume = {86},
year = {1998}
}
@article{TinKamHo1995,
abstract = {Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits},
author = {{Tin Kam Ho}},
doi = {10.1109/ICDAR.1995.598994},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Tin Kam Ho - 1995 - Random decision forests.pdf:pdf},
isbn = {0-8186-7128-9},
journal = {Proceedings of 3rd International Conference on Document Analysis and Recognition},
keywords = {Classification tree analysis,Decision trees,Handwriting recognition,Hidden Markov models,Multilayer perceptrons,Optimization methods,Stochastic processes,Testing,Tin,Training data,complexity,decision theory,generalization accuracy,handwritten digits,optical character recognition,random decision forests,stochastic modeling,suboptimal accuracy,tree-based classifiers},
pages = {278--282},
title = {{Random decision forests}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=598994},
volume = {1},
year = {1995}
}
@article{Neyshabur2015,
abstract = {We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.},
archivePrefix = {arXiv},
arxivId = {1506.02617},
author = {Neyshabur, Behnam and Salakhutdinov, Ruslan and Srebro, Nathan},
eprint = {1506.02617},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Neyshabur, Salakhutdinov, Srebro - 2015 - Path-SGD Path-Normalized Optimization in Deep Neural Networks.pdf:pdf},
issn = {10495258},
journal = {arXiv},
pages = {1--12},
title = {{Path-SGD: Path-Normalized Optimization in Deep Neural Networks}},
url = {http://arxiv.org/abs/1506.02617},
year = {2015}
}
@article{Larsson2016,
abstract = {We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a single expansion rule generates an extremely deep network whose structural layout is precisely a truncated fractal. Such a network contains interacting subpaths of different lengths, but does not include any pass-through connections: every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. This property stands in stark contrast to the current approach of explicitly structuring very deep networks so that training is a residual learning problem. Our experiments demonstrate that residual representation is not fundamental to the success of extremely deep convolutional neural networks. A fractal design achieves an error rate of 22.85{\%} on CIFAR-100, matching the state-of-the-art held by residual networks. Fractal networks exhibit intriguing properties beyond their high performance. They can be regarded as a computationally efficient implicit union of subnetworks of every depth. We explore consequences for training, touching upon connection with student-teacher behavior, and, most importantly, demonstrating the ability to extract high-performance fixed-depth subnetworks. To facilitate this latter task, we develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. With such regularization, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.},
archivePrefix = {arXiv},
arxivId = {1605.07648},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
eprint = {1605.07648},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Larsson, Maire, Shakhnarovich - 2016 - FractalNet Ultra-Deep Neural Networks without Residuals(2).pdf:pdf},
journal = {arXiv preprint},
month = {may},
title = {{FractalNet: Ultra-Deep Neural Networks without Residuals}},
url = {http://arxiv.org/abs/1605.07648},
year = {2016}
}
@article{Karpathy2016,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02078v1},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1506.02078v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {ICLR},
month = {jun},
pages = {1--13},
title = {{Visualizing and Understanding Recurrent Networks}},
url = {http://arxiv.org/abs/1506.02078},
year = {2016}
}
@article{Rousseeuw1987,
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate' number of clusters.},
author = {Rousseeuw, Peter J.},
doi = {10.1016/0377-0427(87)90125-7},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Rousseeuw - 1987 - Silhouettes A graphical aid to the interpretation and validation of cluster analysis.pdf:pdf},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
pages = {53--65},
title = {{Silhouettes: A graphical aid to the interpretation and validation of cluster analysis}},
volume = {20},
year = {1987}
}
@article{Sutskever2013a,
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
author = {Sutskever, Ilya and Martens, James and Dahl, George E and Hinton, Geoffrey E},
doi = {10.1109/ICASSP.2013.6639346},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever et al. - 2013 - On the importance of initialization and momentum in deep learning.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {15206149},
journal = {Jmlr},
keywords = {dblp},
number = {2010},
pages = {1139--1147},
title = {{On the importance of initialization and momentum in deep learning}},
url = {http://dblp.uni-trier.de/db/conf/icml/icml2013.html{\#}SutskeverMDH13},
volume = {28},
year = {2013}
}
@article{Meinshausen2006,
abstract = {The pattern of zero entries in the inverse covariance matrix of a multivariate normal distribution corresponds to conditional independence restrictions between variables. Covariance selection aims at estimating those structural zeros from data. We show that neighborhood selection with the Lasso is a computationally attractive alternative to standard covariance selection for sparse high-dimensional graphs. Neighborhood selection estimates the conditional independence restrictions separately for each node in the graph and is hence equivalent to variable selection for Gaussian linear models. We show that the proposed neighborhood selection scheme is consistent for sparse high-dimensional graphs. Consistency hinges on the choice of the penalty parameter. The oracle value for optimal prediction does not lead to a consistent neighborhood estimate. Controlling instead the probability of falsely joining some distinct connectivity components of the graph, consistent estimation for sparse graphs is achieved (with exponential rates), even when the number of variables grows as the number of observations raised to an arbitrary power.},
archivePrefix = {arXiv},
arxivId = {math/0608017},
author = {Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
doi = {10.1214/009053606000000281},
eprint = {0608017},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Meinshausen, B{\"{u}}hlmann - 2006 - High-dimensional graphs and variable selection with the Lasso.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Covariance selection,Gaussian graphical models,Linear regression,Penalized regression},
number = {3},
pages = {1436--1462},
pmid = {239471300013},
primaryClass = {math},
title = {{High-dimensional graphs and variable selection with the Lasso}},
volume = {34},
year = {2006}
}
@article{Karpathy2015,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2014}
}
@article{Chen2016,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
archivePrefix = {arXiv},
arxivId = {1603.02754},
author = {Chen, Tianqi and Guestrin, Carlos},
eprint = {1603.02754},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:pdf},
month = {mar},
title = {{XGBoost: A Scalable Tree Boosting System}},
url = {http://arxiv.org/abs/1603.02754},
year = {2016}
}
@article{Hitchcock1928,
author = {Hitchcock, Frank L},
doi = {10.1002/sapm19287139},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Hitchcock - 1928 - Multiple Invariants and Generalized Rank of a P-Way Matrix or Tensor.pdf:pdf},
issn = {00971421},
journal = {Journal of Mathematics and Physics},
month = {apr},
number = {1-4},
pages = {39--79},
title = {{Multiple Invariants and Generalized Rank of a P-Way Matrix or Tensor}},
url = {http://doi.wiley.com/10.1002/sapm19287139},
volume = {7},
year = {1928}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:Users/pfcmathews/Downloads/1211.5063v2.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}
@article{Salimans2016a,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.07868v1},
author = {Salimans, Tim},
eprint = {arXiv:1602.07868v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Salimans - 2016 - Weight Normalization A Simple Reparameterization to Accelerate Training of Deep Neural Networks.pdf:pdf},
title = {{Weight Normalization : A Simple Reparameterization to Accelerate Training of Deep Neural Networks}},
year = {2016}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Qv and Mikolov, Tomas},
eprint = {1405.4053},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
journal = {International Conference on Machine Learning - ICML 2014},
pages = {1188--1196},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Mikolov2015,
abstract = {Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming a kind of longer term memory. We evaluate our model on language modeling tasks on benchmark datasets, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter {\&} Schmidhuber, 1997).},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.7753v1},
author = {Mikolov, Tomas and Joulin, Armand and Chopra, Sumit and Mathieu, Michael},
eprint = {arXiv:1412.7753v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2014 - Learning Longer Memory in Recurrent Neural Networks.pdf:pdf},
journal = {Iclr},
month = {dec},
pages = {1--9},
title = {{Learning Longer Memory in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1412.7753 http://arxiv.org/pdf/1412.7753v1.pdf},
year = {2015}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at rel-atively low computational cost. Recently, the introduction of residual connections in conjunction with a more tradi-tional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Incep-tion networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These varia-tions improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We fur-ther demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent},
eprint = {1602.07261},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy, Ioffe, Vanhoucke - 2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf:pdf},
journal = {Arxiv},
pages = {12},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
year = {2016}
}
@book{Minsky1969,
author = {Minsky, Marvin and Papert, Seymour},
publisher = {M.I.T. Press},
title = {{Perceptrons.}},
year = {1969}
}
@incollection{Hochreiter2001,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"{u}}rgen},
booktitle = {A Field Guide to Dynamical Recurrent Networks},
chapter = {Gradient F},
doi = {10.1109/9780470544037.ch14},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-7803-5369-5},
issn = {1098-6596},
pmid = {25246403},
publisher = {IEEE Press},
title = {{Gradient Flow in Recurrent Nets: The Difficulty of Learning LongTerm Dependencies}},
url = {http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=5264952},
year = {2001}
}
@article{Salimans2016,
author = {Salimans, Tim and Kingma, Diederik P.},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Salimans, Kingma - 2016 - salimans - weight normalization, a simple reparameterization to accelerate training of DNNs.pdf:pdf},
title = {{salimans - weight normalization, a simple reparameterization to accelerate training of DNNs}},
year = {2016}
}
@article{Jean2014,
archivePrefix = {arXiv},
arxivId = {1412.2007},
author = {Jean, S{\'{e}}bastien and Cho, Kyunghyun and Memisevic, Roland and Bengio, Yoshua},
eprint = {1412.2007},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Jean et al. - 2014 - On using very large target vocabulary for neural machine translation.pdf:pdf},
title = {{On using very large target vocabulary for neural machine translation}},
year = {2014}
}
@article{Choi2016,
abstract = {We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with two CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
archivePrefix = {arXiv},
arxivId = {1609.04243},
author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark and Cho, Kyunghyun},
eprint = {1609.04243},
journal = {arXiv preprint},
month = {sep},
title = {{Convolutional Recurrent Neural Networks for Music Classification}},
url = {http://arxiv.org/abs/1609.04243},
year = {2016}
}
@article{Novikov,
abstract = {Deep neural networks currently demonstrate state-of-the-art performance in sev- eral domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06569v1},
author = {Novikov, Alexander and Vetrov, Dmitry and Podoprikhin, Dimitry and Osokin, Anton},
eprint = {arXiv:1509.06569v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Novikov et al. - Unknown - Tensorizing Neural Networks.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Tensorizing Neural Networks}},
url = {http://arxiv.org/pdf/1509.06569v1.pdf},
year = {2015}
}
@article{Burges2010,
abstract = {LambdaMART is the boosted tree version of LambdaRank, which is based on RankNet. RankNet, LambdaRank, and LambdaMART have proven to be very suc- cessful algorithms for solving real world ranking problems: for example an ensem- ble of LambdaMART rankers won Track 1 of the 2010 Yahoo! Learning To Rank Challenge. The details of these algorithms are spread across several papers and re- ports, and so here we give a self-contained, detailed and complete description of them.},
author = {Burges, Christopher J C},
doi = {10.1111/j.1467-8535.2010.01085.x},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Burges - 2010 - From ranknet to lambdarank to lambdamart An overview.pdf:pdf},
issn = {00071013},
journal = {Learning},
pages = {23--581},
title = {{From ranknet to lambdarank to lambdamart: An overview}},
volume = {11},
year = {2010}
}
@article{Carroll1970,
abstract = {An individual differences model for multidimensional scaling is outlined in which individuals are assumed differentially to weight the several dimensions of a common “psychological space”. A corresponding method of analyzing similarities data is proposed, involving a generalization of “Eckart-Young analysis” to decomposition of three-way (or higher-way) tables. In the present case this decomposition is applied to a derived three-way table of scalar products between stimuli for individuals. This analysis yields a stimulus by dimensions coordinate matrix and a subjects by dimensions matrix of weights. This method is illustrated with data on auditory stimuli and on perception of nations.},
author = {Carroll, J. Douglas and Chang, Jih Jie},
doi = {10.1007/BF02310791},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Carroll, Chang - 1970 - Analysis of individual differences in multidimensional scaling via an n-way generalization of “Eckart-Young”.pdf:pdf},
isbn = {0033-3123},
issn = {00333123},
journal = {Psychometrika},
month = {sep},
number = {3},
pages = {283--319},
publisher = {Springer-Verlag},
title = {{Analysis of individual differences in multidimensional scaling via an n-way generalization of "Eckart-Young" decomposition}},
url = {http://link.springer.com/10.1007/BF02310791},
volume = {35},
year = {1970}
}
@article{Hihi1995,
abstract = {We have already shown that extracting long-term dependencies from sequential data is difficult, both for deterministic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.},
author = {Hihi, Salah El and Bengio, Yoshua},
file = {:Users/pfcmathews/Downloads/1102-hierarchical-recurrent-neural-networks-for-long-term-dependencies.pdf:pdf},
journal = {Nips},
pages = {493--499},
title = {{Hierarchical Recurrent Neural Networks for Long-Term Dependencies.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.7574{\&}rep=rep1{\&}type=pdf},
year = {1995}
}
@phdthesis{Sutskever2013,
abstract = {Recurrent Neural Networks (RNNs) are powerful sequence models that were believed to be difficult to train, and as a result they were rarely used in machine learning applications. This thesis presents methods that overcome the difficulty of training RNNs, and applications of RNNs to challenging problems. We first describe a newprobabilistic sequence model that combines Restricted Boltzmann Machines and RNNs. The new model is more powerful than similar models while being less difficult to train. Next, we present a new variant of the Hessian-free (HF) optimizer and show that it can train RNNs on tasks that have extreme long-range temporal dependencies, which were previously considered to be impossibly hard. We then apply HF to character-level language modelling and get excellent results. We also apply HF to optimal control and obtain RNN control laws that can successfully operate under conditions of delayed feedback and unknown disturbances. Finally, we describe a random parameter initialization scheme that allows gradient descent with mo- mentum to train RNNs on problems with long-term dependencies. This directly contradicts widespread beliefs about the inability of first-order methods to do so, and suggests that previous attempts at training RNNs failed partly due to flaws in the random initialization},
archivePrefix = {arXiv},
arxivId = {arXiv:submit/1456339},
author = {Sutskever, Ilya},
booktitle = {PhD thesis},
eprint = {1456339},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever - 2013 - Training Recurrent neural Networks.pdf:pdf},
isbn = {978-0-499-22066-0},
pages = {101},
primaryClass = {arXiv:submit},
title = {{Training Recurrent neural Networks}},
year = {2013}
}
@article{Lorenz1963,
author = {Lorenz, Edward N.},
doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
issn = {0022-4928},
journal = {Journal of the Atmospheric Sciences},
month = {mar},
number = {2},
pages = {130--141},
title = {{Deterministic Nonperiodic Flow}},
url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0469{\%}281963{\%}29020{\%}3C0130{\%}3ADNF{\%}3E2.0.CO{\%}3B2},
volume = {20},
year = {1963}
}
@article{Martens2011,
abstract = {In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-the-art method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of sch (2002) which is used within the HF approach of Martens. Copyright 2011 by the author(s)/owner(s).},
author = {Martens, James and Sutskever, Ilya},
doi = {10.1145/346152.346166},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Martens, Sutskever - 2011 - Learning recurrent neural networks with Hessian-free optimization.pdf:pdf},
isbn = {978-1-4503-0619-5},
issn = {01635980},
journal = {Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
keywords = {Learning systems,Optimization},
pages = {1033--1040},
title = {{Learning recurrent neural networks with Hessian-free optimization}},
year = {2011}
}
@article{Novikov2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.03795v1},
author = {Novikov, Alexander and Trofimov, Mikhail and Oseledets, Ivan},
eprint = {arXiv:1605.03795v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Novikov, Trofimov, Oseledets - 2016 - Tensor Train polynomial models via Riemannian optimization.pdf:pdf},
journal = {arXiv},
title = {{Tensor Train polynomial models via Riemannian optimization}},
year = {2016}
}
@article{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
doi = {10.1.1.208.6449},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Deep Sparse Rectifier Neural Networks.pdf:pdf},
issn = {15324435},
journal = {AISTATS},
pages = {315--323},
title = {{Deep Sparse Rectifier Neural Networks}},
volume = {15},
year = {2011}
}
@article{Duvenaud2014,
abstract = {Choosing appropriate architectures and regularization strategies for deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network. We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit. We propose an alternate network architecture which does not suffer from this pathology. We also examine deep covariance functions, obtained by composing infinitely many feature transforms. Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes.},
archivePrefix = {arXiv},
arxivId = {1402.5836},
author = {Duvenaud, David and Rippel, Oren and Adams, Ryan P. and Ghahramani, Zoubin},
eprint = {1402.5836},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Duvenaud et al. - 2014 - Avoiding pathologies in very deep networks.pdf:pdf},
issn = {15337928},
journal = {Mlg.Eng.Cam.Ac.Uk},
month = {feb},
pages = {9},
title = {{Avoiding pathologies in very deep networks}},
url = {http://arxiv.org/abs/1402.5836},
year = {2014}
}
@inproceedings{Mikolov2010,
author = {Mikolov, Tomas and Karafi{\'{a}}t, Martin and Burget, Luk{\'{a}}s and Cernock{\'{y}}, Jan and Khudanpur, Sanjeev},
booktitle = {Proceedings of the Conference of the International Speech Communication Association},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Recurrent neural network based language model.pdf:pdf},
pages = {1045--1048},
title = {{Recurrent neural network based language model}},
year = {2010}
}
@article{Panda2009,
abstract = {Classification and regression tree learning on massive datasets is a common data mining task at Google, yet many state of the art tree learning algorithms require training data to reside in memory on a single machine. While more scalable implementations of tree learning have been proposed, they typically require specialized parallel computing architectures. In contrast, the majority of Google's computing infrastructure is based on commodity hardware. In this paper, we describe PLANET: a scalable distributed framework for learning tree models over large datasets. PLANET defines tree learning as a series of distributed computations, and implements each one using the MapReduce model of distributed computation. We show how this framework supports scalable construction of classification and regression trees, as well as ensembles of such models. We discuss the benefits and challenges of using a MapReduce compute cluster for tree learning, and demonstrate the scalability of this approach by applying it to a real world learning task from the domain of computational advertising.},
author = {Panda, Biswanath and Herbach, Joshua S and Basu, Sugato and Bayardo, Roberto J},
doi = {10.14778/1687553.1687569},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Panda et al. - 2009 - PLANET massively parallel learning of tree ensembles with MapReduce.pdf:pdf},
isbn = {0000000000000},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
number = {2},
pages = {1426--1437},
title = {{PLANET: massively parallel learning of tree ensembles with MapReduce}},
url = {http://portal.acm.org/citation.cfm?id=1687553.1687569},
volume = {2},
year = {2009}
}
@article{Vinh2009,
author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
doi = {10.1145/1553374.1553511},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Vinh, Epps, Bailey - 2009 - Information theoretic measures for clusterings comparison is a correction for chance necessary.pdf:pdf},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1073--1080},
publisher = {ACM},
title = {{Information theoretic measures for clusterings comparison: is a correction for chance necessary?}},
year = {2009}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Glorot, Bengio - Unknown - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Cho2014a,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
pages = {1724--1734},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Rumelhart, Hinton, Williams - 1986 - Learning representations by back-propagating errors.pdf:pdf},
isbn = {0262661160},
issn = {0028-0836},
journal = {Nature},
number = {6088},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learning+representations+by+back-propagating+errors{\&}ots=zZDj2mGYVQ{\&}sig=mcyEACaE{\_}ZB4FB4xsoTgXgcbE2g$\backslash$nhttp://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learn},
volume = {323},
year = {1986}
}
@article{Nair,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Nair, Hinton - 2010 - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Henaff2016,
abstract = {Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.},
archivePrefix = {arXiv},
arxivId = {1602.06662},
author = {Henaff, Mikael and Szlam, Arthur and LeCun, Yann},
eprint = {1602.06662},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - henaff - orthogonal RNNs and long-memory tasks.pdf:pdf},
journal = {arxiv},
title = {{Orthogonal RNNs and Long-Memory Tasks}},
url = {http://arxiv.org/abs/1602.06662},
year = {2016}
}
@article{Gregor2015,
abstract = {This paper introduces the Deep Recurrent Atten-tive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin-guished from real data with the naked eye.},
archivePrefix = {arXiv},
arxivId = {1502.04623},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and {Jimenez Rezende}, Danilo and Wierstra, Daan},
eprint = {1502.04623},
journal = {Icml-2015},
month = {feb},
pages = {1462--1471},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
url = {http://arxiv.org/abs/1502.04623},
year = {2015}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
author = {Bengio, Yoshua},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Bengio - 1994 - Learning Long-Term Dependencies with Gradient Descent is Difficult.pdf:pdf},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
url = {http://www.dsi.unifi.it/{~}paolo/ps/tnn-94-gradient.pdf http://dsii.dsi.unifi.it/{~}paolo/ps/tnn-94-gradient.pdf},
volume = {5},
year = {1994}
}
@article{Sussillo2014,
abstract = {Training very deep networks is an important open problem in machine learning. One of many difficulties is that the norm of the back-propagated error gradient can grow or decay exponentially. Here we show that training very deep feed-forward networks (FFNs) is not as difficult as previously thought. Unlike when back-propagation is applied to a recurrent network, application to an FFN amounts to multiplying the error gradient by a different random matrix at each layer. We show that the successive application of correctly scaled random matrices to an initial vector results in a random walk of the log of the norm of the resulting vectors, and we compute the scaling that makes this walk unbiased. The variance of the random walk grows only linearly with network depth and is inversely proportional to the size of each layer. Practically, this implies a gradient whose log-norm scales with the square root of the network depth and shows that the vanishing gradient problem can be mitigated by increasing the width of the layers. Mathematical analyses and experimental results using stochastic gradient descent to optimize tasks related to the MNIST and TIMIT datasets are provided to support these claims. Equations for the optimal matrix scaling are provided for the linear and ReLU cases.},
archivePrefix = {arXiv},
arxivId = {1412.6558},
author = {Sussillo, David and Abbott, L. F.},
eprint = {1412.6558},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Sussillo, Abbott - 2014 - Random Walk Initialization for Training Very Deep Feedforward Networks.pdf:pdf},
journal = {arXiv},
pages = {10},
title = {{Random Walk Initialization for Training Very Deep Feedforward Networks}},
url = {http://arxiv.org/abs/1412.6558},
year = {2014}
}
@inproceedings{Freund1996,
author = {Freund, Yoav and Schapire, Robert E},
booktitle = {Thirteenth International Conference on Machine Learning},
doi = {10.1.1.133.1040},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Freund, Schapire - 1996 - Experiments with a new boosting algorithm.pdf:pdf},
isbn = {1558604197},
issn = {0706-652X, 1205-7533},
pages = {148--156},
title = {{Experiments with a new boosting algorithm}},
url = {http://www.public.asu.edu/{~}jye02/CLASSES/Fall-2005/PAPERS/boosting-icml.pdf},
year = {1996}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@phdthesis{Mikolov2012,
abstract = {Statistical language models are crucial part of many successful applications, such as au- tomatic speech recognition and statistical machine translation (for example well-known Google Translate). Traditional techniques for estimating these models are based on N- gram counts. Despite known weaknesses of N-grams and huge efforts of research commu- nities across many fields (speech recognition, machine translation, neuroscience, artificial intelligence, natural language processing, data compression, psychology etc.), N-grams remained basically the state-of-the-art. The goal of this thesis is to present various archi- tectures of language models that are based on artificial neural networks. Although these models are computationally more expensive than N-gram models, with the presented techniques it is possible to apply them to state-of-the-art systems efficiently. Achieved reductions of word error rate of speech recognition systems are up to 20{\%}, against state- of-the-art N-gram model. The presented recurrent neural network based model achieves the best published performance on well-known Penn Treebank setup. Kl´cov´},
archivePrefix = {arXiv},
arxivId = {1312.3005},
author = {Mikolov, Tomas},
booktitle = {PhD thesis},
doi = {10.1016/j.csl.2015.07.001},
eprint = {1312.3005},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov Tomas - Unknown - Statistical Language Models Based on Neural Networks.pdf:pdf},
isbn = {0885-2308},
issn = {08852308},
number = {April},
pages = {1--129},
pmid = {18244602},
title = {{Statistical Language Models Based on Neural Networks}},
url = {http://www.fit.vutbr.cz/research/pubs/diss.php.cs?id=10158},
year = {2012}
}
@inproceedings{Mikolov2011,
author = {Mikolov, Tomas and Kombrink, Stefan and Burget, Lukas and Cernocky, Jan and Khudanpur, Sanjeev},
booktitle = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2011.5947611},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2011 - Extensions of recurrent neural network language model.pdf:pdf},
isbn = {978-1-4577-0538-0},
keywords = {Artificial neural networks,Backpropagation,Computational modeling,Probability distribution,Recurrent neural networks,Training,Vocabulary,backpropagation,competitive language modeling techniques,computational complexity,feedforward network,feedforward neural nets,language modeling,natural language processing,recurrent neural nets,recurrent neural network language model,recurrent neural networks,speech recognition},
month = {may},
pages = {5528--5531},
publisher = {IEEE},
title = {{Extensions of recurrent neural network language model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5947611},
year = {2011}
}
@inproceedings{Mukherjee2013,
abstract = {We describe a new, simplified, and general analysis of a fusion of Nesterov's accelerated gradient with parallel coordinate de-scent. The resulting algorithm, which we call BOOM, for boosting with momentum, enjoys the merits of both techniques. Namely, BOOM re-tains the momentum and convergence properties of the accelerated gra-dient method while taking into account the curvature of the objective function. We describe an distributed implementation of BOOM which is suitable for massive high dimensional datasets. We show experimentally that BOOM is especially effective in large scale learning problems with rare yet informative features.},
author = {Mukherjee, Indraneel and Canini, Kevin and Frongillo, Rafael and Singer, Yoram},
booktitle = {ECML PKDD 2013, Part III, LNAI 8190},
doi = {10.1007/978-3-642-40994-3_2},
isbn = {9783642409936},
issn = {03029743},
keywords = {accelerated gradient,boosting,coordinate descent},
number = {PART 3},
pages = {17--32},
title = {{Parallel Boosting with Momentum}},
volume = {8190 LNAI},
year = {2013}
}
@article{Ioffe2015,
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {arXiv},
month = {feb},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Wu2016,
abstract = {We introduce a general and simple structural design called Multiplicative Integration (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.},
archivePrefix = {arXiv},
arxivId = {1606.06630},
author = {Wu, Yuhuai and Zhang, Saizheng and Zhang, Ying and Bengio, Yoshua and Salakhutdinov, Ruslan},
eprint = {1606.06630},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - On Multiplicative Integration with Recurrent Neural Networks.pdf:pdf},
isbn = {9789537619084},
journal = {arXiv},
month = {jun},
title = {{On Multiplicative Integration with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1606.06630},
year = {2016}
}
@article{Zamir,
abstract = {Users of Web search engines are often forced to sift through the long ordered list of document " snippets " returned by the engines. The IR community has explored document clustering as an alternative method of organizing retrieval results, but clustering has yet to be deployed on the major search engines. The paper articulates the unique requirements of Web document clustering and reports on the first evaluation of clustering methods in this domain. A key requirement is that the methods create their clusters based on the short snippets returned by Web search engines. Surprisingly, we find that clusters based on snippets are almost as good as clusters created using the full text of Web documents. To satisfy the stringent requirements of the Web domain, we introduce an incremental, linear time (in the document collection size) algorithm called Suffix Tree Clustering (STC), which creates clusters based on phrases shared between documents. We show that STC is faster than standard clustering methods in this domain, and argue that Web document clustering via STC is both feasible and potentially beneficial.},
author = {Zamir, Oren and Etzioni, Oren},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Zamir, Etzioni - Unknown - Web Document Clustering A Feasibility Demonstration.pdf:pdf},
title = {{Web Document Clustering: A Feasibility Demonstration}}
}
@article{Hitchcock1927,
author = {Hitchcock, Frank L.},
doi = {10.1002/sapm192761164},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Hitchcock - 1927 - The Expression of a Tensor or a Polyadic as a Sum of Products.pdf:pdf},
issn = {00971421},
journal = {Journal of Mathematics and Physics},
month = {apr},
number = {1-4},
pages = {164--189},
title = {{The Expression of a Tensor or a Polyadic as a Sum of Products}},
url = {http://doi.wiley.com/10.1002/sapm192761164},
volume = {6},
year = {1927}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {arXiv},
keywords = {deep learning,denoising auto-encoder,image denoising},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385 http://arxiv.org/pdf/1512.03385v1.pdf},
year = {2015}
}
@article{Sigaud2015,
abstract = {Gated networks are networks that contain gating connections, in which the outputs of at least two neurons are multiplied. Initially, gated networks were used to learn relationships between two input sources, such as pixels from two images. More recently, they have been applied to learning activity recognition or multi-modal representations. The aims of this paper are threefold: 1) to explain the basic computations in gated networks to the non-expert, while adopting a standpoint that insists on their symmetric nature. 2) to serve as a quick reference guide to the recent literature, by providing an inventory of applications of these networks, as well as recent extensions to the basic architecture. 3) to suggest future research directions and applications.},
archivePrefix = {arXiv},
arxivId = {1512.03201},
author = {Sigaud, Olivier and Masson, Cl{\'{e}}ment and Filliat, David and Stulp, Freek},
eprint = {1512.03201},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Sigaud et al. - 2015 - GATED NETWORKS AN INVENTORY.pdf:pdf},
journal = {arXiv},
keywords = {()},
title = {{Gated networks: an inventory}},
url = {http://arxiv.org/abs/1512.03201},
year = {2015}
}
@article{Werbos1990,
abstract = {Basic backpropagation, which is a simple method now being widely$\backslash$nused in areas like pattern recognition and fault diagnosis, is reviewed.$\backslash$nThe basic equations for backpropagation through time, and applications$\backslash$nto areas like pattern recognition involving dynamic systems, systems$\backslash$nidentification, and control are discussed. Further extensions of this$\backslash$nmethod, to deal with systems other than neural networks, systems$\backslash$ninvolving simultaneous equations, or true recurrent networks, and other$\backslash$npractical issues arising with the method are described. Pseudocode is$\backslash$nprovided to clarify the algorithms. The chain rule for ordered$\backslash$nderivatives-the theorem which underlies backpropagation-is briefly$\backslash$ndiscussed. The focus is on designing a simpler version of$\backslash$nbackpropagation which can be translated into computer code and applied$\backslash$ndirectly by neutral network users},
author = {Werbos, Paul J},
doi = {10.1109/5.58337},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Werbos - 1990 - Backpropagation Through Time What It Does and How to Do It.pdf:pdf},
isbn = {0018-9219},
issn = {15582256},
journal = {Proceedings of the IEEE},
number = {10},
pages = {1550--1560},
title = {{Backpropagation Through Time: What It Does and How to Do It}},
volume = {78},
year = {1990}
}
@inproceedings{Jozefowicz2015,
abstract = {The Recurrent Neural Network (RNN) is an ex-tremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's archi-tecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thor-ough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.},
author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
booktitle = {ICML},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Jozefowicz, Zaremba, Sutskever - Unknown - An Empirical Exploration of Recurrent Network Architectures.pdf:pdf},
title = {{An Empirical Exploration of Recurrent Network Architectures}},
year = {2015}
}
@article{Kolda2009,
author = {Kolda, Tamara G. and Bader, Brett W.},
doi = {10.1137/07070111X},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Kolda, Bader - 2009 - Tensor Decompositions and Applications.pdf:pdf},
issn = {0036-1445},
journal = {SIAM Review},
month = {aug},
number = {3},
pages = {455--500},
title = {{Tensor Decompositions and Applications}},
url = {http://epubs.siam.org/doi/abs/10.1137/07070111X},
volume = {51},
year = {2009}
}
@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
doi = {10.1145/1143844.1143891},
file = {:Users/pfcmathews/Downloads/icml{\_}2006.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
@article{Dauphin2014,
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
archivePrefix = {arXiv},
arxivId = {1406.2572},
author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
eprint = {1406.2572},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.pdf:pdf},
isbn = {1406.2572},
issn = {10495258},
journal = {arXiv},
month = {jun},
pages = {1--14},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
url = {http://arxiv.org/abs/1406.2572},
year = {2014}
}
@article{Neyshabur2016,
abstract = {We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.},
archivePrefix = {arXiv},
arxivId = {1605.07154},
author = {Neyshabur, Behnam and Wu, Yuhuai and Salakhutdinov, Ruslan and Srebro, Nathan},
eprint = {1605.07154},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Neyshabur et al. - 2016 - Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations.pdf:pdf},
journal = {arXiv preprint},
month = {may},
title = {{Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations}},
url = {http://arxiv.org/abs/1605.07154},
year = {2016}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - 2014 - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.pdf:pdf},
title = {{ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION}},
year = {2014}
}
@article{Mansimov2015,
abstract = {Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.},
archivePrefix = {arXiv},
arxivId = {1511.02793},
author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
doi = {10.1088/0004-6256/150/6/203},
eprint = {1511.02793},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mansimov et al. - 2015 - Generating Images from Captions with Attention.pdf:pdf},
journal = {arXiv preprint},
pages = {1--12},
title = {{Generating Images from Captions with Attention}},
url = {http://arxiv.org/abs/1511.02793},
year = {2015}
}
@article{Cho2014b,
abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1259v2},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
eprint = {arXiv:1409.1259v2},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - On the Properties of Neural Machine Translation Encoder-Decoder Approaches.pdf:pdf},
isbn = {9781937284961},
journal = {Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
month = {sep},
pages = {103--111},
title = {{On the Properties of Neural Machine Translation: Encoder–Decoder Approaches}},
url = {http://arxiv.org/abs/1409.1259 http://arxiv.org/pdf/1409.1259v2.pdf$\backslash$nhttp://arxiv.org/abs/1409.1259},
year = {2014}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
month = {jun},
pages = {1724--1734},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Im2016,
abstract = {Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual "canvas". We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other.},
archivePrefix = {arXiv},
arxivId = {1602.05110},
author = {Im, Daniel Jiwoong and Kim, Chris Dongjoo and Jiang, Hui and Memisevic, Roland},
eprint = {1602.05110},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Im et al. - 2016 - Generating images with recurrent adversarial networks.pdf:pdf},
journal = {arxiv},
month = {feb},
title = {{Generating images with recurrent adversarial networks}},
url = {http://arxiv.org/abs/1602.05110},
year = {2016}
}
@article{Harshman1970,
abstract = {Simple structure and other common principles of factor rotation do not in general provide strong grounds for attributing explanatory significance to the factors which they select. In contrast, it is shown that an extension of Cattell's principle of rotation to Proportional Profiles (PP) offers a basis for determining explanatory factors for three-way or higher order multi-mode data. Conceptual models are developed for two basic patterns of multi-mode data variation, system- and object-variation, and PP analysis is found to apply in the system-variation case. Although PP was originally formulated as a principle of rotation to be used with classic two-way factor analysis, it is shown to embody a latent three-mode factor model, which is here made explicit and generalized frown two to N "parallel occasions". As originally formulated, PP rotation was restricted to orthogonal factors. The generalized PP model is demonstrated to give unique "correct" solutions with oblique, non-simple structure, and even non-linear factor structures. A series of tests, conducted with synthetic data of known factor composition, demonstrate the capabilities of linear and non-linear versions of the model, provide data on the minimal necessary conditions of uniqueness, and reveal the properties of the analysis procedures when these minimal conditions are not fulfilled. In addition, a mathematical proof is presented for the uniqueness of the solution given certain conditions on the data. Three-mode PP factor analysis is applied to a three-way set of real data consisting of the fundamental and first three formant frequencies of 11 persons saying 8 vowels. A unique solution is extracted, consisting of three factors which are highly meaningful and consistent with prior knowledge and theory concerning vowel quality. The relationships between the three-mode PP model and Tucker's multi-modal model, McDonald's non-linear model and Carroll and Chang's multi-dimensional scaling model are explored.},
author = {Harshman, Richard A},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Harshman - 1970 - FOUNDATIONS OF THE PARAFAC PROCEDURE MODELS AND CONDITIONS FOR AN {\&}quotEXPLANATORY{\&}quot MULTIMODAL FACTOR ANALYSIS.pdf:pdf},
journal = {UCLA Working Papers in Phonetics},
number = {10},
pages = {1-- 84},
title = {{Foundations of the PARAFAC procedure: Models and conditions for an “explanatory” multimodal factor analysis}},
url = {http://www.psychology.uwo.ca/faculty/harshman/wpppfac0.pdf},
volume = {16},
year = {1970}
}
@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555v1},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
eprint = {1412.3555v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:pdf},
journal = {arXiv},
month = {dec},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
url = {http://arxiv.org/abs/1412.3555},
year = {2014}
}
@article{Breiman2001,
author = {Breiman, L},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 2001 - Random forests(2).pdf:pdf},
journal = {Machine learning},
title = {{Random forests}},
url = {http://link.springer.com/article/10.1023/A:1010933404324},
year = {2001}
}
@article{Dean2004,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.},
archivePrefix = {arXiv},
arxivId = {10.1.1.163.5292},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
eprint = {10.1.1.163.5292},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Dean, Ghemawat - 2004 - MapReduce Simplied Data Processing on Large Clusters.pdf:pdf},
isbn = {9781595936868},
issn = {00010782},
journal = {Proceedings of 6th Symposium on Operating Systems Design and Implementation},
pages = {137--149},
pmid = {11687618},
title = {{MapReduce: Simplied Data Processing on Large Clusters}},
year = {2004}
}
@article{Koutnik2014,
abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is required. This paper introduces a simple, yet powerful modification to the simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of SRN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification, where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.3511v1},
author = {Koutnik, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Juergen},
eprint = {arXiv:1402.3511v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Koutnik et al. - 2014 - A Clockwork RNN.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of The 31st International Conference on Machine Learning},
pages = {1863--1871},
title = {{A Clockwork RNN}},
url = {http://jmlr.org/proceedings/papers/v32/koutnik14.html},
volume = {32},
year = {2014}
}
@article{Friedman2001,
author = {Friedman, Jerome H.},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Friedman - 2001 - Greedy function approximation A gradient boosting machine.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Function estimation,boosting,decision trees,robust nonparametric regression},
number = {5},
pages = {1189--1232},
publisher = {Institute of Mathematical Statistics},
title = {{Greedy function approximation: A gradient boosting machine.}},
volume = {29},
year = {2001}
}
@article{Barone2016,
abstract = {Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. Effective learning in this setting is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem of naive deep networks. Many of these architectures, such as LSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single structural principle: the state passthrough. We observe that these architectures, hereby characterized as Passthrough Networks, in addition to the mitigation of the vanishing gradient problem, enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on synthetic tasks and a state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.},
archivePrefix = {arXiv},
arxivId = {1603.03116},
author = {Barone, Antonio Valerio Miceli},
eprint = {1603.03116},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Barone - 2016 - Low-rank passthrough neural networks.pdf:pdf},
journal = {Arxiv},
month = {mar},
pages = {16},
title = {{Low-rank passthrough neural networks}},
url = {http://arxiv.org/abs/1603.03116},
year = {2016}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@inproceedings{Mason1999,
abstract = {We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theo-retical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions.},
author = {Mason, Llew and Bartlett, Peter and Baxter, Jonathan and Frean, Marcus},
booktitle = {NIPS},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Mason et al. - 1999 - Boosting Algorithms as Gradient Descent.pdf:pdf},
title = {{Boosting Algorithms as Gradient Descent}},
year = {1999}
}
@article{Dai2015,
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
archivePrefix = {arXiv},
arxivId = {1511.01432},
author = {Dai, Andrew M. and Le, Quoc V.},
eprint = {1511.01432},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Dai, Le - 2015 - Semi-supervised Sequence Learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS '15)},
month = {nov},
pages = {1--9},
title = {{Semi-supervised Sequence Learning}},
url = {http://arxiv.org/abs/1511.01432},
year = {2015}
}
@article{Schapire1990,
abstract = {The problem of improving the accuracy of a hypothesis output by a$\backslash$nlearning algorithm in the distribution-free learning model is$\backslash$nconsidered. A concept class is learnable (or strongly learnable) if,$\backslash$ngiven access to a source of examples from the unknown concept, the$\backslash$nlearner with high probability is able to output a hypothesis that is$\backslash$ncorrect on all but an arbitrarily small fraction of the instances. The$\backslash$nconcept class is weakly learnable if the learner can produce a$\backslash$nhypothesis that forms only slightly better than random guessing. It is$\backslash$nshown that these two notions of learnability are equivalent. An explicit$\backslash$nmethod is described for directly converting a weak learning algorithm$\backslash$ninto one that achieves arbitrarily high accuracy. This construction may$\backslash$nhave practical applications as a tool for efficiently converting a$\backslash$nmediocre learning algorithm into one that performs extremely well. In$\backslash$naddition, the construction has some interesting theoretical consequences$\backslash$n},
author = {Schapire, Robert E},
doi = {10.1023/A:1022648800760},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Schapire - 1990 - The Strength of Weak Learnability.pdf:pdf},
isbn = {0-8186-1982-1},
issn = {15730565},
journal = {Machine Learning},
keywords = {Machine learning,PAC learning,learnability theory,learning from examples,polynomial-time identification},
number = {2},
pages = {197--227},
title = {{The Strength of Weak Learnability}},
volume = {5},
year = {1990}
}
@article{Ho1998,
abstract = {Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy},
author = {Ho, Tin Kam},
doi = {10.1109/34.709601},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Ho - 1998 - The Random Subspace Method for Constructing Decision Forest.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {832--844},
title = {{The Random Subspace Method for Constructing Decision Forest}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=709601 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=709601},
volume = {20},
year = {1998}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, QVV},
eprint = {1409.3215},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks.pdf:pdf},
isbn = {1409.3215},
journal = {Advances in Neural Information Processing Systems},
pages = {3104--3112},
title = {{Sequence to sequence learning with neural networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
@article{Krueger2016,
abstract = {We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modelling and phoneme recognition, and outperforming weight noise. With this penalty term, IRNN can achieve similar performance to LSTM on language modelling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences.},
archivePrefix = {arXiv},
arxivId = {1511.08400},
author = {Krueger, David and Memisevic, Roland},
eprint = {1511.08400},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Krueger, Memisevic - 2015 - Regularizing RNNs by Stabilizing Activations.pdf:pdf},
journal = {International Conference On Learning Representations},
month = {nov},
pages = {1--8},
title = {{Regularizing RNNs by Stabilizing Activations}},
url = {http://arxiv.org/abs/1511.08400},
year = {2016}
}
@article{Breiman2001a,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength og the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 2001 - Random Forests.pdf:pdf},
journal = {Machine Learning},
number = {1},
pages = {5--32},
title = {{Random Forests}},
url = {http://link.springer.com/article/10.1023/A:1010933404324},
volume = {45},
year = {2001}
}
@article{Elman1990,
author = {Elman, Jeffrey l},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Elman - 1990 - Finding Structure in Time.pdf:pdf},
journal = {COGNITIVE SCIENCE},
pages = {179--211},
title = {{Finding Structure in Time}},
volume = {14},
year = {1990}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {arXiv:1308.0850v5},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Graves - 2013 - Generating sequences with recurrent neural networks(2).pdf:pdf},
isbn = {2000201075},
issn = {18792782},
journal = {arXiv preprint arXiv:1308.0850},
pages = {1--43},
pmid = {23459267},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Le2015,
abstract = {Learning long term dependencies in recurrent networks is difficult due to van-ishing and exploding gradients. To overcome this difficulty, researchers have de-veloped sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to a standard implementation of LSTMs on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00941v1},
author = {Le, Quoc V. and Jaitly, Navdeep and Hinton, Geoffrey E.},
doi = {10.1109/72.279181},
eprint = {arXiv:1504.00941v1},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Le, Jaitly, Hinton - 2015 - A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.pdf:pdf},
isbn = {9781461268758},
issn = {1045-9227},
journal = {arXiv preprint arXiv:1504.00941},
pages = {1--9},
pmid = {17756722},
title = {{A Simple Way to Initialize Recurrent Networks of Rectified Linear Units}},
year = {2015}
}
@article{Hopfield1982,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
author = {Hopfield, J J},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Hopfield - 1982 - Neural networks and physical systems with emergent collective computational abilities.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {apr},
number = {8},
pages = {2554--8},
pmid = {6953413},
publisher = {National Academy of Sciences},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6953413 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC346238},
volume = {79},
year = {1982}
}
@inproceedings{Taylor,
abstract = {The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factor the three-way weight tensor implied by the multiplicative model, reducing the number of parameters from O(N3) to O(N2). The result is an efficient, compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM, our model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly among them.},
author = {Taylor, Graham W. and Hinton, Geoffrey E.},
booktitle = {Proceedings of the 26th International Conference on Machine Learning (ICML 09)},
doi = {10.1145/1553374.1553505},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Taylor, Hinton - 2009 - Factored conditional restricted Boltzmann Machines for modeling motion style.pdf:pdf},
isbn = {9781605585161},
issn = {1520510X},
pages = {1025--1032},
title = {{Factored conditional restricted Boltzmann Machines for modeling motion style}},
url = {http://dl.acm.org/citation.cfm?id=1553374.1553505},
year = {2009}
}
@article{Ye2009,
abstract = {Stochastic Gradient Boosted Decision Trees (GBDT) is one of the most widely used learning algorithms in machine learning today. It is adaptable, easy to interpret, and produces highly accurate models. However, most implementations today are computationally expensive and require all training data to be in main memory. As training data becomes ever larger, there is motivation for us to parallelize the GBDT algorithm. Parallelizing decision tree training is intuitive and various approaches have been explored in existing literature. Stochastic boosting on the other hand is inherently a sequential process and have not been applied to distributed decision trees. In this work, we present two different distributed methods that generates exact stochastic GBDT models, the first is a MapReduce implementation and the second utilizes MPI on the Hadoop grid environment.},
author = {Ye, Jerry and Chow, Jyh-Herng and Chen, Jiang and Zheng, Zhaohui},
doi = {10.1145/1645953.1646301},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Ye et al. - 2009 - Stochastic Gradient Boosted Distributed Decision Trees.pdf:pdf},
isbn = {978-1-60558-512-3},
issn = {1605585122},
journal = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
keywords = {decision trees,distributed learning,gradient boosting,hadoop,learning to rank,mpi,web search ranking},
pages = {2061--2064},
title = {{Stochastic Gradient Boosted Distributed Decision Trees}},
url = {http://doi.acm.org/10.1145/1645953.1646301},
year = {2009}
}
@article{Li2015,
abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
archivePrefix = {arXiv},
arxivId = {1511.05493},
author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
eprint = {1511.05493},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2015 - Gated Graph Sequence Neural Networks.pdf:pdf},
journal = {ICLR},
keywords = {Gated Feedback,Graph Structure},
number = {1},
pages = {1--19},
title = {{Gated Graph Sequence Neural Networks}},
url = {http://arxiv.org/abs/1511.05493},
year = {2016}
}
@article{Singhal2001,
abstract = {For thousands of years people have realized the importance of archiving and finding information. With the advent of computers, it became possible to store large amounts of information; and finding useful information from such collections became a necessity. The field of Information Retrieval (IR) was born in the 1950s out of this necessity. Over the last forty years, the field has matured considerably. Several IR systems are used on an everyday basis by a wide variety of users. This article is a brief overview of the key advances in the field of Information Retrieval, and a description of where the state-of-the-art is at in the field.},
author = {Singhal, Amit},
doi = {10.1.1.117.7676‎},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Singhal - 2001 - Modern Information Retrieval A Brief Overview.pdf:pdf},
issn = {00218979},
journal = {Bulletin of the Ieee Computer Society Technical Committee on Data Engineering},
number = {4},
pages = {1--9},
title = {{Modern Information Retrieval: A Brief Overview}},
url = {http://160592857366.free.fr/joe/ebooks/ShareData/Modern Information Retrieval - A Brief Overview.pdf},
volume = {24},
year = {2001}
}
@article{Breiman1996,
abstract = {Bagging predictors is a method for generating multiple versions of a pre-dictor and using these to get an aggregated predictor. The aggregation av-erages over the versions when predicting a numerical outcome and does a plurality v ote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classiication and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability o f the prediction method. If perturbing the learning set can cause signiicant changes in the predictor constructed, then bagging can improve accuracy.},
author = {Breiman, Leo},
doi = {10.1007/BF00058655},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 1996 - Bagging Predictors.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {aggregation,averaging,bootstrap,combining},
pages = {123--140},
pmid = {17634459},
title = {{Bagging Predictors}},
url = {http://link.springer.com/article/10.1007/BF00058655},
volume = {24},
year = {1996}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Palit2012,
abstract = {In this era of data abundance, it has become critical to process large volumes of data at much faster rates than ever before. Boosting is a powerful predictive model that has been successfully used in many real-world applications. However, due to the inherent sequential nature, achieving scalability for boosting is nontrivial and demands the development of new parallelized versions which will allow them to efficiently handle large-scale data. In this paper, we propose two parallel boosting algorithms, AdaBoost.PL and LogitBoost.PL, which facilitate simultaneous participation of multiple computing nodes to construct a boosted ensemble classifier. The proposed algorithms are competitive to the corresponding serial versions in terms of the generalization performance. We achieve a significant speedup since our approach does not require individual computing nodes to communicate with each other for sharing their data. In addition, the proposed approach also allows for preserving privacy of computations in distributed environments. We used MapReduce framework to implement our algorithms and demonstrated the performance in terms of classification accuracy, speedup and scaleup using a wide variety of synthetic and real-world data sets.},
author = {Palit, Indranil and Reddy, Chandan K},
doi = {10.1109/TKDE.2011.208},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Palit, Reddy - 2012 - Scalable and Parallel Boosting with MapReduce.pdf:pdf},
isbn = {2011030145},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Boosting,MapReduce,classification,distributed computing,parallel algorithms},
number = {10},
pages = {1904--1916},
title = {{Scalable and Parallel Boosting with MapReduce}},
volume = {24},
year = {2012}
}
@article{Friedman2002,
abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current “pseudo”-residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner.},
author = {Friedman, Jerome H.},
doi = {10.1016/S0167-9473(01)00065-2},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Friedman - 2002 - Stochastic gradient boosting.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
number = {4},
pages = {367--378},
title = {{Stochastic gradient boosting}},
volume = {38},
year = {2002}
}
@article{Chung2015,
abstract = {In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.},
archivePrefix = {arXiv},
arxivId = {1502.02367},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1145/2661829.2661935},
eprint = {1502.02367},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Chung et al. - 2015 - Gated feedback recurrent neural networks.pdf:pdf},
isbn = {9781634393973},
issn = {18792782},
journal = {Proceedings of the 32nd International Conference on Machine Learning},
pages = {2067----2075},
pmid = {23459267},
title = {{Gated feedback recurrent neural networks}},
url = {http://arxiv.org/abs/1502.02367},
volume = {37},
year = {2015}
}
@article{Dinarelli2016,
abstract = {In this paper we study different types of Recurrent Neural Networks (RNN) for sequence labeling tasks. We propose two new variants of RNNs integrating improvements for sequence labeling, and we compare them to the more traditional Elman and Jordan RNNs. We compare all models, either traditional or new, on four distinct tasks of sequence labeling: two on Spoken Language Understanding (ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the Penn Treebank (PTB) corpora. The results show that our new variants of RNNs are always more effective than the others.},
archivePrefix = {arXiv},
arxivId = {1606.02555},
author = {Dinarelli, Marco and Tellier, Isabelle},
eprint = {1606.02555},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Dinarelli, Tellier - 2016 - Improving Recurrent Neural Networks For Sequence Labelling.pdf:pdf},
month = {jun},
title = {{Improving Recurrent Neural Networks For Sequence Labelling}},
url = {http://arxiv.org/abs/1606.02555},
year = {2016}
}
@article{Orus2014,
abstract = {This is a partly non-technical introduction to selected topics on tensor network methods, based on several lectures and introductory seminars given on the subject. It should be a good place for newcomers to get familiarized with some of the key ideas in the field, specially regarding the numerics. After a very general introduction we motivate the concept of tensor network and provide several examples. We then move on to explain some basics about Matrix Product States (MPS) and Projected Entangled Pair States (PEPS). Selected details on some of the associated numerical methods for 1. d and 2. d quantum lattice systems are also discussed. ?? 2014 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {1306.2164},
author = {Or{\'{u}}s, Rom{\'{a}}n},
doi = {10.1016/j.aop.2014.06.013},
eprint = {1306.2164},
issn = {1096035X},
journal = {Annals of Physics},
keywords = {Entanglement,MPS,PEPS,Tensor networks},
month = {jun},
pages = {117--158},
title = {{A practical introduction to tensor networks: Matrix product states and projected entangled pair states}},
url = {http://arxiv.org/abs/1306.2164 http://dx.doi.org/10.1016/j.aop.2014.06.013},
volume = {349},
year = {2014}
}
@article{Hihi1995,
abstract = {We have already shown that extracting long-term dependencies from sequential data is difficult, both for deterministic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.},
author = {Hihi, Salah El and Bengio, Yoshua},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Hihi, Bengio - 1995 - Hierarchical Recurrent Neural Networks for Long-Term Dependencies.pdf:pdf},
journal = {Nips},
pages = {493--499},
title = {{Hierarchical Recurrent Neural Networks for Long-Term Dependencies.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.7574{\&}rep=rep1{\&}type=pdf},
year = {1995}
}
@article{Dietterich2000,
abstract = {Publication View. 42637098. Dietterich TG: for Constructing Ensembles of Decision Trees (2000). Shuly Wintner. Abstract. While the morphology of Modem Hebrew is well accounted},
author = {Dietterich, Thomas G.},
doi = {10.1023/A:1007607513941},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Dietterich - 2000 - An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {5,bagging,boosting,c4,decision trees,ensemble learning,monte carlo methods},
pages = {139--157},
title = {{An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees}},
url = {http://en.scientificcommons.org/42637098$\backslash$nuuid/7906280C-AEF8-405A-9A94-6BAA1DDAED1E},
volume = {40},
year = {2000}
}
@book{Tan2006,
abstract = {1st ed. 1. Introduction -- 2. Data -- 3. Exploring data -- 4. Classification : basic concepts, decision trees, and model evaluation -- 5. Classification : alternative techniques -- 6. Association analysis : basic concepts and algorithms -- 7. Association analysis : advanced concepts -- 8. Cluster analysis : basic concepts and algorithms -- 9. Cluster analysis : additional issues and algorithms -- 10. Anomaly detection -- App. A. Linear algebra -- App. B. Dimensionality reduction -- App. C. Probability and statistics -- App. D. Regression -- App. E. Optimization.},
author = {Tan, Pang-Ning. and Steinbach, Michael. and Kumar, Vipin},
isbn = {978-0-321-32136-7},
keywords = {Data mining},
pages = {769},
publisher = {Pearson Addison Wesley},
title = {{Introduction to data mining}},
year = {2006}
}
@article{Rezende2014,
abstract = {Abstract We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep , directed generative models , endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition ... $\backslash$n},
author = {Rezende, D J and Mohamed, S and Wierstra, D},
journal = {Proceedings of The 31st {\ldots}},
pages = {1278--1286},
title = {{Stochastic backpropagation and approximate inference in deep generative models}},
url = {http://jmlr.org/proceedings/papers/v32/rezende14.html$\backslash$npapers3://publication/uuid/F2747569-7719-4EAC-A5A7-9ECA9D6A8FE6},
volume = {32},
year = {2014}
}
@article{XuanVinh2010,
abstract = {Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing pop-ular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then high-light to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants.},
author = {{Xuan Vinh}, Nguyen and {Julien Epps}, Unsweduau and Bailey, James},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Xuan Vinh, Julien Epps, Bailey - 2010 - Information Theoretic Measures for Clusterings Comparison Variants, Properties, Normalization an.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {adjustment for chance,clustering comparison,information theory,normalized infor-mation distance},
pages = {2837--2854},
title = {{Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance}},
volume = {11},
year = {2010}
}
@article{Breiman1997,
abstract = {Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman[1996]. The first successful arcing procedure was introduced by Freund and Schapire[1995,1996] and called Adaboost. In an effort to explain why Adaboost works, Schapire et.al. [1997] derived a bound on the generalization error of a convex combination of classifiers in terms of the margin. We introduce a function called the edge, which differs from the margin only if there are more than two classes. A framework for understanding arcing algorithms is defined. In this framework, we see that the arcing algorithms currently in the literature are optimization algorithms which minimize some function of the edge. A relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner. Two algorithms are described which achieve the optimal reduction. Tests on both synthetic and real data cast doubt on the Schapire et.al. explanation.},
author = {Breiman, Leo},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 1997 - Arcing the edge.pdf:pdf},
journal = {Statistics},
pages = {1--14},
title = {{Arcing the edge}},
volume = {4},
year = {1997}
}
@article{Boulanger-Lewandowski2012,
abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
archivePrefix = {arXiv},
arxivId = {1206.6392},
author = {Boulanger-Lewandowski, Nicolas and Vincent, Pascal and Bengio, Yoshua},
eprint = {1206.6392},
isbn = {978-1-4503-1285-1},
journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
month = {jun},
number = {Cd},
pages = {1159--1166},
title = {{Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription}},
url = {http://arxiv.org/abs/1206.6392},
year = {2012}
}
@article{Chung2016,
abstract = {Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.},
archivePrefix = {arXiv},
arxivId = {1609.01704},
author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
eprint = {1609.01704},
file = {:Users/pfcmathews/Library/Application Support/Mendeley Desktop/Downloaded/Chung, Ahn, Bengio - 2016 - Hierarchical Multiscale Recurrent Neural Networks.pdf:pdf},
journal = {arXiv preprint},
title = {{Hierarchical Multiscale Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1609.01704},
year = {2016}
}
